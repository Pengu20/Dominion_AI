 --------- CARDS IN THIS GAME --------- 
Game cards: [['Copper' '0' '0']
 ['Silver' '1' '3']
 ['Gold' '2' '6']
 ['Estate' '3' '2']
 ['Duchy' '4' '5']
 ['Province' '5' '8']
 ['Curse' '6' '0']
 ['Remodel' '16' '4']
 ['Workshop' '11' '3']
 ['Chapel' '8' '2']
 ['Witch' '25' '5']
 ['Poacher' '29' '4']
 ['Militia' '14' '4']
 ['Market' '23' '5']
 ['Village' '10' '3']
 ['Library' '22' '5']
 ['Moneylender' '15' '4']]






         -------------------- Turn: 1 -------------------- 
Player: 0 
cards in hand: [0. 0. 0. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[5.7487082]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[      -5 -3000000        0      -60        0        0       20        0
        0        0        0      -60        0     -300        0        0] 
sum of rewards: -3000405 

action type: buy - action 6.0
Learning step: -120007.2734375
desired expected reward: -120230.2109375





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[  5.583735 ]
 [ 20.148157 ]
 [  9.452707 ]
 [-29.026865 ]
 [-88.066475 ]
 [ 14.548498 ]
 [ 16.47585  ]
 [  5.2782874]
 [ 21.692654 ]
 [ 28.478941 ]
 [ 15.627676 ]
 [ 21.1982   ]
 [ 13.199385 ]
 [ 14.498255 ]
 [ 20.54213  ]
 [  2.930327 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 0 
buys: 1 
player value: 5 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 5.244390964508057



buy possibilites: [-1] 
expected returns: [[-1.0327461]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [29.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
action values: 0 
buys: 0 
player value: 1 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 27.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 28.47894859313965






Player: 1 
cards in hand: [0. 0. 0. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 3. 0. 3.] 
adversary cards in discard: [29.  0.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 0 
buys: 1 
player value: 4 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 3. 0. 3.] 
adversary cards in discard: [29.  0.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
adversary victory points: 3
player victory points: 3 


 --------- NOTHING HAPPENED --------- 



         -------------------- Turn: 2 -------------------- 
Player: 0 
cards in hand: [0. 3. 3. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[-3.5032759]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 3. 0. 3.] 
cards in discard: [29.  0.  0.  0.  0.  0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 3.] 
adversary cards in discard: [0. 0. 0. 0. 3.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: -1.0327460765838623





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
expected returns: [[ -0.5678282]
 [  3.199266 ]
 [-91.91464  ]
 [ -0.7774601]
 [ -3.3178337]]
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 3. 0. 3.] 
cards in discard: [29.  0.  0.  0.  0.  0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
action values: 0 
buys: 1 
player value: 2 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 3.] 
adversary cards in discard: [0. 0. 0. 0. 3.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -4.515507698059082



buy possibilites: [-1] 
expected returns: [[-10.956625]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 3. 0. 3.] 
cards in discard: [29.  0.  0.  0.  0.  0.  3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3] -> size -> 12 
action values: 0 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 3.] 
adversary cards in discard: [0. 0. 0. 0. 3.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0 16  0] 
sum of rewards: 41 

action type: buy - action 3.0
Learning step: 0
desired expected reward: 3.1992692947387695






Player: 1 
cards in hand: [0. 0. 3. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3. 0. 3.] 
cards in discard: [0. 0. 0. 0. 3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [29.  3.  3.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3] -> size -> 12 
adversary victory points: 4
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 11.0 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 3.] 
cards in discard: [0. 0. 0. 0. 3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 0 
buys: 1 
player value: 3 
card supply: [30. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [29.  3.  3.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3] -> size -> 12 
adversary victory points: 4
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 3.] 
cards in discard: [ 0.  0.  0.  0.  3. 11.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11] -> size -> 11 
action values: 0 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 29. 30.  8. 10. 10.  9. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [29.  3.  3.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3] -> size -> 12 
adversary victory points: 4
player victory points: 3 





         -------------------- Turn: 3 -------------------- 
Player: 0 
cards in hand: [29.  3.  3.  0.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[-10.059698]
 [ 14.297569]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  3.  3.  0.  3.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3] -> size -> 12 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 29. 30.  8. 10. 10.  9. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11] -> size -> 11 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: -10.956624984741211



action possibilites: [-1.] 
expected returns: [[-12.162647]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 3. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 6 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3] -> size -> 12 
action values: 1 
buys: 0 
player value: 1 
card supply: [30. 30. 30. 29. 30.  8. 10. 10.  9. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11] -> size -> 11 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 11.499101638793945





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[ -10.090979 ]
 [   4.098097 ]
 [  -6.2491455]
 [-114.73062  ]
 [   0.5104642]
 [ -10.495623 ]
 [  -2.6673694]
 [ -11.874191 ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 6 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3] -> size -> 12 
action values: 0 
buys: 1 
player value: 3 
card supply: [30. 30. 30. 29. 30.  8. 10. 10.  9. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11] -> size -> 11 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -12.162647247314453



buy possibilites: [-1] 
expected returns: [[-4.797538]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 0. 3. 0.] 
cards in discard: [1.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1] -> size -> 13 
action values: 0 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 29. 30.  8. 10. 10.  9. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11] -> size -> 11 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0 54  0] 
sum of rewards: 99 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 4.098100662231445






Player: 1 
cards in hand: [0. 0. 0. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11] -> size -> 11 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 29. 30.  8. 10. 10.  9. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 0. 0.] 
adversary cards in discard: [ 1. 29.  3.  3.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1] -> size -> 13 
adversary victory points: 4
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
Chosen buy action: 14.0 : ['Militia' '14' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11] -> size -> 11 
action values: 0 
buys: 1 
player value: 5 
card supply: [30. 29. 30. 29. 30.  8. 10. 10.  9. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 0. 0.] 
adversary cards in discard: [ 1. 29.  3.  3.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1] -> size -> 13 
adversary victory points: 4
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [14.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14] -> size -> 12 
action values: 0 
buys: 0 
player value: 1 
card supply: [30. 29. 30. 29. 30.  8. 10. 10.  9. 10. 10.  9.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 0. 0.] 
adversary cards in discard: [ 1. 29.  3.  3.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1] -> size -> 13 
adversary victory points: 4
player victory points: 3 





         -------------------- Turn: 4 -------------------- 
Player: 0 
cards in hand: [0. 3. 0. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[-23.101107]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [ 1. 29.  3.  3.  0.  3.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1] -> size -> 13 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 29. 30.  8. 10. 10.  9. 10. 10.  9.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 3.  0.  3. 11.  3.] 
adversary cards in discard: [14.  0.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14] -> size -> 12 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: -4.797537803649902





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
expected returns: [[-17.333601 ]
 [ -2.6960595]
 [-13.476355 ]
 [-89.46545  ]
 [ -8.399    ]
 [ -6.4872255]
 [-17.691988 ]
 [  5.765797 ]
 [ -7.3423343]
 [ -9.775625 ]
 [ -2.3463051]
 [-20.143509 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [ 1. 29.  3.  3.  0.  3.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1] -> size -> 13 
action values: 0 
buys: 1 
player value: 4 
card supply: [30. 29. 30. 29. 30.  8. 10. 10.  9. 10. 10.  9.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 3.  0.  3. 11.  3.] 
adversary cards in discard: [14.  0.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14] -> size -> 12 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -22.980451583862305



buy possibilites: [-1] 
expected returns: [[-13.203165]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [ 1. 29.  3.  3.  0.  3.  0. 29.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29] -> size -> 14 
action values: 0 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 29. 30.  8. 10. 10.  9. 10. 10.  8.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 3.  0.  3. 11.  3.] 
adversary cards in discard: [14.  0.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14] -> size -> 12 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  30   0   0   0   0   0   0   0   0   0   0 128   0] 
sum of rewards: 153 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 5.76580286026001






Player: 1 
cards in hand: [ 3.  0.  3. 11.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0.  3. 11.  3.] 
cards in discard: [14.  0.  0.  0.  0.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14] -> size -> 12 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 29. 30.  8. 10. 10.  9. 10. 10.  8.  9. 10. 10. 10. 10.] 
adversary cards in hand: [29.  0.  3.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29] -> size -> 14 
adversary victory points: 4
player victory points: 3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 3. 3.] 
cards in discard: [14.  0.  0.  0.  0.  0.  6.] 
cards in deck: 1 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6] -> size -> 13 
action values: 0 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  8.  9. 10. 10. 10. 10.] 
adversary cards in hand: [29.  0.  3.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29] -> size -> 14 
adversary victory points: 4
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 3. 3.] 
cards in discard: [14.  0.  0.  0.  0.  0.  6.] 
cards in deck: 1 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6] -> size -> 13 
action values: 0 
buys: 1 
player value: 1 
card supply: [30. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  8.  9. 10. 10. 10. 10.] 
adversary cards in hand: [29.  0.  3.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29] -> size -> 14 
adversary victory points: 4
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 3. 3.] 
cards in discard: [14.  0.  0.  0.  0.  0.  6.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0] -> size -> 14 
action values: 0 
buys: 0 
player value: 1 
card supply: [29. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  8.  9. 10. 10. 10. 10.] 
adversary cards in hand: [29.  0.  3.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29] -> size -> 14 
adversary victory points: 4
player victory points: 2 





         -------------------- Turn: 5 -------------------- 
Player: 0 
cards in hand: [29.  0.  3.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[-14.051395 ]
 [  1.0963879]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0.  3.  0.  0.] 
cards in discard: [] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29] -> size -> 14 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  8.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0] -> size -> 14 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: -13.203165054321289



action possibilites: [-1.] 
expected returns: [[-0.42082787]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 8 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29] -> size -> 14 
action values: 1 
buys: 0 
player value: 1 
card supply: [29. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  8.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0] -> size -> 14 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: -4.524951457977295





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[   2.3031592]
 [  17.485924 ]
 [   6.287028 ]
 [ -43.193874 ]
 [-112.49685  ]
 [  11.812658 ]
 [  13.617102 ]
 [   2.1514916]
 [  18.940538 ]
 [  25.864697 ]
 [  12.807421 ]
 [  18.447943 ]
 [  10.349497 ]
 [  11.7365265]
 [  17.696693 ]
 [   0.6863165]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 8 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29] -> size -> 14 
action values: 0 
buys: 1 
player value: 5 
card supply: [29. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  8.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0] -> size -> 14 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -0.42082786560058594



buy possibilites: [-1] 
expected returns: [[-25.55678]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [29.] 
cards in deck: 8 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29] -> size -> 15 
action values: 0 
buys: 0 
player value: 1 
card supply: [29. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  7.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0] -> size -> 14 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5.  0.  0. 60.  0.  0. 20.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 107.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 25.864686965942383






Player: 1 
cards in hand: [0. 0. 0. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0] -> size -> 14 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  7.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  0. 29.  0.  3.] 
adversary cards in discard: [29. 29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29] -> size -> 15 
adversary victory points: 4
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0] -> size -> 14 
action values: 0 
buys: 1 
player value: 4 
card supply: [29. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  7.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  0. 29.  0.  3.] 
adversary cards in discard: [29. 29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29] -> size -> 15 
adversary victory points: 4
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3. 0.] 
cards in discard: [0.] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0] -> size -> 15 
action values: 0 
buys: 0 
player value: 4 
card supply: [28. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  7.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  0. 29.  0.  3.] 
adversary cards in discard: [29. 29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29] -> size -> 15 
adversary victory points: 4
player victory points: 2 





         -------------------- Turn: 6 -------------------- 
Player: 0 
cards in hand: [ 0.  0. 29.  0.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[-13.529589]
 [  9.834522]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0. 29.  0.  3.] 
cards in discard: [29. 29.  0.  3.  0.  0.  0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29] -> size -> 15 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  7.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  3. 11.  0.  6.] 
adversary cards in discard: [0. 0. 0. 0. 3. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0] -> size -> 15 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: -25.556779861450195



action possibilites: [-1.] 
expected returns: [[-1.4073339]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 3. 3.] 
cards in discard: [29. 29.  0.  3.  0.  0.  0.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29] -> size -> 15 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  7.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  3. 11.  0.  6.] 
adversary cards in discard: [0. 0. 0. 0. 3. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0] -> size -> 15 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 9.504268646240234





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
expected returns: [[  2.6545997 ]
 [ 16.855753  ]
 [  6.273417  ]
 [-91.81433   ]
 [ 11.364639  ]
 [ 13.103378  ]
 [  2.3660011 ]
 [ 24.953266  ]
 [ 12.297424  ]
 [  9.971621  ]
 [ 17.089773  ]
 [  0.23190975]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3. 3.] 
cards in discard: [29. 29.  0.  3.  0.  0.  0.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29] -> size -> 15 
action values: 0 
buys: 1 
player value: 4 
card supply: [28. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  7.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  3. 11.  0.  6.] 
adversary cards in discard: [0. 0. 0. 0. 3. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0] -> size -> 15 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -1.4073338508605957



buy possibilites: [-1] 
expected returns: [[3.5085044]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3. 3.] 
cards in discard: [29. 29.  0.  3.  0.  0.  0. 29.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29] -> size -> 16 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  6.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  3. 11.  0.  6.] 
adversary cards in discard: [0. 0. 0. 0. 3. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0] -> size -> 15 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  60   0   0  20   0   0   0   0   0   0   0 128   0] 
sum of rewards: 203 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 24.953256607055664






Player: 1 
cards in hand: [ 0.  3. 11.  0.  6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3. 11.  0.  6.] 
cards in discard: [0. 0. 0. 0. 3. 0.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0] -> size -> 15 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  6.  9. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 3. 1. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29] -> size -> 16 
adversary victory points: 4
player victory points: 2 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 0. 6.] 
cards in discard: [ 0.  0.  0.  0.  3.  0. 14.] 
cards in deck: 4 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14] -> size -> 16 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  6.  8. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 3. 1. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29] -> size -> 16 
adversary victory points: 4
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 6.] 
cards in discard: [ 0.  0.  0.  0.  3.  0. 14.] 
cards in deck: 4 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14] -> size -> 16 
action values: 0 
buys: 1 
player value: 2 
card supply: [28. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  6.  8. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 3. 1. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29] -> size -> 16 
adversary victory points: 4
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 6.] 
cards in discard: [ 0.  0.  0.  0.  3.  0. 14.  0.] 
cards in deck: 4 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0] -> size -> 17 
action values: 0 
buys: 0 
player value: 2 
card supply: [27. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  6.  8. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 3. 1. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29] -> size -> 16 
adversary victory points: 4
player victory points: 2 





         -------------------- Turn: 7 -------------------- 
Player: 0 
cards in hand: [3. 0. 3. 1. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[11.313606]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 3. 1. 3.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29] -> size -> 16 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  6.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  0. 14.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0] -> size -> 17 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: 3.5085043907165527





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[  13.086493]
 [  27.411476]
 [  17.486156]
 [-103.360695]
 [  24.391867]
 [  12.294865]
 [  21.06957 ]
 [  12.119623]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 3. 1. 3.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29] -> size -> 16 
action values: 0 
buys: 1 
player value: 3 
card supply: [27. 29. 30. 29. 30.  8.  9. 10.  9. 10. 10.  6.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  0. 14.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0] -> size -> 17 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 7.759546756744385



buy possibilites: [-1] 
expected returns: [[-15.254202]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 3. 1. 3.] 
cards in discard: [1.] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1] -> size -> 17 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9. 10. 10.  6.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  0. 14.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0] -> size -> 17 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0 54  0] 
sum of rewards: 109 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 27.411481857299805






Player: 1 
cards in hand: [ 0.  0. 14.  0.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 14.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0. 14.  0.  3.] 
cards in discard: [] 
cards in deck: 12 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0] -> size -> 17 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9. 10. 10.  6.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  0.  0. 29.  0.] 
adversary cards in discard: [1. 3. 0. 3. 1. 3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1] -> size -> 17 
adversary victory points: 4
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0. 14.  0.  3.] 
cards in discard: [] 
cards in deck: 12 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0] -> size -> 17 
action values: 0 
buys: 1 
player value: 3 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9. 10. 10.  6.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  0.  0. 29.  0.] 
adversary cards in discard: [1. 3. 0. 3. 1. 3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1] -> size -> 17 
adversary victory points: 4
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0. 14.  0.  3.] 
cards in discard: [8.] 
cards in deck: 12 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8] -> size -> 18 
action values: 0 
buys: 0 
player value: 1 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9.  9. 10.  6.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  0.  0. 29.  0.] 
adversary cards in discard: [1. 3. 0. 3. 1. 3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1] -> size -> 17 
adversary victory points: 4
player victory points: 2 





         -------------------- Turn: 8 -------------------- 
Player: 0 
cards in hand: [ 0.  0.  0. 29.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[-18.372051 ]
 [  5.7219014]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  0. 29.  0.] 
cards in discard: [1. 3. 0. 3. 1. 3.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1] -> size -> 17 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9.  9. 10.  6.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 14. 11.  0.  3.] 
adversary cards in discard: [ 8.  0.  0. 14.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8] -> size -> 18 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: -15.254201889038086



action possibilites: [-1. 29.] 
expected returns: [[-11.249594]
 [ 13.0886  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  0.  0. 29.] 
cards in discard: [1. 3. 0. 3. 1. 3.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1] -> size -> 17 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9.  9. 10.  6.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 14. 11.  0.  3.] 
adversary cards in discard: [ 8.  0.  0. 14.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8] -> size -> 18 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 4.223185062408447



action possibilites: [-1. 29.] 
expected returns: [[-8.727217]
 [16.440674]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  0.  0. 29.] 
cards in discard: [1. 3. 0. 3. 1. 3.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1] -> size -> 17 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9.  9. 10.  6.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 14. 11.  0.  3.] 
adversary cards in discard: [ 8.  0.  0. 14.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8] -> size -> 18 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 95 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 13.08860969543457



action possibilites: [-1. 29.] 
expected returns: [[-0.6401763]
 [23.523811 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  0.  0. 29.] 
cards in discard: [1. 3. 0. 3. 1. 3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1] -> size -> 17 
action values: 1 
buys: 0 
player value: 3 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9.  9. 10.  6.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 14. 11.  0.  3.] 
adversary cards in discard: [ 8.  0.  0. 14.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8] -> size -> 18 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 115 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 16.44066047668457



action possibilites: [-1.] 
expected returns: [[4.046728]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [1. 3. 0. 3. 1. 3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1] -> size -> 17 
action values: 1 
buys: 0 
player value: 4 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9.  9. 10.  6.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 14. 11.  0.  3.] 
adversary cards in discard: [ 8.  0.  0. 14.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8] -> size -> 18 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 135 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 23.5238094329834





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  5.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[   5.7974334]
 [  24.357779 ]
 [ -43.311337 ]
 [   8.367435 ]
 [ -45.897625 ]
 [  12.61043  ]
 [-108.878784 ]
 [  18.103865 ]
 [  15.933283 ]
 [   2.84061  ]
 [  23.578943 ]
 [  31.923822 ]
 [  15.709948 ]
 [  23.246836 ]
 [  13.22155  ]
 [  15.962366 ]
 [  20.541634 ]
 [   8.629021 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [1. 3. 0. 3. 1. 3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1] -> size -> 17 
action values: 0 
buys: 1 
player value: 8 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9.  9. 10.  6.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 14. 11.  0.  3.] 
adversary cards in discard: [ 8.  0.  0. 14.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8] -> size -> 18 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 135 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 4.046728134155273



buy possibilites: [-1] 
expected returns: [[-8.216066]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [ 1.  3.  0.  3.  1.  3. 29.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29] -> size -> 18 
action values: 0 
buys: 0 
player value: 4 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9.  9. 10.  5.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 14. 11.  0.  3.] 
adversary cards in discard: [ 8.  0.  0. 14.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8] -> size -> 18 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5.  0.  0. 60.  0.  0. 80.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 167.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 31.923818588256836






Player: 1 
cards in hand: [ 0. 14. 11.  0.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 14. 11.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 14. 11.  0.  3.] 
cards in discard: [ 8.  0.  0. 14.  0.  3.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8] -> size -> 18 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9.  9. 10.  5.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 3. 29.  0.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29] -> size -> 18 
adversary victory points: 4
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 14. 11.  0.  3.] 
cards in discard: [ 8.  0.  0. 14.  0.  3.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8] -> size -> 18 
action values: 0 
buys: 1 
player value: 2 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9.  9. 10.  5.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 3. 29.  0.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29] -> size -> 18 
adversary victory points: 4
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 14. 11.  0.  3.] 
cards in discard: [ 8.  0.  0. 14.  0.  3.  8.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8] -> size -> 19 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9.  8. 10.  5.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 3. 29.  0.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29] -> size -> 18 
adversary victory points: 4
player victory points: 2 





         -------------------- Turn: 9 -------------------- 
Player: 0 
cards in hand: [ 3. 29.  0.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[-17.84754  ]
 [  6.5113683]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29.  0.  0.  0.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29] -> size -> 18 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9.  8. 10.  5.  8. 10. 10. 10. 10.] 
adversary cards in hand: [0. 6. 0. 0. 0.] 
adversary cards in discard: [ 8.  0.  0. 14.  0.  3.  8.  0. 14. 11.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8] -> size -> 19 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: -8.216066360473633



action possibilites: [-1.] 
expected returns: [[17.24455]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 12 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29] -> size -> 18 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9.  8. 10.  5.  8. 10. 10. 10. 10.] 
adversary cards in hand: [0. 6. 0. 0. 0.] 
adversary cards in discard: [ 8.  0.  0. 14.  0.  3.  8.  0. 14. 11.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8] -> size -> 19 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 3.701913356781006





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
expected returns: [[ 20.367403]
 [ 34.64308 ]
 [ 24.064707]
 [-97.649506]
 [ 29.527712]
 [ 31.479635]
 [ 17.18289 ]
 [ 42.522575]
 [ 30.690947]
 [ 28.232483]
 [ 35.268734]
 [ 18.88393 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 12 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29] -> size -> 18 
action values: 0 
buys: 1 
player value: 4 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9.  8. 10.  5.  8. 10. 10. 10. 10.] 
adversary cards in hand: [0. 6. 0. 0. 0.] 
adversary cards in discard: [ 8.  0.  0. 14.  0.  3.  8.  0. 14. 11.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8] -> size -> 19 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 17.244550704956055



buy possibilites: [-1] 
expected returns: [[-11.34532]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 3.] 
cards in discard: [29.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29] -> size -> 19 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9.  8. 10.  4.  8. 10. 10. 10. 10.] 
adversary cards in hand: [0. 6. 0. 0. 0.] 
adversary cards in discard: [ 8.  0.  0. 14.  0.  3.  8.  0. 14. 11.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8] -> size -> 19 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  60   0   0  20   0   0   0   0   0   0   0 128   0] 
sum of rewards: 203 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 42.522579193115234






Player: 1 
cards in hand: [0. 6. 0. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 6. 0. 0. 0.] 
cards in discard: [ 8.  0.  0. 14.  0.  3.  8.  0. 14. 11.  0.  3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8] -> size -> 19 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9.  8. 10.  4.  8. 10. 10. 10. 10.] 
adversary cards in hand: [29. 29.  3.  3.  1.] 
adversary cards in discard: [29. 29.  3.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29] -> size -> 19 
adversary victory points: 4
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 6. 0. 0. 0.] 
cards in discard: [ 8.  0.  0. 14.  0.  3.  8.  0. 14. 11.  0.  3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8] -> size -> 19 
action values: 0 
buys: 1 
player value: 4 
card supply: [27. 28. 30. 29. 30.  8.  9. 10.  9.  8. 10.  4.  8. 10. 10. 10. 10.] 
adversary cards in hand: [29. 29.  3.  3.  1.] 
adversary cards in discard: [29. 29.  3.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29] -> size -> 19 
adversary victory points: 4
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 6. 0. 0. 0.] 
cards in discard: [ 8.  0.  0. 14.  0.  3.  8.  0. 14. 11.  0.  3.  1.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1] -> size -> 20 
action values: 0 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8.  9. 10.  9.  8. 10.  4.  8. 10. 10. 10. 10.] 
adversary cards in hand: [29. 29.  3.  3.  1.] 
adversary cards in discard: [29. 29.  3.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29] -> size -> 19 
adversary victory points: 4
player victory points: 2 





         -------------------- Turn: 10 -------------------- 
Player: 0 
cards in hand: [29. 29.  3.  3.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[15.972219]
 [38.848732]
 [38.848732]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29.  3.  3.  1.] 
cards in discard: [29. 29.  3.  0.  0.  0.  3.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29] -> size -> 19 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  9. 10.  9.  8. 10.  4.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 1.  0. 14.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: -11.345319747924805



action possibilites: [-1. 29.] 
expected returns: [[21.64071 ]
 [44.462616]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  3.  3.  1.  1.] 
cards in discard: [29. 29.  3.  0.  0.  0.  3.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29] -> size -> 19 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8.  9. 10.  9.  8. 10.  4.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 1.  0. 14.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 36.47665023803711



action possibilites: [-1. 29.] 
expected returns: [[15.366232]
 [38.711746]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  3.  1.  1. 29.] 
cards in discard: [29. 29.  3.  0.  0.  0.  3.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29] -> size -> 19 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8.  9. 10.  9.  8. 10.  4.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 1.  0. 14.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 95 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 44.46261978149414



action possibilites: [-1.] 
expected returns: [[28.768251]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 3. 1. 1. 0.] 
cards in discard: [29. 29.  3.  0.  0.  0.  3.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29] -> size -> 19 
action values: 1 
buys: 0 
player value: 3 
card supply: [27. 27. 30. 29. 30.  8.  9. 10.  9.  8. 10.  4.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 1.  0. 14.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 115 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 38.71174240112305





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  5.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 36.128056  ]
 [ 49.614174  ]
 [  2.1173568 ]
 [ 39.527527  ]
 [ -0.28337312]
 [ 41.475616  ]
 [-54.56915   ]
 [ 44.640575  ]
 [ 46.36466   ]
 [ 36.232006  ]
 [ 50.993477  ]
 [ 57.019352  ]
 [ 45.609234  ]
 [ 50.55366   ]
 [ 43.417873  ]
 [ 44.60178   ]
 [ 49.97641   ]
 [ 34.30033   ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 1. 1. 0.] 
cards in discard: [29. 29.  3.  0.  0.  0.  3.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29] -> size -> 19 
action values: 0 
buys: 1 
player value: 8 
card supply: [27. 27. 30. 29. 30.  8.  9. 10.  9.  8. 10.  4.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 1.  0. 14.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 115 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 28.768251419067383



buy possibilites: [-1] 
expected returns: [[59.951862]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 1. 1. 0.] 
cards in discard: [29. 29.  3.  0.  0.  0.  3. 29.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29] -> size -> 20 
action values: 0 
buys: 0 
player value: 4 
card supply: [27. 27. 30. 29. 30.  8.  9. 10.  9.  8. 10.  3.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 1.  0. 14.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5.  0.  0. 60.  0.  0. 60.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 147.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 57.019371032714844






Player: 1 
cards in hand: [ 1.  0. 14.  0.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 14.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  0. 14.  0.  3.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1] -> size -> 20 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  9. 10.  9.  8. 10.  3.  8. 10. 10. 10. 10.] 
adversary cards in hand: [29. 29.  0.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29] -> size -> 20 
adversary victory points: 4
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: 16.0 : ['Remodel' '16' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  0. 14.  0.  3.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1] -> size -> 20 
action values: 0 
buys: 1 
player value: 4 
card supply: [27. 27. 30. 29. 30.  8.  9. 10.  9.  8. 10.  3.  8. 10. 10. 10. 10.] 
adversary cards in hand: [29. 29.  0.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29] -> size -> 20 
adversary victory points: 4
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  0. 14.  0.  3.] 
cards in discard: [16.] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1 16] -> size -> 21 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  3.  8. 10. 10. 10. 10.] 
adversary cards in hand: [29. 29.  0.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29] -> size -> 20 
adversary victory points: 4
player victory points: 2 





         -------------------- Turn: 11 -------------------- 
Player: 0 
cards in hand: [29. 29.  0.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[-31.270157]
 [ -9.54217 ]
 [ -9.54217 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29.  0.  0.  0.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29] -> size -> 20 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  3.  8. 10. 10. 10. 10.] 
adversary cards in hand: [8. 0. 0. 3. 0.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1 16] -> size -> 21 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: 59.95186233520508



action possibilites: [-1. 29.] 
expected returns: [[ 0.4661932]
 [22.69932  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0.  0.  0.  0.] 
cards in discard: [] 
cards in deck: 14 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29] -> size -> 20 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  3.  8. 10. 10. 10. 10.] 
adversary cards in hand: [8. 0. 0. 3. 0.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1 16] -> size -> 21 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: -12.0870361328125



action possibilites: [-1.] 
expected returns: [[-3.7954953]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29] -> size -> 20 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  3.  8. 10. 10. 10. 10.] 
adversary cards in hand: [8. 0. 0. 3. 0.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1 16] -> size -> 21 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 95 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 22.699312210083008





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ -1.3924224]
 [ 12.581177 ]
 [-34.074444 ]
 [  2.743617 ]
 [-36.609024 ]
 [-86.065445 ]
 [  7.7675624]
 [  9.4934845]
 [ -2.1014862]
 [ 13.951187 ]
 [ 19.775324 ]
 [  8.752594 ]
 [ 13.522472 ]
 [  6.3059535]
 [  7.757343 ]
 [ 12.989449 ]
 [ -2.3386464]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29] -> size -> 20 
action values: 0 
buys: 1 
player value: 7 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  3.  8. 10. 10. 10. 10.] 
adversary cards in hand: [8. 0. 0. 3. 0.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1 16] -> size -> 21 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 95 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -3.7954952716827393



buy possibilites: [-1] 
expected returns: [[2.7660804]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [29.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29] -> size -> 21 
action values: 0 
buys: 0 
player value: 3 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  2.  8. 10. 10. 10. 10.] 
adversary cards in hand: [8. 0. 0. 3. 0.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1 16] -> size -> 21 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5.  0.  0. 60.  0.  0. 40.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 127.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 19.775325775146484






Player: 1 
cards in hand: [8. 0. 0. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [8. 0. 0. 3. 0.] 
cards in discard: [16.  1.  0. 14.  0.  3.] 
cards in deck: 10 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1 16] -> size -> 21 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  2.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  3.  0. 29. 29.] 
adversary cards in discard: [29. 29. 29.  0.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29] -> size -> 21 
adversary victory points: 4
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 10.0 : ['Village' '10' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [8. 0. 0. 3. 0.] 
cards in discard: [16.  1.  0. 14.  0.  3.] 
cards in deck: 10 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1 16] -> size -> 21 
action values: 0 
buys: 1 
player value: 3 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  2.  8. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  3.  0. 29. 29.] 
adversary cards in discard: [29. 29. 29.  0.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29] -> size -> 21 
adversary victory points: 4
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [8. 0. 0. 3. 0.] 
cards in discard: [16.  1.  0. 14.  0.  3. 10.] 
cards in deck: 10 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1 16 10] -> size -> 22 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  2.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  3.  0. 29. 29.] 
adversary cards in discard: [29. 29. 29.  0.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29] -> size -> 21 
adversary victory points: 4
player victory points: 2 





         -------------------- Turn: 12 -------------------- 
Player: 0 
cards in hand: [ 0.  3.  0. 29. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[21.345434]
 [43.71367 ]
 [43.71367 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3.  0. 29. 29.] 
cards in discard: [29. 29. 29.  0.  0.  0.  0.  0.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29] -> size -> 21 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  2.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  8. 14.  0.  6.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1 16 10] -> size -> 22 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: 2.766080379486084



action possibilites: [-1. 29.] 
expected returns: [[22.625708]
 [45.002274]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3.  0. 29.  1.] 
cards in discard: [29. 29. 29.  0.  0.  0.  0.  0.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29] -> size -> 21 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  2.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  8. 14.  0.  6.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1 16 10] -> size -> 22 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 40.023155212402344



action possibilites: [-1.] 
expected returns: [[35.544743]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 0. 1. 3.] 
cards in discard: [29. 29. 29.  0.  0.  0.  0.  0.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29] -> size -> 21 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  2.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  8. 14.  0.  6.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1 16 10] -> size -> 22 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 95 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 45.002281188964844





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 40.32485  ]
 [ 53.79762  ]
 [  6.3950515]
 [ 43.7628   ]
 [  3.703669 ]
 [-46.165203 ]
 [ 48.974854 ]
 [ 50.502922 ]
 [ 40.182602 ]
 [ 55.047073 ]
 [ 60.910004 ]
 [ 49.794888 ]
 [ 54.628223 ]
 [ 47.59221  ]
 [ 48.868675 ]
 [ 54.004116 ]
 [ 39.195507 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 1. 3.] 
cards in discard: [29. 29. 29.  0.  0.  0.  0.  0.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29] -> size -> 21 
action values: 0 
buys: 1 
player value: 6 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  2.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  8. 14.  0.  6.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1 16 10] -> size -> 22 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 95 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 35.544742584228516



buy possibilites: [-1] 
expected returns: [[65.46622]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 1. 3.] 
cards in discard: [29. 29. 29.  0.  0.  0.  0.  0. 29.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
action values: 0 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  1.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  8. 14.  0.  6.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1 16 10] -> size -> 22 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5.  0.  0. 60.  0.  0. 40.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 127.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 60.91002655029297






Player: 1 
cards in hand: [ 0.  8. 14.  0.  6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8. 14.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  8. 14.  0.  6.] 
cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 11 14  6  0  0 14  0  8  8  1 16 10] -> size -> 22 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  1.  8. 10.  9. 10. 10.] 
adversary cards in hand: [29. 29.  1.  3.  3.] 
adversary cards in discard: [29. 29. 29.  0.  0.  0.  0.  0. 29. 29. 29.  0.  3.  0.  1.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
adversary victory points: 4
player victory points: 2 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 6.] 
cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.] 
cards in deck: 5 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  1.  8. 10.  9. 10. 10.] 
adversary cards in hand: [29. 29.  1.  3.  3.] 
adversary cards in discard: [29. 29. 29.  0.  0.  0.  0.  0. 29. 29. 29.  0.  3.  0.  1.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
adversary victory points: 4
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 6.] 
cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.] 
cards in deck: 5 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
action values: 0 
buys: 1 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  1.  8. 10.  9. 10. 10.] 
adversary cards in hand: [29. 29.  1.  3.  3.] 
adversary cards in discard: [29. 29. 29.  0.  0.  0.  0.  0. 29. 29. 29.  0.  3.  0.  1.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
adversary victory points: 4
player victory points: 2 





         -------------------- Turn: 13 -------------------- 
Player: 0 
cards in hand: [29. 29.  1.  3.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[53.529594]
 [70.13201 ]
 [70.13201 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29.  1.  3.  3.] 
cards in discard: [29. 29. 29.  0.  0.  0.  0.  0. 29. 29. 29.  0.  3.  0.  1.  3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  1.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  0.  0. 11.  3.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.  8.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: 65.46621704101562



action possibilites: [-1. 29. 29.] 
expected returns: [[68.13685 ]
 [84.304016]
 [84.304016]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  1.  3.  3. 29.] 
cards in discard: [29. 29. 29.  0.  0.  0.  0.  0. 29. 29. 29.  0.  3.  0.  1.  3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  1.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  0.  0. 11.  3.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.  8.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 69.76033782958984



action possibilites: [-1. 29. 29.] 
expected returns: [[20.373018]
 [42.918724]
 [42.918724]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  3.  3. 29. 29.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  1.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  0.  0. 11.  3.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.  8.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 95 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 84.30400848388672



action possibilites: [-1. 29. 29.] 
expected returns: [[35.825134]
 [57.605064]
 [57.605064]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  3.  3. 29. 29.] 
cards in discard: [] 
cards in deck: 14 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 3 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  1.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  0.  0. 11.  3.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.  8.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 115 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 42.918724060058594



action possibilites: [-1. 29.] 
expected returns: [[30.782324]
 [53.735397]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  3.  3. 29.  0.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 4 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  1.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  0.  0. 11.  3.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.  8.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 135 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 57.60507583618164



action possibilites: [-1.] 
expected returns: [[36.561897]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 3. 3. 0. 0.] 
cards in discard: [] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 5 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  1.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  0.  0. 11.  3.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.  8.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  60   0   0 100   0   0   0   0   0   0   0   0   0] 
sum of rewards: 155 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 53.73542022705078





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  5.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 40.61798  ]
 [ 54.23259  ]
 [  7.6788135]
 [ 44.46243  ]
 [  5.3177276]
 [ 46.188736 ]
 [-49.72187  ]
 [ 49.427746 ]
 [ 51.261124 ]
 [ 40.360054 ]
 [ 55.667824 ]
 [ 61.466072 ]
 [ 50.502747 ]
 [ 55.234024 ]
 [ 48.172863 ]
 [ 49.470303 ]
 [ 54.751488 ]
 [ 39.18043  ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 3. 3. 0. 0.] 
cards in discard: [] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
action values: 0 
buys: 1 
player value: 9 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  1.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  0.  0. 11.  3.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.  8.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  60   0   0 100   0   0   0   0   0   0   0   0   0] 
sum of rewards: 155 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 36.56189727783203



buy possibilites: [-1] 
expected returns: [[46.116978]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 3. 3. 0. 0.] 
cards in discard: [29.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29] -> size -> 23 
action values: 0 
buys: 0 
player value: 5 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  0.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  0.  0. 11.  3.] 
adversary cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.  8.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[ -5.   0.   0.  60.   0.   0. 100.   0.   0.   0.   0.   0.   0.   0.
  32.   0.] 
sum of rewards: 187.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 61.46607971191406






Player: 1 
cards in hand: [ 0.  0.  0. 11.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  0. 11.  3.] 
cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.  8.  0.  6.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  0.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 3.  0. 29. 29.  0.] 
adversary cards in discard: [29. 29. 29. 29. 29. 29.  1.  3.  3.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29] -> size -> 23 
adversary victory points: 4
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0.  0. 11.  3.] 
cards in discard: [16.  1.  0. 14.  0.  3. 10.  8.  0.  0.  3.  0.  8.  0.  6.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
action values: 0 
buys: 1 
player value: 3 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  0.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 3.  0. 29. 29.  0.] 
adversary cards in discard: [29. 29. 29. 29. 29. 29.  1.  3.  3.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29] -> size -> 23 
adversary victory points: 4
player victory points: 2 


 --------- NOTHING HAPPENED --------- 



         -------------------- Turn: 14 -------------------- 
Player: 0 
cards in hand: [ 3.  0. 29. 29.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[28.469595]
 [48.709396]
 [48.709396]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 29. 29.  0.] 
cards in discard: [29. 29. 29. 29. 29. 29.  1.  3.  3.  0.  0.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29] -> size -> 23 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  0.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 3. 10.  0. 14.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: 46.11697769165039



action possibilites: [-1.] 
expected returns: [[133.61797]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 1.] 
cards in discard: [29. 29. 29. 29. 29. 29.  1.  3.  3.  0.  0. 29.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29] -> size -> 23 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  0.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 3. 10.  0. 14.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 40.130252838134766





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[130.77628 ]
 [143.86154 ]
 [134.46669 ]
 [ 93.95385 ]
 [ 53.080784]
 [139.71234 ]
 [141.01051 ]
 [130.23906 ]
 [144.93442 ]
 [140.40108 ]
 [144.57349 ]
 [138.16576 ]
 [139.60492 ]
 [144.03125 ]
 [132.21022 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 1.] 
cards in discard: [29. 29. 29. 29. 29. 29.  1.  3.  3.  0.  0. 29.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29] -> size -> 23 
action values: 0 
buys: 1 
player value: 5 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8. 10.  0.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 3. 10.  0. 14.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 133.6179656982422



buy possibilites: [-1] 
expected returns: [[155.91911]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 1.] 
cards in discard: [29. 29. 29. 29. 29. 29.  1.  3.  3.  0.  0. 29. 25.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25] -> size -> 24 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8.  9.  0.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 3. 10.  0. 14.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  60   0   0  20   0   0   0   0   0   0   0 250   0] 
sum of rewards: 325 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 144.9344024658203






Player: 1 
cards in hand: [ 3. 10.  0. 14.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 10. 14.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 10.  0. 14.  0.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8.  9.  0.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 3. 29. 29.  0.  0.] 
adversary cards in discard: [29. 29. 29. 29. 29. 29.  1.  3.  3.  0.  0. 29. 25. 29.  3.  0.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25] -> size -> 24 
adversary victory points: 4
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 10.  0. 14.  0.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10] -> size -> 20 
action values: 0 
buys: 1 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8.  9.  9.  9.  8.  9.  0.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 3. 29. 29.  0.  0.] 
adversary cards in discard: [29. 29. 29. 29. 29. 29.  1.  3.  3.  0.  0. 29. 25. 29.  3.  0.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25] -> size -> 24 
adversary victory points: 4
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 10.  0. 14.  0.] 
cards in discard: [3.] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10  3] -> size -> 21 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  9.  0.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 3. 29. 29.  0.  0.] 
adversary cards in discard: [29. 29. 29. 29. 29. 29.  1.  3.  3.  0.  0. 29. 25. 29.  3.  0.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25] -> size -> 24 
adversary victory points: 4
player victory points: 3 





         -------------------- Turn: 15 -------------------- 
Player: 0 
cards in hand: [ 3. 29. 29.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[-26.592016]
 [ -8.280562]
 [ -8.280562]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29. 29.  0.  0.] 
cards in discard: [29. 29. 29. 29. 29. 29.  1.  3.  3.  0.  0. 29. 25. 29.  3.  0.  0.  1.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25] -> size -> 24 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  9.  0.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 8. 16.  8.  0.  6.] 
adversary cards in discard: [ 3.  3. 10.  0. 14.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10  3] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 155.9191131591797



action possibilites: [-1.] 
expected returns: [[-9.896454]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 0.] 
cards in discard: [29. 29. 29. 29. 29. 29.  1.  3.  3.  0.  0. 29. 25. 29.  3.  0.  0.  1.
 29.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25] -> size -> 24 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  9.  0.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 8. 16.  8.  0.  6.] 
adversary cards in discard: [ 3.  3. 10.  0. 14.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10  3] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -14.31589126586914





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[-10.040049  ]
 [  3.1060462 ]
 [ -5.937516  ]
 [-82.881256  ]
 [ -1.1001158 ]
 [  0.3702631 ]
 [-11.019743  ]
 [ -0.27069783]
 [ -2.631755  ]
 [  3.4248052 ]
 [ -9.866052  ]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0.] 
cards in discard: [29. 29. 29. 29. 29. 29.  1.  3.  3.  0.  0. 29. 25. 29.  3.  0.  0.  1.
 29.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25] -> size -> 24 
action values: 0 
buys: 1 
player value: 4 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  9.  0.  8. 10.  9. 10. 10.] 
adversary cards in hand: [ 8. 16.  8.  0.  6.] 
adversary cards in discard: [ 3.  3. 10.  0. 14.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10  3] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -9.896453857421875



buy possibilites: [-1] 
expected returns: [[-1.9911873]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0.] 
cards in discard: [29. 29. 29. 29. 29. 29.  1.  3.  3.  0.  0. 29. 25. 29.  3.  0.  0.  1.
 29. 15.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25
 15] -> size -> 25 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  9.  0.  8. 10.  9. 10.  9.] 
adversary cards in hand: [ 8. 16.  8.  0.  6.] 
adversary cards in discard: [ 3.  3. 10.  0. 14.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10  3] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  30   0   0  20   0   0   0   0   0   0   0 128   0] 
sum of rewards: 173 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 3.424787998199463






Player: 1 
cards in hand: [ 8. 16.  8.  0.  6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8. 16.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 8. 16.  8.  0.  6.] 
cards in discard: [ 3.  3. 10.  0. 14.  0.] 
cards in deck: 10 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 16 10  3] -> size -> 21 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  9.  0.  8. 10.  9. 10.  9.] 
adversary cards in hand: [ 3. 29. 29.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25
 15] -> size -> 25 
adversary victory points: 4
player victory points: 3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [8. 0. 6.] 
cards in discard: [ 3.  3. 10.  0. 14.  0.] 
cards in deck: 10 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3] -> size -> 20 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  9.  0.  8. 10.  9. 10.  9.] 
adversary cards in hand: [ 3. 29. 29.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25
 15] -> size -> 25 
adversary victory points: 4
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [8. 0. 6.] 
cards in discard: [ 3.  3. 10.  0. 14.  0.] 
cards in deck: 10 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3] -> size -> 20 
action values: 0 
buys: 1 
player value: 1 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  9.  0.  8. 10.  9. 10.  9.] 
adversary cards in hand: [ 3. 29. 29.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25
 15] -> size -> 25 
adversary victory points: 4
player victory points: 3 





         -------------------- Turn: 16 -------------------- 
Player: 0 
cards in hand: [ 3. 29. 29.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[18.736296]
 [41.184547]
 [41.184547]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29. 29.  0.  0.] 
cards in discard: [] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25
 15] -> size -> 25 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  9.  0.  8. 10.  9. 10.  9.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3] -> size -> 20 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: -1.991187334060669



action possibilites: [-1. 15.] 
expected returns: [[30.969664]
 [47.017326]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0.  0. 15.] 
cards in discard: [29.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25
 15] -> size -> 25 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  9.  0.  8. 10.  9. 10.  9.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3] -> size -> 20 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 29.457571029663086



action possibilites: [-1] 
expected returns: [[-6.67943]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0.] 
cards in discard: [29.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15] -> size -> 24 
action values: 0 
buys: 0 
player value: 4 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  9.  0.  8. 10.  9. 10.  9.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3] -> size -> 20 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 47.017330169677734





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ -5.4026246]
 [  7.9573846]
 [ -1.3497291]
 [-37.0788   ]
 [-74.82403  ]
 [  3.3770251]
 [  4.7706656]
 [ -6.6211896]
 [  9.108742 ]
 [  4.108041 ]
 [  8.714012 ]
 [  1.7347846]
 [  3.2486944]
 [  8.095459 ]
 [ -5.8116446]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0.] 
cards in discard: [29.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15] -> size -> 24 
action values: 0 
buys: 1 
player value: 5 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  9.  0.  8. 10.  9. 10.  9.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3] -> size -> 20 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action -1
Learning step: 0
desired expected reward: -6.67943000793457



buy possibilites: [-1] 
expected returns: [[2.6486955]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0.] 
cards in discard: [29. 25.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25] -> size -> 25 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  8.  0.  8. 10.  9. 10.  9.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3] -> size -> 20 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  30   0   0  40   0   0   0   0   0   0   0 250   0] 
sum of rewards: 315 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 9.108743667602539






Player: 1 
cards in hand: [0. 0. 0. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3] -> size -> 20 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  8.  0.  8. 10.  9. 10.  9.] 
adversary cards in hand: [ 3. 29.  1. 29.  3.] 
adversary cards in discard: [29. 25. 29. 15.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25] -> size -> 25 
adversary victory points: 4
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3] -> size -> 20 
action values: 0 
buys: 1 
player value: 4 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  8.  0.  8. 10.  9. 10.  9.] 
adversary cards in hand: [ 3. 29.  1. 29.  3.] 
adversary cards in discard: [29. 25. 29. 15.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25] -> size -> 25 
adversary victory points: 4
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6. 15.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15] -> size -> 21 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  8.  0.  8. 10.  9. 10.  8.] 
adversary cards in hand: [ 3. 29.  1. 29.  3.] 
adversary cards in discard: [29. 25. 29. 15.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25] -> size -> 25 
adversary victory points: 4
player victory points: 3 





         -------------------- Turn: 17 -------------------- 
Player: 0 
cards in hand: [ 3. 29.  1. 29.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[54.436066]
 [72.08069 ]
 [72.08069 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29.  1. 29.  3.] 
cards in discard: [29. 25. 29. 15.  3.  0.] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25] -> size -> 25 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  8.  0.  8. 10.  9. 10.  8.] 
adversary cards in hand: [ 3.  1.  0.  0. 11.] 
adversary cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6. 15.  0.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 2.648695468902588



action possibilites: [-1. 29. 29.] 
expected returns: [[75.89977]
 [93.60891]
 [93.60891]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 29.  3. 29.] 
cards in discard: [29. 25. 29. 15.  3.  0.  3.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25] -> size -> 25 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  8.  0.  8. 10.  9. 10.  8.] 
adversary cards in hand: [ 3.  1.  0.  0. 11.] 
adversary cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6. 15.  0.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 63.18787384033203



action possibilites: [-1. 29.] 
expected returns: [[57.581173]
 [77.27003 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29.  0.] 
cards in discard: [29. 25. 29. 15.  3.  0.  3.  1.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25] -> size -> 25 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  8.  0.  8. 10.  9. 10.  8.] 
adversary cards in hand: [ 3.  1.  0.  0. 11.] 
adversary cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6. 15.  0.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 87.71150970458984



action possibilites: [-1. 25.] 
expected returns: [[102.60395]
 [115.77599]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 25.] 
cards in discard: [29. 25. 29. 15.  3.  0.  3.  1.  3.] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25] -> size -> 25 
action values: 1 
buys: 0 
player value: 3 
card supply: [27. 27. 30. 28. 30.  8.  9.  9.  9.  8.  8.  0.  8. 10.  9. 10.  8.] 
adversary cards in hand: [ 3.  1.  0.  0. 11.] 
adversary cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6. 15.  0.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 70.81316375732422



action possibilites: [-1] 
expected returns: [[114.805534]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0. 29.] 
cards in discard: [29. 25. 29. 15.  3.  0.  3.  1.  3.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 29. 29. 25.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25] -> size -> 25 
action values: 0 
buys: 0 
player value: 3 
card supply: [27. 27. 30. 28. 30.  8.  8.  9.  9.  8.  8.  0.  8. 10.  9. 10.  8.] 
adversary cards in hand: [ 3.  1.  0.  0. 11.] 
adversary cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6. 15.  0.  0.  0.  0.  3.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6] -> size -> 22 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 105 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 115.77600860595703





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[110.488335]
 [123.2843  ]
 [113.93654 ]
 [ 74.35973 ]
 [ 32.899967]
 [118.95643 ]
 [120.39274 ]
 [110.242546]
 [124.44538 ]
 [119.74628 ]
 [124.06605 ]
 [117.60484 ]
 [118.89218 ]
 [123.53512 ]
 [110.06869 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0. 29.] 
cards in discard: [29. 25. 29. 15.  3.  0.  3.  1.  3.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 29. 29. 25.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25] -> size -> 25 
action values: 0 
buys: 1 
player value: 5 
card supply: [27. 27. 30. 28. 30.  8.  8.  9.  9.  8.  8.  0.  8. 10.  9. 10.  8.] 
adversary cards in hand: [ 3.  1.  0.  0. 11.] 
adversary cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6. 15.  0.  0.  0.  0.  3.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6] -> size -> 22 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 105 

action type: take_action - action -1
Learning step: 0
desired expected reward: 114.80553436279297



buy possibilites: [-1] 
expected returns: [[112.60178]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0. 29.] 
cards in discard: [29. 25. 29. 15.  3.  0.  3.  1.  3. 25.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 29. 29. 25.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25 25] -> size -> 26 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 28. 30.  8.  8.  9.  9.  8.  7.  0.  8. 10.  9. 10.  8.] 
adversary cards in hand: [ 3.  1.  0.  0. 11.] 
adversary cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6. 15.  0.  0.  0.  0.  3.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6] -> size -> 22 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  30   0   0  80   0   0   0   0   0   0   0 250   0] 
sum of rewards: 355 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 124.44538116455078






Player: 1 
cards in hand: [ 3.  1.  0.  0. 11.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  1.  0.  0. 11.] 
cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6. 15.  0.  0.  0.  0.  3.  6.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6] -> size -> 22 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 28. 30.  8.  8.  9.  9.  8.  7.  0.  8. 10.  9. 10.  8.] 
adversary cards in hand: [ 0. 29.  0. 29.  3.] 
adversary cards in discard: [29. 25. 29. 15.  3.  0.  3.  1.  3. 25. 29. 29. 29. 25.  0.  0. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25 25] -> size -> 26 
adversary victory points: 4
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  1.  0.  0. 11.] 
cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6. 15.  0.  0.  0.  0.  3.  6.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6] -> size -> 22 
action values: 0 
buys: 1 
player value: 4 
card supply: [27. 27. 30. 28. 30.  8.  8.  9.  9.  8.  7.  0.  8. 10.  9. 10.  8.] 
adversary cards in hand: [ 0. 29.  0. 29.  3.] 
adversary cards in discard: [29. 25. 29. 15.  3.  0.  3.  1.  3. 25. 29. 29. 29. 25.  0.  0. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25 25] -> size -> 26 
adversary victory points: 4
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  1.  0.  0. 11.] 
cards in discard: [ 3.  3. 10.  0. 14.  0.  8.  8.  0.  6. 15.  0.  0.  0.  0.  3.  6.  1.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1] -> size -> 23 
action values: 0 
buys: 0 
player value: 1 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  8.  7.  0.  8. 10.  9. 10.  8.] 
adversary cards in hand: [ 0. 29.  0. 29.  3.] 
adversary cards in discard: [29. 25. 29. 15.  3.  0.  3.  1.  3. 25. 29. 29. 29. 25.  0.  0. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25 25] -> size -> 26 
adversary victory points: 4
player victory points: 2 





         -------------------- Turn: 18 -------------------- 
Player: 0 
cards in hand: [ 0. 29.  0. 29.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[79.99152]
 [95.28864]
 [95.28864]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29.  0. 29.  3.] 
cards in discard: [29. 25. 29. 15.  3.  0.  3.  1.  3. 25. 29. 29. 29. 25.  0.  0. 29.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25 25] -> size -> 26 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  8.  7.  0.  8. 10.  9. 10.  8.] 
adversary cards in hand: [ 3. 15.  0.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1] -> size -> 23 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: 112.6017837524414



action possibilites: [-1.] 
expected returns: [[60.19763]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3. 0.] 
cards in discard: [29. 25. 29. 15.  3.  0.  3.  1.  3. 25. 29. 29. 29. 25.  0.  0. 29. 29.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25 25] -> size -> 26 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  8.  7.  0.  8. 10.  9. 10.  8.] 
adversary cards in hand: [ 3. 15.  0.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1] -> size -> 23 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 90.27789306640625





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[ 56.578407]
 [ 69.29575 ]
 [ 60.918396]
 [-19.604258]
 [ 65.89766 ]
 [ 66.97304 ]
 [ 55.050014]
 [ 66.47438 ]
 [ 64.08666 ]
 [ 69.43997 ]
 [ 59.009377]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0.] 
cards in discard: [29. 25. 29. 15.  3.  0.  3.  1.  3. 25. 29. 29. 29. 25.  0.  0. 29. 29.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25 25] -> size -> 26 
action values: 0 
buys: 1 
player value: 4 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  8.  7.  0.  8. 10.  9. 10.  8.] 
adversary cards in hand: [ 3. 15.  0.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1] -> size -> 23 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 60.1976318359375



buy possibilites: [-1] 
expected returns: [[27.597754]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0.] 
cards in discard: [29. 25. 29. 15.  3.  0.  3.  1.  3. 25. 29. 29. 29. 25.  0.  0. 29. 29.
 15.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25 25 15] -> size -> 27 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  8.  7.  0.  8. 10.  9. 10.  7.] 
adversary cards in hand: [ 3. 15.  0.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1] -> size -> 23 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  60   0   0  20   0   0   0   0   0   0   0 128   0] 
sum of rewards: 203 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 69.43995666503906






Player: 1 
cards in hand: [ 3. 15.  0.  0.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 15.  0.  0.  3.] 
cards in discard: [] 
cards in deck: 18 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1] -> size -> 23 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  8.  7.  0.  8. 10.  9. 10.  7.] 
adversary cards in hand: [15.  0. 29. 29.  1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25 25 15] -> size -> 27 
adversary victory points: 4
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 15.  0.  0.  3.] 
cards in discard: [] 
cards in deck: 18 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1] -> size -> 23 
action values: 0 
buys: 1 
player value: 2 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  8.  7.  0.  8. 10.  9. 10.  7.] 
adversary cards in hand: [15.  0. 29. 29.  1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25 25 15] -> size -> 27 
adversary victory points: 4
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 15.  0.  0.  3.] 
cards in discard: [8.] 
cards in deck: 18 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8] -> size -> 24 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  7.  7.  0.  8. 10.  9. 10.  7.] 
adversary cards in hand: [15.  0. 29. 29.  1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25 25 15] -> size -> 27 
adversary victory points: 4
player victory points: 2 





         -------------------- Turn: 19 -------------------- 
Player: 0 
cards in hand: [15.  0. 29. 29.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 29. 29.] 
expected returns: [[ 0.52444506]
 [13.921473  ]
 [20.554195  ]
 [20.554195  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0. 29. 29.  1.] 
cards in discard: [] 
cards in deck: 22 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25 25 15] -> size -> 27 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  7.  7.  0.  8. 10.  9. 10.  7.] 
adversary cards in hand: [0. 3. 1. 0. 0.] 
adversary cards in discard: [ 8.  3. 15.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8] -> size -> 24 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: 27.597753524780273



action possibilites: [-1. 15. 29.] 
expected returns: [[ 7.438211]
 [22.118826]
 [28.869131]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0. 29.  3.] 
cards in discard: [1.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25 25 15] -> size -> 27 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  7.  7.  0.  8. 10.  9. 10.  7.] 
adversary cards in hand: [0. 3. 1. 0. 0.] 
adversary cards in discard: [ 8.  3. 15.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8] -> size -> 24 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 10.814096450805664



action possibilites: [-1. 15.] 
expected returns: [[-2.613719]
 [12.189182]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0.  3.] 
cards in discard: [ 1. 29.] 
cards in deck: 20 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15
 25 25 15] -> size -> 27 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  7.  7.  0.  8. 10.  9. 10.  7.] 
adversary cards in hand: [0. 3. 1. 0. 0.] 
adversary cards in discard: [ 8.  3. 15.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8] -> size -> 24 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 95 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 21.531105041503906



action possibilites: [-1] 
expected returns: [[-15.928022]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3.] 
cards in discard: [ 1. 29.] 
cards in deck: 20 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15] -> size -> 26 
action values: 0 
buys: 0 
player value: 5 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  7.  7.  0.  8. 10.  9. 10.  7.] 
adversary cards in hand: [0. 3. 1. 0. 0.] 
adversary cards in discard: [ 8.  3. 15.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8] -> size -> 24 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 115 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 12.189163208007812





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[-14.60199  ]
 [ -2.0119786]
 [-10.836451 ]
 [-44.776745 ]
 [-97.146576 ]
 [ -6.2806573]
 [ -5.155321 ]
 [-15.862349 ]
 [ -1.0451851]
 [ -5.74331  ]
 [ -1.4038501]
 [ -7.9578505]
 [ -6.4829946]
 [ -2.057993 ]
 [-14.705912 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3.] 
cards in discard: [ 1. 29.] 
cards in deck: 20 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15] -> size -> 26 
action values: 0 
buys: 1 
player value: 5 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  7.  7.  0.  8. 10.  9. 10.  7.] 
adversary cards in hand: [0. 3. 1. 0. 0.] 
adversary cards in discard: [ 8.  3. 15.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8] -> size -> 24 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 115 

action type: take_action - action -1
Learning step: 0
desired expected reward: -15.928022384643555



buy possibilites: [-1] 
expected returns: [[8.567808]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3.] 
cards in discard: [ 1. 29. 25.] 
cards in deck: 20 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15 25] -> size -> 27 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  7.  6.  0.  8. 10.  9. 10.  7.] 
adversary cards in hand: [0. 3. 1. 0. 0.] 
adversary cards in discard: [ 8.  3. 15.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8] -> size -> 24 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  60   0   0  60   0   0   0   0   0   0   0 250   0] 
sum of rewards: 365 

action type: buy - action 25.0
Learning step: 0
desired expected reward: -1.0451750755310059






Player: 1 
cards in hand: [0. 3. 1. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 1. 0. 0.] 
cards in discard: [ 8.  3. 15.  0.  0.  3.] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8] -> size -> 24 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  7.  6.  0.  8. 10.  9. 10.  7.] 
adversary cards in hand: [29. 29. 29. 25.  1.] 
adversary cards in discard: [ 1. 29. 25. 29. 29. 15.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15 25] -> size -> 27 
adversary victory points: 4
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
Chosen buy action: 23.0 : ['Market' '23' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 1. 0. 0.] 
cards in discard: [ 8.  3. 15.  0.  0.  3.] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8] -> size -> 24 
action values: 0 
buys: 1 
player value: 5 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  7.  6.  0.  8. 10.  9. 10.  7.] 
adversary cards in hand: [29. 29. 29. 25.  1.] 
adversary cards in discard: [ 1. 29. 25. 29. 29. 15.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15 25] -> size -> 27 
adversary victory points: 4
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 1. 0. 0.] 
cards in discard: [ 8.  3. 15.  0.  0.  3. 23.] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23] -> size -> 25 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  7.  6.  0.  8.  9.  9. 10.  7.] 
adversary cards in hand: [29. 29. 29. 25.  1.] 
adversary cards in discard: [ 1. 29. 25. 29. 29. 15.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15 25] -> size -> 27 
adversary victory points: 4
player victory points: 2 





         -------------------- Turn: 20 -------------------- 
Player: 0 
cards in hand: [29. 29. 29. 25.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29. 25.] 
expected returns: [[-2.7187448]
 [15.518774 ]
 [15.518774 ]
 [15.518774 ]
 [10.633289 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29. 29. 25.  1.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  3.] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15 25] -> size -> 27 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  7.  6.  0.  8.  9.  9. 10.  7.] 
adversary cards in hand: [ 3. 14.  6.  0.  0.] 
adversary cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23] -> size -> 25 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: 8.567808151245117



action possibilites: [-1. 29. 25.] 
expected returns: [[43.766205]
 [60.56904 ]
 [56.119896]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 25.  1.  0.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.] 
cards in deck: 14 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15 25] -> size -> 27 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  7.  6.  0.  8.  9.  9. 10.  7.] 
adversary cards in hand: [ 3. 14.  6.  0.  0.] 
adversary cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23] -> size -> 25 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 9.558177947998047



action possibilites: [-1. 25.] 
expected returns: [[101.24916 ]
 [115.208725]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  0.  3.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15 25] -> size -> 27 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 26. 30. 28. 30.  8.  8.  9.  9.  7.  6.  0.  8.  9.  9. 10.  7.] 
adversary cards in hand: [ 3. 14.  6.  0.  0.] 
adversary cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23] -> size -> 25 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 95 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 55.094764709472656



action possibilites: [-1] 
expected returns: [[72.01841]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3. 29. 25.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15 25] -> size -> 27 
action values: 0 
buys: 0 
player value: 2 
card supply: [27. 26. 30. 28. 30.  8.  7.  9.  9.  7.  6.  0.  8.  9.  9. 10.  7.] 
adversary cards in hand: [ 3. 14.  6.  0.  0.] 
adversary cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6] -> size -> 26 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 115 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 115.2087173461914





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[70.55287   ]
 [82.588524  ]
 [74.139084  ]
 [-0.77603245]
 [80.05963   ]
 [69.88703   ]
 [77.36923   ]
 [70.82056   ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3. 29. 25.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15 25] -> size -> 27 
action values: 0 
buys: 1 
player value: 3 
card supply: [27. 26. 30. 28. 30.  8.  7.  9.  9.  7.  6.  0.  8.  9.  9. 10.  7.] 
adversary cards in hand: [ 3. 14.  6.  0.  0.] 
adversary cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6] -> size -> 26 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 115 

action type: take_action - action -1
Learning step: 0
desired expected reward: 72.0184097290039



buy possibilites: [-1] 
expected returns: [[54.217773]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3. 29. 25.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.  1.] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15 25  1] -> size -> 28 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 25. 30. 28. 30.  8.  7.  9.  9.  7.  6.  0.  8.  9.  9. 10.  7.] 
adversary cards in hand: [ 3. 14.  6.  0.  0.] 
adversary cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6] -> size -> 26 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 60  0  0  0  0  0  0  0 54  0] 
sum of rewards: 169 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 82.5884780883789






Player: 1 
cards in hand: [ 3. 14.  6.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 14.] 
Chosen action: 14 : ['Militia' '14' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 14.  6.  0.  0.] 
cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.  6.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6] -> size -> 26 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 25. 30. 28. 30.  8.  7.  9.  9.  7.  6.  0.  8.  9.  9. 10.  7.] 
adversary cards in hand: [25.  0. 29. 15. 29.] 
adversary cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.  1. 29. 29. 25.  0.  3. 29. 25.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15 25  1] -> size -> 28 
adversary victory points: 4
player victory points: 1 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 6. 0. 0.] 
cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.  6.] 
cards in deck: 8 
card top of deck: [] 
played cards: [14.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6] -> size -> 26 
action values: 0 
buys: 0 
player value: 2 
card supply: [27. 25. 30. 28. 30.  8.  7.  9.  9.  7.  6.  0.  8.  9.  9. 10.  7.] 
adversary cards in hand: [ 0. 15. 29.] 
adversary cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.  1. 29. 29. 25.  0.  3. 29. 25. 25.
 29.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15 25  1] -> size -> 28 
adversary victory points: 4
player victory points: 1 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 6. 0. 0.] 
cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.  6.] 
cards in deck: 8 
card top of deck: [] 
played cards: [14.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6] -> size -> 26 
action values: 0 
buys: 1 
player value: 4 
card supply: [27. 25. 30. 28. 30.  8.  7.  9.  9.  7.  6.  0.  8.  9.  9. 10.  7.] 
adversary cards in hand: [ 0. 15. 29.] 
adversary cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.  1. 29. 29. 25.  0.  3. 29. 25. 25.
 29.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15 25  1] -> size -> 28 
adversary victory points: 4
player victory points: 1 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 6. 0. 0.] 
cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.  6. 15.] 
cards in deck: 8 
card top of deck: [] 
played cards: [14.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15] -> size -> 27 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 25. 30. 28. 30.  8.  7.  9.  9.  7.  6.  0.  8.  9.  9. 10.  6.] 
adversary cards in hand: [ 0. 15. 29.] 
adversary cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.  1. 29. 29. 25.  0.  3. 29. 25. 25.
 29.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15 25  1] -> size -> 28 
adversary victory points: 4
player victory points: 1 





         -------------------- Turn: 21 -------------------- 
Player: 0 
cards in hand: [ 0. 15. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 29.] 
expected returns: [[52.200546]
 [62.842903]
 [67.85034 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 15. 29.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.  1. 29. 29. 25.  0.  3. 29. 25. 25.
 29.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15 25  1] -> size -> 28 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 25. 30. 28. 30.  8.  7.  9.  9.  7.  6.  0.  8.  9.  9. 10.  6.] 
adversary cards in hand: [ 8. 11.  8.  6.  0.] 
adversary cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.  6. 15. 14.  3.  6.  0.
  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15] -> size -> 27 
adversary victory points: 1
player victory points: 4 

Reward from previous game state: 
[  -5    0    0   90    0    0    0    0    0    0    0    0    0    0
 1231    0] 
sum of rewards: 1316 

action type: discard_down_to_3_cards - action 1
Learning step: 0
desired expected reward: -41.729331970214844



action possibilites: [-1. 15.] 
expected returns: [[62.85238]
 [73.6965 ]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 15.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.  1. 29. 29. 25.  0.  3. 29. 25. 25.
 29. 29.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25
 25 15 25  1] -> size -> 28 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 25. 30. 28. 30.  8.  7.  9.  9.  7.  6.  0.  8.  9.  9. 10.  6.] 
adversary cards in hand: [ 8. 11.  8.  6.  0.] 
adversary cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.  6. 15. 14.  3.  6.  0.
  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15] -> size -> 27 
adversary victory points: 1
player victory points: 4 

Reward from previous game state: 
[-5  0  0 90  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 105 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 62.71632385253906



action possibilites: [-1] 
expected returns: [[223.19821]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.  1. 29. 29. 25.  0.  3. 29. 25. 25.
 29. 29.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1] -> size -> 27 
action values: 0 
buys: 0 
player value: 4 
card supply: [27. 25. 30. 28. 30.  8.  7.  9.  9.  7.  6.  0.  8.  9.  9. 10.  6.] 
adversary cards in hand: [ 8. 11.  8.  6.  0.] 
adversary cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.  6. 15. 14.  3.  6.  0.
  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15] -> size -> 27 
adversary victory points: 1
player victory points: 4 

Reward from previous game state: 
[-5  0  0 90  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 125 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 73.69647216796875





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[220.42279]
 [233.17728]
 [224.8328 ]
 [144.68094]
 [229.79944]
 [230.86278]
 [218.7976 ]
 [230.36804]
 [227.95668]
 [233.31493]
 [222.96193]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.  1. 29. 29. 25.  0.  3. 29. 25. 25.
 29. 29.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1] -> size -> 27 
action values: 0 
buys: 1 
player value: 4 
card supply: [27. 25. 30. 28. 30.  8.  7.  9.  9.  7.  6.  0.  8.  9.  9. 10.  6.] 
adversary cards in hand: [ 8. 11.  8.  6.  0.] 
adversary cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.  6. 15. 14.  3.  6.  0.
  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15] -> size -> 27 
adversary victory points: 1
player victory points: 4 

Reward from previous game state: 
[-5  0  0 90  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 125 

action type: take_action - action -1
Learning step: 0
desired expected reward: 223.19821166992188



buy possibilites: [-1] 
expected returns: [[204.57182]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.  1. 29. 29. 25.  0.  3. 29. 25. 25.
 29. 29. 15.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15] -> size -> 28 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 25. 30. 28. 30.  8.  7.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 8. 11.  8.  6.  0.] 
adversary cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.  6. 15. 14.  3.  6.  0.
  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15] -> size -> 27 
adversary victory points: 1
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  90   0   0  40   0   0   0   0   0   0   0 128   0] 
sum of rewards: 253 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 233.3148956298828






Player: 1 
cards in hand: [ 8. 11.  8.  6.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8. 11.  8.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 8. 11.  8.  6.  0.] 
cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.  6. 15. 14.  3.  6.  0.
  0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15] -> size -> 27 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 25. 30. 28. 30.  8.  7.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [3. 3. 0. 0. 0.] 
adversary cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.  1. 29. 29. 25.  0.  3. 29. 25. 25.
 29. 29. 15. 29. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15] -> size -> 28 
adversary victory points: 4
player victory points: 1 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [8. 8. 6. 0.] 
cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.  6. 15. 14.  3.  6.  0.
  0.  6.] 
cards in deck: 3 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6] -> size -> 28 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 25. 30. 28. 30.  8.  6.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [3. 3. 0. 0. 0.] 
adversary cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.  1. 29. 29. 25.  0.  3. 29. 25. 25.
 29. 29. 15. 29. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15] -> size -> 28 
adversary victory points: 4
player victory points: 0 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [8. 8. 6. 0.] 
cards in discard: [ 8.  3. 15.  0.  0.  3. 23.  0.  3.  1.  0.  0.  6. 15. 14.  3.  6.  0.
  0.  6.] 
cards in deck: 3 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6] -> size -> 28 
action values: 0 
buys: 1 
player value: 1 
card supply: [27. 25. 30. 28. 30.  8.  6.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [3. 3. 0. 0. 0.] 
adversary cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.  1. 29. 29. 25.  0.  3. 29. 25. 25.
 29. 29. 15. 29. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15] -> size -> 28 
adversary victory points: 4
player victory points: 0 





         -------------------- Turn: 22 -------------------- 
Player: 0 
cards in hand: [3. 3. 0. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[48.553627]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 3. 0. 0. 0.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.  1. 29. 29. 25.  0.  3. 29. 25. 25.
 29. 29. 15. 29. 15.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15] -> size -> 28 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 25. 30. 28. 30.  8.  6.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 6.  3.  1. 10.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6] -> size -> 28 
adversary victory points: 0
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 120   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 115 

action type: buy - action -1
Learning step: 0
desired expected reward: 204.5718231201172





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[ 48.050713]
 [ 61.120193]
 [ 52.14027 ]
 [-18.86461 ]
 [ 57.95621 ]
 [ 46.5693  ]
 [ 55.004723]
 [ 48.758785]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 0. 0. 0.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.  1. 29. 29. 25.  0.  3. 29. 25. 25.
 29. 29. 15. 29. 15.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15] -> size -> 28 
action values: 0 
buys: 1 
player value: 3 
card supply: [27. 25. 30. 28. 30.  8.  6.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 6.  3.  1. 10.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6] -> size -> 28 
adversary victory points: 0
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 120   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 115 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 48.553627014160156



buy possibilites: [-1] 
expected returns: [[55.50013]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 0. 0. 0.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  3. 29.  1.  1. 29. 29. 25.  0.  3. 29. 25. 25.
 29. 29. 15. 29. 15.  1.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1] -> size -> 29 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 24. 30. 28. 30.  8.  6.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 6.  3.  1. 10.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6] -> size -> 28 
adversary victory points: 0
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 120   0   0   0   0   0   0   0   0   0   0  54   0] 
sum of rewards: 169 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 61.12018585205078






Player: 1 
cards in hand: [ 6.  3.  1. 10.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 10.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 6.  3.  1. 10.  0.] 
cards in discard: [] 
cards in deck: 23 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6] -> size -> 28 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 24. 30. 28. 30.  8.  6.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [25. 29.  0. 29. 25.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1] -> size -> 29 
adversary victory points: 4
player victory points: 0 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 6.  3.  1. 10.  0.] 
cards in discard: [] 
cards in deck: 23 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6] -> size -> 28 
action values: 0 
buys: 1 
player value: 3 
card supply: [27. 24. 30. 28. 30.  8.  6.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [25. 29.  0. 29. 25.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1] -> size -> 29 
adversary victory points: 4
player victory points: 0 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 6.  3.  1. 10.  0.] 
cards in discard: [1.] 
cards in deck: 23 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6  1] -> size -> 29 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 28. 30.  8.  6.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [25. 29.  0. 29. 25.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1] -> size -> 29 
adversary victory points: 4
player victory points: 0 





         -------------------- Turn: 23 -------------------- 
Player: 0 
cards in hand: [25. 29.  0. 29. 25.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 29. 29. 25.] 
expected returns: [[-69.959885]
 [-68.37735 ]
 [-63.286804]
 [-63.286804]
 [-68.37735 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 29.  0. 29. 25.] 
cards in discard: [] 
cards in deck: 24 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1] -> size -> 29 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 28. 30.  8.  6.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [15. 23. 15.  3.  0.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6  1] -> size -> 29 
adversary victory points: 0
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 120   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 115 

action type: buy - action -1
Learning step: 0
desired expected reward: 55.50012969970703



action possibilites: [-1. 25. 25.] 
expected returns: [[-41.19619]
 [-38.13562]
 [-38.13562]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  0. 25.  3.] 
cards in discard: [29.] 
cards in deck: 23 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1] -> size -> 29 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 23. 30. 28. 30.  8.  6.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [15. 23. 15.  3.  0.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6  1] -> size -> 29 
adversary victory points: 0
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 120   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 135 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -67.02812957763672



action possibilites: [-1] 
expected returns: [[-34.104702]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 25.  3. 29. 15.] 
cards in discard: [29.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1] -> size -> 29 
action values: 0 
buys: 0 
player value: 1 
card supply: [27. 23. 30. 28. 30.  8.  5.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [15. 23. 15.  3.  0.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6  1  6] -> size -> 30 
adversary victory points: 0
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 120   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 155 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: -38.13566589355469





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
expected returns: [[ -32.820293]
 [ -29.181541]
 [-125.88094 ]
 [ -33.89243 ]
 [ -33.215256]]
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 25.  3. 29. 15.] 
cards in discard: [29.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1] -> size -> 29 
action values: 0 
buys: 1 
player value: 2 
card supply: [27. 23. 30. 28. 30.  8.  5.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [15. 23. 15.  3.  0.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6  1  6] -> size -> 30 
adversary victory points: 0
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 120   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 155 

action type: take_action - action -1
Learning step: 0
desired expected reward: -34.10470199584961



buy possibilites: [-1] 
expected returns: [[-24.070995]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 25.  3. 29. 15.] 
cards in discard: [29.  3.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1  3] -> size -> 30 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [15. 23. 15.  3.  0.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6  1  6] -> size -> 30 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0  40   0   0   0   0   0   0   0  16   0] 
sum of rewards: 201 

action type: buy - action 3.0
Learning step: 0
desired expected reward: -29.181541442871094






Player: 1 
cards in hand: [15. 23. 15.  3.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 23. 15.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 23. 15.  3.  0.] 
cards in discard: [ 1.  6.  3.  1. 10.  0.  6.] 
cards in deck: 18 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6  1  6] -> size -> 30 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [25. 29. 29. 29. 29.] 
adversary cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1  3] -> size -> 30 
adversary victory points: 5
player victory points: -1 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15. 23. 15.  3.  0.] 
cards in discard: [ 1.  6.  3.  1. 10.  0.  6.] 
cards in deck: 18 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6  1  6] -> size -> 30 
action values: 0 
buys: 1 
player value: 1 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [25. 29. 29. 29. 29.] 
adversary cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1  3] -> size -> 30 
adversary victory points: 5
player victory points: -1 


 --------- NOTHING HAPPENED --------- 



         -------------------- Turn: 24 -------------------- 
Player: 0 
cards in hand: [25. 29. 29. 29. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 29. 29. 29. 29.] 
expected returns: [[53.219894]
 [65.19274 ]
 [69.741295]
 [69.741295]
 [69.741295]
 [69.741295]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 29. 29. 29. 29.] 
cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.] 
cards in deck: 16 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1  3] -> size -> 30 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 6.  8. 14.  0.  0.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6  1  6] -> size -> 30 
adversary victory points: -1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 180   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 175 

action type: buy - action -1
Learning step: 0
desired expected reward: -24.070995330810547



action possibilites: [-1. 25. 29. 29. 29.] 
expected returns: [[ 85.97974]
 [ 99.21185]
 [104.08463]
 [104.08463]
 [104.08463]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 29. 29. 29.] 
cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1  3] -> size -> 30 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 6.  8. 14.  0.  0.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6  1  6] -> size -> 30 
adversary victory points: -1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 180   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 195 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 64.35948944091797



action possibilites: [-1. 29. 29. 25.] 
expected returns: [[20.32078 ]
 [39.12013 ]
 [39.12013 ]
 [34.107506]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29. 25.] 
cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.] 
cards in deck: 14 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1  3] -> size -> 30 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 6.  8. 14.  0.  0.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6  1  6] -> size -> 30 
adversary victory points: -1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 180   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 215 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 98.15974426269531



action possibilites: [-1. 29. 25.] 
expected returns: [[44.669815]
 [62.221016]
 [57.490047]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 25.] 
cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.  1.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1  3] -> size -> 30 
action values: 1 
buys: 0 
player value: 3 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 6.  8. 14.  0.  0.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6  1  6] -> size -> 30 
adversary victory points: -1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 180   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 235 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 32.98375701904297



action possibilites: [-1.] 
expected returns: [[35.084152]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0.] 
cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.  1. 25.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1  3] -> size -> 30 
action values: 1 
buys: 0 
player value: 4 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 6.  8. 14.  0.  0.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6  1  6] -> size -> 30 
adversary victory points: -1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 180   0   0  80   0   0   0   0   0   0   0   0   0] 
sum of rewards: 255 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 56.47492980957031





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 35.186623 ]
 [ 47.517597 ]
 [ 38.78421  ]
 [  2.0038018]
 [-36.63334  ]
 [ 43.45638  ]
 [ 44.803566 ]
 [ 34.532738 ]
 [ 48.60675  ]
 [ 44.197044 ]
 [ 48.250835 ]
 [ 42.053555 ]
 [ 43.395775 ]
 [ 47.75232  ]
 [ 35.11731  ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.  1. 25.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1  3] -> size -> 30 
action values: 0 
buys: 1 
player value: 5 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  6.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 6.  8. 14.  0.  0.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6  1  6] -> size -> 30 
adversary victory points: -1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 180   0   0  80   0   0   0   0   0   0   0   0   0] 
sum of rewards: 255 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 35.08415222167969



buy possibilites: [-1] 
expected returns: [[27.630533]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.  1. 25. 25.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1  3 25] -> size -> 31 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  5.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 6.  8. 14.  0.  0.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6  1  6] -> size -> 30 
adversary victory points: -1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 180   0   0  80   0   0   0   0   0   0   0 250   0] 
sum of rewards: 505 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 48.60676193237305






Player: 1 
cards in hand: [ 6.  8. 14.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8. 14.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 6.  8. 14.  0.  0.] 
cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 11  6  0  0 14  0  8  8  1 10  3 15  6  1  8
 23  6 15  6  1  6] -> size -> 30 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  5.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0. 29.  1.  3. 29.] 
adversary cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.  1. 25. 25. 29. 29. 29. 29.
  0.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1  3 25] -> size -> 31 
adversary victory points: 5
player victory points: -1 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [14.  0.] 
cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.] 
cards in deck: 13 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6] -> size -> 28 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  5.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0. 29.  1.  3. 29.] 
adversary cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.  1. 25. 25. 29. 29. 29. 29.
  0.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1  3 25] -> size -> 31 
adversary victory points: 5
player victory points: 0 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [14.  0.] 
cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.] 
cards in deck: 13 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6] -> size -> 28 
action values: 0 
buys: 1 
player value: 1 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  5.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0. 29.  1.  3. 29.] 
adversary cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.  1. 25. 25. 29. 29. 29. 29.
  0.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1  3 25] -> size -> 31 
adversary victory points: 5
player victory points: 0 





         -------------------- Turn: 25 -------------------- 
Player: 0 
cards in hand: [ 0. 29.  1.  3. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[51.362007]
 [68.37843 ]
 [68.37843 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29.  1.  3. 29.] 
cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.  1. 25. 25. 29. 29. 29. 29.
  0.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1  3 25] -> size -> 31 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  5.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.  8. 14.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6] -> size -> 28 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 145 

action type: buy - action -1
Learning step: 0
desired expected reward: 27.63053321838379



action possibilites: [-1. 29.] 
expected returns: [[28.573229]
 [44.84337 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3. 29.  1.] 
cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.  1. 25. 25. 29. 29. 29. 29.
  0.  1.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1  3 25] -> size -> 31 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  5.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.  8. 14.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6] -> size -> 28 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 165 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 62.83222198486328



action possibilites: [-1. 15.] 
expected returns: [[22.919039]
 [34.10845 ]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3. 15.] 
cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.  1. 25. 25. 29. 29. 29. 29.
  0.  1.  1.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25
 15 25  1 15  1  3 25] -> size -> 31 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  5.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.  8. 14.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6] -> size -> 28 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 185 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 39.48149490356445



action possibilites: [-1] 
expected returns: [[40.628586]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3.] 
cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.  1. 25. 25. 29. 29. 29. 29.
  0.  1.  1.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15
 25  1 15  1  3 25] -> size -> 30 
action values: 0 
buys: 0 
player value: 5 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  5.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.  8. 14.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6] -> size -> 28 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 205 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 34.108455657958984





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 38.860428]
 [ 50.619354]
 [ 42.705887]
 [  5.673096]
 [-31.632904]
 [ 47.275246]
 [ 48.400223]
 [ 37.754177]
 [ 51.527603]
 [ 47.898155]
 [ 51.232807]
 [ 45.73864 ]
 [ 47.232864]
 [ 50.82825 ]
 [ 40.382034]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3.] 
cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.  1. 25. 25. 29. 29. 29. 29.
  0.  1.  1.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15
 25  1 15  1  3 25] -> size -> 30 
action values: 0 
buys: 1 
player value: 5 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  5.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.  8. 14.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6] -> size -> 28 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 205 

action type: take_action - action -1
Learning step: 0
desired expected reward: 40.62858581542969



buy possibilites: [-1] 
expected returns: [[9.279692]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3.] 
cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.  1. 25. 25. 29. 29. 29. 29.
  0.  1.  1. 25.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15
 25  1 15  1  3 25 25] -> size -> 31 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.  8. 14.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6] -> size -> 28 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0  60   0   0   0   0   0   0   0 250   0] 
sum of rewards: 455 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 51.52757263183594






Player: 1 
cards in hand: [0. 0. 0. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.  8. 14.  0.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6] -> size -> 28 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0. 15.  3. 29.  3.] 
adversary cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.  1. 25. 25. 29. 29. 29. 29.
  0.  1.  1. 25. 29. 29. 15.  3.] 
adversary owned cards: [ 0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15
 25  1 15  1  3 25 25] -> size -> 31 
adversary victory points: 5
player victory points: 0 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.  8. 14.  0.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6] -> size -> 28 
action values: 0 
buys: 1 
player value: 4 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  7.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0. 15.  3. 29.  3.] 
adversary cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.  1. 25. 25. 29. 29. 29. 29.
  0.  1.  1. 25. 29. 29. 15.  3.] 
adversary owned cards: [ 0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15
 25  1 15  1  3 25 25] -> size -> 31 
adversary victory points: 5
player victory points: 0 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.  8. 14.  0.  8.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8] -> size -> 29 
action values: 0 
buys: 0 
player value: 2 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0. 15.  3. 29.  3.] 
adversary cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.  1. 25. 25. 29. 29. 29. 29.
  0.  1.  1. 25. 29. 29. 15.  3.] 
adversary owned cards: [ 0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15
 25  1 15  1  3 25 25] -> size -> 31 
adversary victory points: 5
player victory points: 0 





         -------------------- Turn: 26 -------------------- 
Player: 0 
cards in hand: [ 0. 15.  3. 29.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 29.] 
expected returns: [[51.150738]
 [63.466805]
 [69.49002 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 15.  3. 29.  3.] 
cards in discard: [29.  3. 29. 25.  0. 25.  3. 29. 15.  1. 25.  1. 25. 25. 29. 29. 29. 29.
  0.  1.  1. 25. 29. 29. 15.  3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15
 25  1 15  1  3 25 25] -> size -> 31 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [8. 1. 3. 8. 6.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.  8. 14.  0.  8.  0.  0.
  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8] -> size -> 29 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 145 

action type: buy - action -1
Learning step: 0
desired expected reward: 9.279691696166992



action possibilites: [-1. 15.] 
expected returns: [[-36.912468]
 [-23.358015]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 15.  3.  3.] 
cards in discard: [29.] 
cards in deck: 25 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15
 25  1 15  1  3 25 25] -> size -> 31 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [8. 1. 3. 8. 6.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.  8. 14.  0.  8.  0.  0.
  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8] -> size -> 29 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 165 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 63.44526672363281



action possibilites: [-1] 
expected returns: [[-66.27105]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 3.] 
cards in discard: [29.] 
cards in deck: 25 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25] -> size -> 30 
action values: 0 
buys: 0 
player value: 4 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [8. 1. 3. 8. 6.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.  8. 14.  0.  8.  0.  0.
  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8] -> size -> 29 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 185 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: -23.35801887512207





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[ -80.0588  ]
 [ -62.272873]
 [ -76.29704 ]
 [-157.09297 ]
 [ -66.39805 ]
 [ -70.40774 ]
 [ -86.39717 ]
 [ -70.107666]
 [ -72.8661  ]
 [ -67.45054 ]
 [ -66.67176 ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3.] 
cards in discard: [29.] 
cards in deck: 25 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25] -> size -> 30 
action values: 0 
buys: 1 
player value: 4 
card supply: [27. 23. 30. 27. 30.  8.  5.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [8. 1. 3. 8. 6.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.  8. 14.  0.  8.  0.  0.
  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8] -> size -> 29 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 185 

action type: take_action - action -1
Learning step: 0
desired expected reward: -66.27104949951172



buy possibilites: [-1] 
expected returns: [[-43.52275]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3.] 
cards in discard: [29.  1.] 
cards in deck: 25 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1] -> size -> 31 
action values: 0 
buys: 0 
player value: 1 
card supply: [27. 22. 30. 27. 30.  8.  5.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [8. 1. 3. 8. 6.] 
adversary cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.  8. 14.  0.  8.  0.  0.
  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8] -> size -> 29 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5.    0.    0.  150.    0.    0.   40.    0.    0.    0.    0.    0.
   0.    0.   13.5   0. ] 
sum of rewards: 198.5 

action type: buy - action 1.0
Learning step: 0
desired expected reward: -62.272865295410156






Player: 1 
cards in hand: [8. 1. 3. 8. 6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.  8.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [8. 1. 3. 8. 6.] 
cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.  8. 14.  0.  8.  0.  0.
  0.  0.  3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8] -> size -> 29 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 22. 30. 27. 30.  8.  5.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [25.  1.  3. 25.  0.] 
adversary cards in discard: [29.  1. 29. 15.  3.  3.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1] -> size -> 31 
adversary victory points: 5
player victory points: 0 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [8. 1. 3. 8. 6.] 
cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.  8. 14.  0.  8.  0.  0.
  0.  0.  3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8] -> size -> 29 
action values: 0 
buys: 1 
player value: 2 
card supply: [27. 22. 30. 27. 30.  8.  5.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [25.  1.  3. 25.  0.] 
adversary cards in discard: [29.  1. 29. 15.  3.  3.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1] -> size -> 31 
adversary victory points: 5
player victory points: 0 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [8. 1. 3. 8. 6.] 
cards in discard: [ 1.  6.  3.  1. 10.  0.  6. 15. 23. 15.  3.  0.  8. 14.  0.  8.  0.  0.
  0.  0.  3.  3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8  3] -> size -> 30 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 22. 30. 26. 30.  8.  5.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [25.  1.  3. 25.  0.] 
adversary cards in discard: [29.  1. 29. 15.  3.  3.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1] -> size -> 31 
adversary victory points: 5
player victory points: 1 





         -------------------- Turn: 27 -------------------- 
Player: 0 
cards in hand: [25.  1.  3. 25.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 25.] 
expected returns: [[-14.129854 ]
 [ -0.7782197]
 [ -0.7782197]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  1.  3. 25.  0.] 
cards in discard: [29.  1. 29. 15.  3.  3.] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1] -> size -> 31 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 22. 30. 26. 30.  8.  5.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 3.  0.  6.  0. 11.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8  3] -> size -> 30 
adversary victory points: 1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 120   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 115 

action type: buy - action -1
Learning step: 0
desired expected reward: -43.52275085449219



action possibilites: [-1] 
expected returns: [[-8.203623]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  3. 25.  0.  3. 25.] 
cards in discard: [29.  1. 29. 15.  3.  3.] 
cards in deck: 18 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1] -> size -> 31 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 22. 30. 26. 30.  8.  4.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 3.  0.  6.  0. 11.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8  3  6] -> size -> 31 
adversary victory points: 1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 120   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 135 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: -0.7782061100006104





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[ -8.903794 ]
 [  3.4282722]
 [ -5.32593  ]
 [-94.06151  ]
 [  0.7176833]
 [ -9.50239  ]
 [ -2.033404 ]
 [ -9.0790415]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  3. 25.  0.  3. 25.] 
cards in discard: [29.  1. 29. 15.  3.  3.] 
cards in deck: 18 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1] -> size -> 31 
action values: 0 
buys: 1 
player value: 3 
card supply: [27. 22. 30. 26. 30.  8.  4.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 3.  0.  6.  0. 11.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8  3  6] -> size -> 31 
adversary victory points: 1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 120   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 135 

action type: take_action - action -1
Learning step: 0
desired expected reward: -8.203622817993164



buy possibilites: [-1] 
expected returns: [[-15.317104]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  3. 25.  0.  3. 25.] 
cards in discard: [29.  1. 29. 15.  3.  3.  1.] 
cards in deck: 18 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1] -> size -> 32 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 21. 30. 26. 30.  8.  4.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 3.  0.  6.  0. 11.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8  3  6] -> size -> 31 
adversary victory points: 1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 120   0   0  20   0   0   0   0   0   0   0  54   0] 
sum of rewards: 189 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 3.4282631874084473






Player: 1 
cards in hand: [ 3.  0.  6.  0. 11.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0.  6.  0. 11.] 
cards in discard: [6.] 
cards in deck: 25 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8  3  6] -> size -> 31 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 21. 30. 26. 30.  8.  4.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [29.  0. 29. 29. 15.] 
adversary cards in discard: [29.  1. 29. 15.  3.  3.  1. 25.  1.  3. 25.  0.  3. 25.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1] -> size -> 32 
adversary victory points: 5
player victory points: 0 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0.  6.  0. 11.] 
cards in discard: [6.] 
cards in deck: 25 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8  3  6] -> size -> 31 
action values: 0 
buys: 1 
player value: 2 
card supply: [27. 21. 30. 26. 30.  8.  4.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [29.  0. 29. 29. 15.] 
adversary cards in discard: [29.  1. 29. 15.  3.  3.  1. 25.  1.  3. 25.  0.  3. 25.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1] -> size -> 32 
adversary victory points: 5
player victory points: 0 


 --------- NOTHING HAPPENED --------- 



         -------------------- Turn: 28 -------------------- 
Player: 0 
cards in hand: [29.  0. 29. 29. 15.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29. 15.] 
expected returns: [[140.61493]
 [156.21364]
 [156.21364]
 [156.21364]
 [151.69366]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0. 29. 29. 15.] 
cards in discard: [29.  1. 29. 15.  3.  3.  1. 25.  1.  3. 25.  0.  3. 25.] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1] -> size -> 32 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 21. 30. 26. 30.  8.  4.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  8. 14.  3.  6.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8  3  6] -> size -> 31 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 145 

action type: buy - action -1
Learning step: 0
desired expected reward: -15.31710433959961



action possibilites: [-1. 29. 15. 29.] 
expected returns: [[130.89241]
 [146.7155 ]
 [141.78957]
 [146.7155 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29. 15. 29.] 
cards in discard: [29.  1. 29. 15.  3.  3.  1. 25.  1.  3. 25.  0.  3. 25. 29.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1] -> size -> 32 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 21. 30. 26. 30.  8.  4.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  8. 14.  3.  6.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8  3  6] -> size -> 31 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 165 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 151.1816864013672



action possibilites: [-1. 15. 25.] 
expected returns: [[ 89.772255]
 [101.3802  ]
 [102.03452 ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 15. 25.] 
cards in discard: [29.  1. 29. 15.  3.  3.  1. 25.  1.  3. 25.  0.  3. 25. 29. 29.] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1] -> size -> 32 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 21. 30. 26. 30.  8.  4.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  8. 14.  3.  6.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8  3  6] -> size -> 31 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 185 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 141.39007568359375



action possibilites: [-1] 
expected returns: [[81.39153]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 15. 29. 29.] 
cards in discard: [29.  1. 29. 15.  3.  3.  1. 25.  1.  3. 25.  0.  3. 25. 29. 29.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1] -> size -> 32 
action values: 0 
buys: 0 
player value: 2 
card supply: [27. 21. 30. 26. 30.  8.  3.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  8. 14.  3.  6.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8  3  6  6] -> size -> 32 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 205 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 102.03449249267578





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[79.29491 ]
 [91.22994 ]
 [83.25796 ]
 [ 8.914984]
 [89.133156]
 [78.41501 ]
 [86.35282 ]
 [79.96558 ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 15. 29. 29.] 
cards in discard: [29.  1. 29. 15.  3.  3.  1. 25.  1.  3. 25.  0.  3. 25. 29. 29.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1] -> size -> 32 
action values: 0 
buys: 1 
player value: 3 
card supply: [27. 21. 30. 26. 30.  8.  3.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  8. 14.  3.  6.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8  3  6  6] -> size -> 32 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 205 

action type: take_action - action -1
Learning step: 0
desired expected reward: 81.39153289794922



buy possibilites: [-1] 
expected returns: [[74.268745]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 15. 29. 29.] 
cards in discard: [29.  1. 29. 15.  3.  3.  1. 25.  1.  3. 25.  0.  3. 25. 29. 29.  1.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1] -> size -> 33 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 20. 30. 26. 30.  8.  3.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  8. 14.  3.  6.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8  3  6  6] -> size -> 32 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0  60   0   0   0   0   0   0   0  54   0] 
sum of rewards: 259 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 91.22992706298828






Player: 1 
cards in hand: [ 0.  8. 14.  3.  6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8. 14.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  8. 14.  3.  6.] 
cards in discard: [ 6.  3.  0.  6.  0. 11.  6.] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 11  0  0 14  0  8  8  1 10  3 15  6  1  8 23  6
 15  6  1  6  8  3  6  6] -> size -> 32 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 20. 30. 26. 30.  8.  3.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 1. 25.  1. 15. 29.] 
adversary cards in discard: [29.  1. 29. 15.  3.  3.  1. 25.  1.  3. 25.  0.  3. 25. 29. 29.  1. 29.
 29. 25.  0. 15. 29. 29.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1] -> size -> 33 
adversary victory points: 5
player victory points: -1 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [6.] 
cards in discard: [ 6.  3.  0.  6.  0. 11.  6.] 
cards in deck: 20 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6] -> size -> 29 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 20. 30. 26. 30.  8.  3.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 1. 25.  1. 15. 29.] 
adversary cards in discard: [29.  1. 29. 15.  3.  3.  1. 25.  1.  3. 25.  0.  3. 25. 29. 29.  1. 29.
 29. 25.  0. 15. 29. 29.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1] -> size -> 33 
adversary victory points: 5
player victory points: -2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [6.] 
cards in discard: [ 6.  3.  0.  6.  0. 11.  6.] 
cards in deck: 20 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6] -> size -> 29 
action values: 0 
buys: 1 
player value: 0 
card supply: [27. 20. 30. 26. 30.  8.  3.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 1. 25.  1. 15. 29.] 
adversary cards in discard: [29.  1. 29. 15.  3.  3.  1. 25.  1.  3. 25.  0.  3. 25. 29. 29.  1. 29.
 29. 25.  0. 15. 29. 29.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1] -> size -> 33 
adversary victory points: 5
player victory points: -2 





         -------------------- Turn: 29 -------------------- 
Player: 0 
cards in hand: [ 1. 25.  1. 15. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 15. 29.] 
expected returns: [[52.756744]
 [65.872   ]
 [65.045525]
 [70.656364]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 25.  1. 15. 29.] 
cards in discard: [29.  1. 29. 15.  3.  3.  1. 25.  1.  3. 25.  0.  3. 25. 29. 29.  1. 29.
 29. 25.  0. 15. 29. 29.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1] -> size -> 33 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 20. 30. 26. 30.  8.  3.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  1. 10. 23.  6.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.] 
adversary owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6] -> size -> 29 
adversary victory points: -2
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 210   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 205 

action type: buy - action -1
Learning step: 0
desired expected reward: 74.26874542236328



action possibilites: [-1. 15. 29.] 
expected returns: [[29.333136]
 [41.213165]
 [47.084217]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  1. 15. 29.] 
cards in discard: [29.  1. 29. 15.  3.  3.  1. 25.  1.  3. 25.  0.  3. 25. 29. 29.  1. 29.
 29. 25.  0. 15. 29. 29. 25.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1] -> size -> 33 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 20. 30. 26. 30.  8.  3.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  1. 10. 23.  6.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.] 
adversary owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6] -> size -> 29 
adversary victory points: -2
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 210   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 225 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 64.80973052978516



action possibilites: [-1. 15.] 
expected returns: [[69.02342]
 [81.02974]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  1. 15.] 
cards in discard: [29.  1. 29. 15.  3.  3.  1. 25.  1.  3. 25.  0.  3. 25. 29. 29.  1. 29.
 29. 25.  0. 15. 29. 29. 25.  3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1] -> size -> 33 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 20. 30. 26. 30.  8.  3.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  1. 10. 23.  6.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.] 
adversary owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6] -> size -> 29 
adversary victory points: -2
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 210   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 245 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 41.117515563964844



action possibilites: [-1] 
expected returns: [[31.103151]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 1.] 
cards in discard: [29.  1. 29. 15.  3.  3.  1. 25.  1.  3. 25.  0.  3. 25. 29. 29.  1. 29.
 29. 25.  0. 15. 29. 29. 25.  3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1] -> size -> 33 
action values: 0 
buys: 0 
player value: 2 
card supply: [27. 20. 30. 26. 30.  8.  3.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  1. 10. 23.  6.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.] 
adversary owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6] -> size -> 29 
adversary victory points: -2
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 210   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 265 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 81.02970886230469





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 28.734972  ]
 [ 41.626408  ]
 [  0.33089662]
 [ 32.912613  ]
 [ -2.0404887 ]
 [-39.493713  ]
 [ 37.504826  ]
 [ 38.704956  ]
 [ 27.148127  ]
 [ 42.629425  ]
 [ 38.117897  ]
 [ 42.27708   ]
 [ 35.75186   ]
 [ 37.363636  ]
 [ 41.69635   ]
 [ 29.329294  ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 1.] 
cards in discard: [29.  1. 29. 15.  3.  3.  1. 25.  1.  3. 25.  0.  3. 25. 29. 29.  1. 29.
 29. 25.  0. 15. 29. 29. 25.  3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1] -> size -> 33 
action values: 0 
buys: 1 
player value: 6 
card supply: [27. 20. 30. 26. 30.  8.  3.  9.  9.  6.  4.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  1. 10. 23.  6.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.] 
adversary owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6] -> size -> 29 
adversary victory points: -2
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 210   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 265 

action type: take_action - action -1
Learning step: 0
desired expected reward: 31.103151321411133



buy possibilites: [-1] 
expected returns: [[38.496212]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 1.] 
cards in discard: [29.  1. 29. 15.  3.  3.  1. 25.  1.  3. 25.  0.  3. 25. 29. 29.  1. 29.
 29. 25.  0. 15. 29. 29. 25.  3. 25.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25] -> size -> 34 
action values: 0 
buys: 0 
player value: 1 
card supply: [27. 20. 30. 26. 30.  8.  3.  9.  9.  6.  3.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  1. 10. 23.  6.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.] 
adversary owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6] -> size -> 29 
adversary victory points: -2
player victory points: 5 

Reward from previous game state: 
[ -5.    0.    0.  210.    0.    0.   60.    0.    0.    0.    0.    0.
   0.    0.   62.5   0. ] 
sum of rewards: 327.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 42.62938690185547






Player: 1 
cards in hand: [ 0.  1. 10. 23.  6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 10. 23.] 
Chosen action: 23 : ['Market' '23' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  1. 10. 23.  6.] 
cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6] -> size -> 29 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 20. 30. 26. 30.  8.  3.  9.  9.  6.  3.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [29.  0. 29. 25.  1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25] -> size -> 34 
adversary victory points: 5
player victory points: -2 


action possibilites: [-1. 10.] 
Chosen action: 10 : ['Village' '10' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  1. 10.  6.  1.] 
cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.] 
cards in deck: 14 
card top of deck: [] 
played cards: [23.] 
owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6] -> size -> 29 
action values: 1 
buys: 1 
player value: 1 
card supply: [27. 20. 30. 26. 30.  8.  3.  9.  9.  6.  3.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [29.  0. 29. 25.  1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25] -> size -> 34 
adversary victory points: 5
player victory points: -2 


action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 1. 6. 1. 0.] 
cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.] 
cards in deck: 13 
card top of deck: [] 
played cards: [23. 10.] 
owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6] -> size -> 29 
action values: 2 
buys: 1 
player value: 1 
card supply: [27. 20. 30. 26. 30.  8.  3.  9.  9.  6.  3.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [29.  0. 29. 25.  1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25] -> size -> 34 
adversary victory points: 5
player victory points: -2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
Chosen buy action: 2.0 : ['Gold' '2' '6']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 6. 1. 0.] 
cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.] 
cards in deck: 13 
card top of deck: [] 
played cards: [23. 10.] 
owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6] -> size -> 29 
action values: 0 
buys: 2 
player value: 7 
card supply: [27. 20. 30. 26. 30.  8.  3.  9.  9.  6.  3.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [29.  0. 29. 25.  1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25] -> size -> 34 
adversary victory points: 5
player victory points: -2 


buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 6. 1. 0.] 
cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2.] 
cards in deck: 13 
card top of deck: [] 
played cards: [23. 10.] 
owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6  2] -> size -> 30 
action values: 0 
buys: 1 
player value: 1 
card supply: [27. 20. 29. 26. 30.  8.  3.  9.  9.  6.  3.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [29.  0. 29. 25.  1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25] -> size -> 34 
adversary victory points: 5
player victory points: -2 





         -------------------- Turn: 30 -------------------- 
Player: 0 
cards in hand: [29.  0. 29. 25.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 25.] 
expected returns: [[-48.957085]
 [-36.45434 ]
 [-36.45434 ]
 [-43.73832 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0. 29. 25.  1.] 
cards in discard: [] 
cards in deck: 29 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25] -> size -> 34 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 20. 29. 26. 30.  8.  3.  9.  9.  6.  3.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  3.  6. 15.  8.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.] 
adversary owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6  2] -> size -> 30 
adversary victory points: -2
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 210   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 205 

action type: buy - action -1
Learning step: 0
desired expected reward: 38.496212005615234



action possibilites: [-1. 29.] 
expected returns: [[-43.236137]
 [-23.810184]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29.  1.  3.] 
cards in discard: [25.] 
cards in deck: 28 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25] -> size -> 34 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 20. 29. 26. 30.  8.  3.  9.  9.  6.  3.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  3.  6. 15.  8.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.] 
adversary owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6  2] -> size -> 30 
adversary victory points: -2
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 210   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 225 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -42.12823486328125



action possibilites: [-1. 25.] 
expected returns: [[-33.573334]
 [-17.776907]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3. 25.] 
cards in discard: [25.  1.] 
cards in deck: 27 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25] -> size -> 34 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 20. 29. 26. 30.  8.  3.  9.  9.  6.  3.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  3.  6. 15.  8.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.] 
adversary owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6  2] -> size -> 30 
adversary victory points: -2
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 210   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 245 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -30.156063079833984



action possibilites: [-1] 
expected returns: [[-43.087944]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3. 15.  1.] 
cards in discard: [25.  1.] 
cards in deck: 25 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25] -> size -> 34 
action values: 0 
buys: 0 
player value: 2 
card supply: [27. 20. 29. 26. 30.  8.  2.  9.  9.  6.  3.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  3.  6. 15.  8.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.  6.] 
adversary owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6  2  6] -> size -> 31 
adversary victory points: -2
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 210   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 265 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: -17.776901245117188





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ -41.524986]
 [ -28.636036]
 [ -37.315388]
 [ -88.39433 ]
 [-129.28116 ]
 [ -32.705837]
 [ -31.578386]
 [ -46.116768]
 [ -27.680887]
 [ -32.148262]
 [ -28.02574 ]
 [ -34.52404 ]
 [ -32.872726]
 [ -28.624971]
 [ -40.679565]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3. 15.  1.] 
cards in discard: [25.  1.] 
cards in deck: 25 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25] -> size -> 34 
action values: 0 
buys: 1 
player value: 5 
card supply: [27. 20. 29. 26. 30.  8.  2.  9.  9.  6.  3.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  3.  6. 15.  8.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.  6.] 
adversary owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6  2  6] -> size -> 31 
adversary victory points: -2
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 210   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 265 

action type: take_action - action -1
Learning step: 0
desired expected reward: -43.08794403076172



buy possibilites: [-1] 
expected returns: [[4.6019387]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3. 15.  1.] 
cards in discard: [25.  1. 25.] 
cards in deck: 25 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25] -> size -> 35 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 20. 29. 26. 30.  8.  2.  9.  9.  6.  2.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 0.  3.  6. 15.  8.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.  6.] 
adversary owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6  2  6] -> size -> 31 
adversary victory points: -2
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 210   0   0  60   0   0   0   0   0   0   0 250   0] 
sum of rewards: 515 

action type: buy - action 25.0
Learning step: 0
desired expected reward: -27.680875778198242






Player: 1 
cards in hand: [ 0.  3.  6. 15.  8.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15.  8.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3.  6. 15.  8.] 
cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.  6.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6  2  6] -> size -> 31 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 20. 29. 26. 30.  8.  2.  9.  9.  6.  2.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 3. 25. 29.  3.  1.] 
adversary cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25] -> size -> 35 
adversary victory points: 5
player victory points: -3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3.  6. 15.  8.] 
cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.  6.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6  2  6] -> size -> 31 
action values: 0 
buys: 1 
player value: 1 
card supply: [27. 20. 29. 26. 30.  8.  2.  9.  9.  6.  2.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 3. 25. 29.  3.  1.] 
adversary cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25] -> size -> 35 
adversary victory points: 5
player victory points: -3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3.  6. 15.  8.] 
cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.  6.
  0.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6  2  6  0] -> size -> 32 
action values: 0 
buys: 0 
player value: 1 
card supply: [26. 20. 29. 26. 30.  8.  2.  9.  9.  6.  2.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 3. 25. 29.  3.  1.] 
adversary cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25] -> size -> 35 
adversary victory points: 5
player victory points: -3 





         -------------------- Turn: 31 -------------------- 
Player: 0 
cards in hand: [ 3. 25. 29.  3.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 29.] 
expected returns: [[-3.531589]
 [ 9.721006]
 [14.606491]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 25. 29.  3.  1.] 
cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25] -> size -> 35 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 20. 29. 26. 30.  8.  2.  9.  9.  6.  2.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 3.  8. 15.  3.  0.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.  6.
  0.  0.  3.  6. 15.  8.] 
adversary owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6  2  6  0] -> size -> 32 
adversary victory points: -3
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 240   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 235 

action type: buy - action -1
Learning step: 0
desired expected reward: 4.601938724517822



action possibilites: [-1. 25. 15.] 
expected returns: [[30.681376]
 [42.046204]
 [41.303436]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  3.  1. 15.] 
cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25] -> size -> 35 
action values: 1 
buys: 0 
player value: 1 
card supply: [26. 20. 29. 26. 30.  8.  2.  9.  9.  6.  2.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 3.  8. 15.  3.  0.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.  6.
  0.  0.  3.  6. 15.  8.] 
adversary owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6  2  6  0] -> size -> 32 
adversary victory points: -3
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 240   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 255 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 8.64588737487793



action possibilites: [-1] 
expected returns: [[35.60212]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  1. 15.  3. 29.] 
cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.] 
cards in deck: 17 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25] -> size -> 35 
action values: 0 
buys: 0 
player value: 1 
card supply: [26. 20. 29. 26. 30.  8.  1.  9.  9.  6.  2.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 3.  8. 15.  3.  0.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.  6.
  0.  0.  3.  6. 15.  8.  6.] 
adversary owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6  2  6  0  6] -> size -> 33 
adversary victory points: -3
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 240   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 275 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 42.04620361328125





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[ 35.305817]
 [ 47.697197]
 [ 38.870373]
 [-37.065056]
 [ 44.835606]
 [ 34.485115]
 [ 42.121887]
 [ 35.702652]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  1. 15.  3. 29.] 
cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.] 
cards in deck: 17 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25] -> size -> 35 
action values: 0 
buys: 1 
player value: 3 
card supply: [26. 20. 29. 26. 30.  8.  1.  9.  9.  6.  2.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 3.  8. 15.  3.  0.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.  6.
  0.  0.  3.  6. 15.  8.  6.] 
adversary owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6  2  6  0  6] -> size -> 33 
adversary victory points: -3
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 240   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 275 

action type: take_action - action -1
Learning step: 0
desired expected reward: 35.60211944580078



buy possibilites: [-1] 
expected returns: [[26.479155]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  1. 15.  3. 29.] 
cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1.] 
cards in deck: 17 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1] -> size -> 36 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 19. 29. 26. 30.  8.  1.  9.  9.  6.  2.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 3.  8. 15.  3.  0.] 
adversary cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.  6.
  0.  0.  3.  6. 15.  8.  6.] 
adversary owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6  2  6  0  6] -> size -> 33 
adversary victory points: -3
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 240   0   0  40   0   0   0   0 -10   0   0  54   0] 
sum of rewards: 319 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 47.69720458984375






Player: 1 
cards in hand: [ 3.  8. 15.  3.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8. 15.] 
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  8. 15.  3.  0.] 
cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.  6.
  0.  0.  3.  6. 15.  8.  6.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1
  6  8  3  6  6  2  6  0  6] -> size -> 33 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 19. 29. 26. 30.  8.  1.  9.  9.  6.  2.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [15. 25. 29. 25.  1.] 
adversary cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1] -> size -> 36 
adversary victory points: 5
player victory points: -4 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 8. 3.] 
cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.  6.
  0.  0.  3.  6. 15.  8.  6.] 
cards in deck: 3 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1  6
  8  3  6  6  2  6  0  6] -> size -> 32 
action values: 0 
buys: 0 
player value: 3 
card supply: [26. 19. 29. 26. 30.  8.  1.  9.  9.  6.  2.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [15. 25. 29. 25.  1.] 
adversary cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1] -> size -> 36 
adversary victory points: 5
player victory points: -4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 8. 3.] 
cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.  6.
  0.  0.  3.  6. 15.  8.  6.] 
cards in deck: 3 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1  6
  8  3  6  6  2  6  0  6] -> size -> 32 
action values: 0 
buys: 1 
player value: 3 
card supply: [26. 19. 29. 26. 30.  8.  1.  9.  9.  6.  2.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [15. 25. 29. 25.  1.] 
adversary cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1] -> size -> 36 
adversary victory points: 5
player victory points: -4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 8. 3.] 
cards in discard: [ 6.  3.  0.  6.  0. 11.  6.  8.  6.  2. 23. 10.  0.  1.  6.  1.  0.  6.
  0.  0.  3.  6. 15.  8.  6.  0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1  6
  8  3  6  6  2  6  0  6  0] -> size -> 33 
action values: 0 
buys: 0 
player value: 3 
card supply: [25. 19. 29. 26. 30.  8.  1.  9.  9.  6.  2.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [15. 25. 29. 25.  1.] 
adversary cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1] -> size -> 36 
adversary victory points: 5
player victory points: -4 





         -------------------- Turn: 32 -------------------- 
Player: 0 
cards in hand: [15. 25. 29. 25.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 25. 29. 25.] 
expected returns: [[-14.314728 ]
 [ -2.673231 ]
 [ -1.8002008]
 [  2.9190798]
 [ -1.8002008]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 25. 29. 25.  1.] 
cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29.] 
cards in deck: 12 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1] -> size -> 36 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 19. 29. 26. 30.  8.  1.  9.  9.  6.  2.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [0. 3. 0. 8. 1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1  6
  8  3  6  6  2  6  0  6  0] -> size -> 33 
adversary victory points: -4
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 270   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 265 

action type: buy - action -1
Learning step: 0
desired expected reward: 26.479154586791992



action possibilites: [-1. 15. 25.] 
expected returns: [[-15.2992   ]
 [ -2.0210667]
 [ -1.0020192]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 25.  1.  1.] 
cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29. 25.] 
cards in deck: 11 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1] -> size -> 36 
action values: 1 
buys: 0 
player value: 1 
card supply: [25. 19. 29. 26. 30.  8.  1.  9.  9.  6.  2.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [0. 3. 0. 8. 1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1  6
  8  3  6  6  2  6  0  6  0] -> size -> 33 
adversary victory points: -4
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 270   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 285 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -2.7478370666503906



action possibilites: [-1] 
expected returns: [[13.73274]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  1.  1. 29.  0.] 
cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29. 25.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1] -> size -> 36 
action values: 0 
buys: 0 
player value: 1 
card supply: [25. 19. 29. 26. 30.  8.  0.  9.  9.  6.  2.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [0. 3. 0. 8. 1.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1  6
  8  3  6  6  2  6  0  6  0  6] -> size -> 34 
adversary victory points: -4
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 270   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 305 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: -1.002035140991211





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[  9.905643]
 [ 22.644075]
 [-20.09584 ]
 [ 13.96319 ]
 [-23.11812 ]
 [ 18.677242]
 [ 19.954317]
 [  8.684467]
 [ 23.684162]
 [ 19.368507]
 [ 23.338558]
 [ 17.05544 ]
 [ 18.599552]
 [ 22.83428 ]
 [ 10.601971]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  1.  1. 29.  0.] 
cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29. 25.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1] -> size -> 36 
action values: 0 
buys: 1 
player value: 6 
card supply: [25. 19. 29. 26. 30.  8.  0.  9.  9.  6.  2.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [0. 3. 0. 8. 1.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1  6
  8  3  6  6  2  6  0  6  0  6] -> size -> 34 
adversary victory points: -4
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 270   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 305 

action type: take_action - action -1
Learning step: 0
desired expected reward: 13.73274040222168



buy possibilites: [-1] 
expected returns: [[-1.5003386]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  1.  1. 29.  0.] 
cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29. 25. 25.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1 25] -> size -> 37 
action values: 0 
buys: 0 
player value: 1 
card supply: [25. 19. 29. 26. 30.  8.  0.  9.  9.  6.  1.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [0. 3. 0. 8. 1.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1  6
  8  3  6  6  2  6  0  6  0  6] -> size -> 34 
adversary victory points: -4
player victory points: 5 

Reward from previous game state: 
[ -5.    0.    0.  270.    0.    0.   40.    0.    0.    0.    0.  -20.
   0.    0.   62.5   0. ] 
sum of rewards: 347.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 23.684125900268555






Player: 1 
cards in hand: [0. 3. 0. 8. 1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 0. 8. 1.] 
cards in discard: [6.] 
cards in deck: 28 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3 11  0  0  0  8  8  1 10  3 15  6  1  8 23  6 15  6  1  6
  8  3  6  6  2  6  0  6  0  6] -> size -> 34 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 19. 29. 26. 30.  8.  0.  9.  9.  6.  1.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 3. 25. 29. 25.  1.] 
adversary cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29. 25. 25. 29. 25. 15.  1.  1. 29.  0.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1 25] -> size -> 37 
adversary victory points: 5
player victory points: -5 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [] 
cards in discard: [6.] 
cards in deck: 28 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  3 11  0  0  0  8  8 10  3 15  6  1  8 23  6 15  6  1  6  8  3  6  6
  2  6  0  6  0  6] -> size -> 30 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 19. 29. 26. 30.  8.  0.  9.  9.  6.  1.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 3. 25. 29. 25.  1.] 
adversary cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29. 25. 25. 29. 25. 15.  1.  1. 29.  0.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1 25] -> size -> 37 
adversary victory points: 5
player victory points: -6 




----------- BUY PHASE ----------- 
buy possibilites: [ 0. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [6.] 
cards in deck: 28 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  3 11  0  0  0  8  8 10  3 15  6  1  8 23  6 15  6  1  6  8  3  6  6
  2  6  0  6  0  6] -> size -> 30 
action values: 0 
buys: 1 
player value: 0 
card supply: [25. 19. 29. 26. 30.  8.  0.  9.  9.  6.  1.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 3. 25. 29. 25.  1.] 
adversary cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29. 25. 25. 29. 25. 15.  1.  1. 29.  0.] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1 25] -> size -> 37 
adversary victory points: 5
player victory points: -6 





         -------------------- Turn: 33 -------------------- 
Player: 0 
cards in hand: [ 3. 25. 29. 25.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 29. 25.] 
expected returns: [[ 3.4761138]
 [16.884388 ]
 [22.911634 ]
 [16.884388 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 25. 29. 25.  1.] 
cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29. 25. 25. 29. 25. 15.  1.  1. 29.  0.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1 25] -> size -> 37 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 19. 29. 26. 30.  8.  0.  9.  9.  6.  1.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [6. 8. 0. 6. 6.] 
adversary cards in discard: [6. 8.] 
adversary owned cards: [ 0  3 11  0  0  0  8  8 10  3 15  6  1  8 23  6 15  6  1  6  8  3  6  6
  2  6  0  6  0  6] -> size -> 30 
adversary victory points: -6
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 330   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 325 

action type: buy - action -1
Learning step: 0
desired expected reward: -1.5003385543823242



action possibilites: [-1. 25. 25. 29.] 
expected returns: [[-4.868458]
 [ 9.077694]
 [ 9.077694]
 [15.104944]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 25. 29.] 
cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29. 25. 25. 29. 25. 15.  1.  1. 29.  0.  3.  1.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1 25] -> size -> 37 
action values: 1 
buys: 0 
player value: 1 
card supply: [25. 19. 29. 26. 30.  8.  0.  9.  9.  6.  1.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [6. 8. 0. 6. 6.] 
adversary cards in discard: [6. 8.] 
adversary owned cards: [ 0  3 11  0  0  0  8  8 10  3 15  6  1  8 23  6 15  6  1  6  8  3  6  6
  2  6  0  6  0  6] -> size -> 30 
adversary victory points: -6
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 330   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 345 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 15.770641326904297



action possibilites: [-1. 29.] 
expected returns: [[-25.947386 ]
 [ -4.6152635]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.] 
cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29. 25. 25. 29. 25. 15.  1.  1. 29.  0.  3.  1. 25. 25.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1 25] -> size -> 37 
action values: 1 
buys: 0 
player value: 2 
card supply: [25. 19. 29. 26. 30.  8.  0.  9.  9.  6.  1.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [6. 8. 0. 6. 6.] 
adversary cards in discard: [6. 8.] 
adversary owned cards: [ 0  3 11  0  0  0  8  8 10  3 15  6  1  8 23  6 15  6  1  6  8  3  6  6
  2  6  0  6  0  6] -> size -> 30 
adversary victory points: -6
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 330   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 365 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 7.700605869293213



action possibilites: [-1.] 
expected returns: [[-26.944345]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [] 
cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29. 25. 25. 29. 25. 15.  1.  1. 29.  0.  3.  1. 25. 25.  1.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1 25] -> size -> 37 
action values: 1 
buys: 0 
player value: 3 
card supply: [25. 19. 29. 26. 30.  8.  0.  9.  9.  6.  1.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [6. 8. 0. 6. 6.] 
adversary cards in discard: [6. 8.] 
adversary owned cards: [ 0  3 11  0  0  0  8  8 10  3 15  6  1  8 23  6 15  6  1  6  8  3  6  6
  2  6  0  6  0  6] -> size -> 30 
adversary victory points: -6
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 330   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 385 

action type: discard_n_cards - action 0
Learning step: 0
desired expected reward: -26.156232833862305





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3. 11.  8. 10. -1.] 
expected returns: [[-27.59905 ]
 [-14.660515]
 [-23.379337]
 [-17.571018]
 [-29.218351]
 [-20.540136]
 [-26.944342]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29. 25. 25. 29. 25. 15.  1.  1. 29.  0.  3.  1. 25. 25.  1.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1 25] -> size -> 37 
action values: 1 
buys: 1 
player value: 3 
card supply: [25. 19. 29. 26. 30.  8.  0.  9.  9.  6.  1.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [6. 8. 0. 6. 6.] 
adversary cards in discard: [6. 8.] 
adversary owned cards: [ 0  3 11  0  0  0  8  8 10  3 15  6  1  8 23  6 15  6  1  6  8  3  6  6
  2  6  0  6  0  6] -> size -> 30 
adversary victory points: -6
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 330   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 385 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -26.944345474243164



buy possibilites: [-1] 
expected returns: [[-31.942749]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [25.  1. 25. 29. 29. 25.  0.  3. 15.  1.  3.  1. 29. 25.  3.  1. 15.  3.
 29. 25. 25. 29. 25. 15.  1.  1. 29.  0.  3.  1. 25. 25.  1.  1.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1 25  1] -> size -> 38 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 18. 29. 26. 30.  8.  0.  9.  9.  6.  1.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [6. 8. 0. 6. 6.] 
adversary cards in discard: [6. 8.] 
adversary owned cards: [ 0  3 11  0  0  0  8  8 10  3 15  6  1  8 23  6 15  6  1  6  8  3  6  6
  2  6  0  6  0  6] -> size -> 30 
adversary victory points: -6
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 330   0   0  60   0   0   0   0 -30   0   0  54   0] 
sum of rewards: 409 

action type: buy - action 1.0
Learning step: 0
desired expected reward: -14.660516738891602






Player: 1 
cards in hand: [6. 8. 0. 6. 6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [6. 8. 0. 6. 6.] 
cards in discard: [6. 8.] 
cards in deck: 23 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  3 11  0  0  0  8  8 10  3 15  6  1  8 23  6 15  6  1  6  8  3  6  6
  2  6  0  6  0  6] -> size -> 30 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 18. 29. 26. 30.  8.  0.  9.  9.  6.  1.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 1.  1. 29.  3. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1 25  1] -> size -> 38 
adversary victory points: 5
player victory points: -6 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 6.] 
cards in discard: [6. 8.] 
cards in deck: 23 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  3 11  0  0  0  8  8 10  3 15  1  8 23 15  6  1  6  8  3  6  6  2  6
  0  6  0  6] -> size -> 28 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 18. 29. 26. 30.  8.  0.  9.  9.  6.  1.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 1.  1. 29.  3. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1 25  1] -> size -> 38 
adversary victory points: 5
player victory points: -4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 6.] 
cards in discard: [6. 8.] 
cards in deck: 23 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  3 11  0  0  0  8  8 10  3 15  1  8 23 15  6  1  6  8  3  6  6  2  6
  0  6  0  6] -> size -> 28 
action values: 0 
buys: 1 
player value: 1 
card supply: [25. 18. 29. 26. 30.  8.  0.  9.  9.  6.  1.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [ 1.  1. 29.  3. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1 25  1] -> size -> 38 
adversary victory points: 5
player victory points: -4 





         -------------------- Turn: 34 -------------------- 
Player: 0 
cards in hand: [ 1.  1. 29.  3. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[-33.50741 ]
 [-11.687983]
 [-11.687983]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  1. 29.  3. 29.] 
cards in discard: [] 
cards in deck: 33 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1 25  1] -> size -> 38 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 18. 29. 26. 30.  8.  0.  9.  9.  6.  1.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [3. 6. 3. 3. 0.] 
adversary cards in discard: [6. 8. 8. 0. 6.] 
adversary owned cards: [ 0  3 11  0  0  0  8  8 10  3 15  1  8 23 15  6  1  6  8  3  6  6  2  6
  0  6  0  6] -> size -> 28 
adversary victory points: -4
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 270   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 265 

action type: buy - action -1
Learning step: 0
desired expected reward: -31.9427490234375



action possibilites: [-1.] 
expected returns: [[-43.1006]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 1. 3.] 
cards in discard: [29. 29.] 
cards in deck: 32 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1 25  1] -> size -> 38 
action values: 1 
buys: 0 
player value: 1 
card supply: [25. 18. 29. 26. 30.  8.  0.  9.  9.  6.  1.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [3. 6. 3. 3. 0.] 
adversary cards in discard: [6. 8. 8. 0. 6.] 
adversary owned cards: [ 0  3 11  0  0  0  8  8 10  3 15  1  8 23 15  6  1  6  8  3  6  6  2  6
  0  6  0  6] -> size -> 28 
adversary victory points: -4
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 270   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 285 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -19.09231185913086





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[-41.22847 ]
 [-28.547932]
 [-37.169655]
 [-86.862465]
 [-32.61451 ]
 [-31.542753]
 [-42.885056]
 [-27.627089]
 [-32.102848]
 [-27.968756]
 [-34.41776 ]
 [-32.807407]
 [-28.59206 ]
 [-40.724228]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 1. 3.] 
cards in discard: [29. 29.] 
cards in deck: 32 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1 25  1] -> size -> 38 
action values: 0 
buys: 1 
player value: 5 
card supply: [25. 18. 29. 26. 30.  8.  0.  9.  9.  6.  1.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [3. 6. 3. 3. 0.] 
adversary cards in discard: [6. 8. 8. 0. 6.] 
adversary owned cards: [ 0  3 11  0  0  0  8  8 10  3 15  1  8 23 15  6  1  6  8  3  6  6  2  6
  0  6  0  6] -> size -> 28 
adversary victory points: -4
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 270   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 285 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -43.10060119628906



Player 0 won the game! 



Player 0 bought cards:
Copper: 0 
Silver: 9 
Gold: 0 
Estate: 2 
Duchy: 0 
Province: 0 
Curse: 0 

Remodel: 0 
Workshop: 0 
Chapel: 0 
Witch: 10 
Poacher: 10 
Militia: 0 
Market: 0 
Village: 0 
Library: 0 
Moneylender: 3 


 --- GAME STATE ---
Action that is taken: None 
cards in hand: [1. 1. 3.] 
cards in discard: [29. 29. 25.] 
cards in deck: 32 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  3  1 29 29 29  1 29 29 29 29 29 29 25 15 25 25 15 25
  1 15  1  3 25 25  1  1  1 25 25  1 25  1 25] -> size -> 39 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 18. 29. 26. 30.  8.  0.  9.  9.  6.  0.  0.  8.  9.  9. 10.  5.] 
adversary cards in hand: [3. 6. 3. 3. 0.] 
adversary cards in discard: [6. 8. 8. 0. 6.] 
adversary owned cards: [ 0  3 11  0  0  0  8  8 10  3 15  1  8 23 15  6  1  6  8  3  6  6  2  6
  0  6  0  6] -> size -> 28 
adversary victory points: -4
player victory points: 5 

Reward from previous game state: 
[     -5 3000000       0     270       0       0      20       0       0
       0       0     -40       0       0     125       0] 
sum of rewards: 3000370 

action type: buy - action 25.0
Learning step: 120015.90625
desired expected reward: 119988.28125



