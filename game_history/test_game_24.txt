 --------- CARDS IN THIS GAME --------- 
Game cards: [['Copper' '0' '0']
 ['Silver' '1' '3']
 ['Gold' '2' '6']
 ['Estate' '3' '2']
 ['Duchy' '4' '5']
 ['Province' '5' '8']
 ['Curse' '6' '0']
 ['Remodel' '16' '4']
 ['Workshop' '11' '3']
 ['Chapel' '8' '2']
 ['Witch' '25' '5']
 ['Poacher' '29' '4']
 ['Militia' '14' '4']
 ['Market' '23' '5']
 ['Village' '10' '3']
 ['Library' '22' '5']
 ['Moneylender' '15' '4']]






         -------------------- Turn: 1 -------------------- 
Player: 0 
cards in hand: [0. 3. 0. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[12.017579]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[  -5    0    0    0    0    0   20    0    0    0    0  -30    0 -300
    0    0] 
sum of rewards: -315 

action type: buy - action 6.0
Learning step: -12.00261402130127
desired expected reward: -26.937232971191406





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
expected returns: [[ 9.418884 ]
 [15.827067 ]
 [11.91141  ]
 [-1.8045425]
 [13.701043 ]
 [15.498094 ]
 [12.250535 ]
 [20.080193 ]
 [ 9.612272 ]
 [11.957418 ]
 [16.553823 ]
 [10.911282 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 0 
buys: 1 
player value: 4 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 11.924886703491211



buy possibilites: [-1] 
expected returns: [[8.890469]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [29.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
action values: 0 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0   0   0   0   0   0   0   0   0   0   0   0 128   0] 
sum of rewards: 123 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 20.080190658569336






Player: 1 
cards in hand: [0. 0. 0. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 3. 0.] 
adversary cards in discard: [29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 0 
buys: 1 
player value: 5 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 3. 0.] 
adversary cards in discard: [29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [8.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3 8] -> size -> 11 
action values: 0 
buys: 0 
player value: 3 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10.  9. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 3. 0.] 
adversary cards in discard: [29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
adversary victory points: 3
player victory points: 3 





         -------------------- Turn: 2 -------------------- 
Player: 0 
cards in hand: [0. 3. 0. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[11.905253]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 0. 3. 0.] 
cards in discard: [29.  0.  3.  0.  0.  0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10.  9. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 3. 3.] 
adversary cards in discard: [8. 0. 0. 0. 0. 0.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 8] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 8.89046859741211





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[11.254378  ]
 [17.972374  ]
 [13.887166  ]
 [-0.40051913]
 [17.618473  ]
 [14.182361  ]
 [13.883855  ]
 [12.804278  ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 3. 0.] 
cards in discard: [29.  0.  3.  0.  0.  0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
action values: 0 
buys: 1 
player value: 3 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10.  9. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 3. 3.] 
adversary cards in discard: [8. 0. 0. 0. 0. 0.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 8] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 10.483170509338379



buy possibilites: [-1] 
expected returns: [[5.491186]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 3. 0.] 
cards in discard: [29.  0.  3.  0.  0.  0.  1.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1] -> size -> 12 
action values: 0 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10.  9. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 3. 3.] 
adversary cards in discard: [8. 0. 0. 0. 0. 0.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 8] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0 54  0] 
sum of rewards: 49 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 17.972383499145508






Player: 1 
cards in hand: [0. 0. 3. 3. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3. 3. 3.] 
cards in discard: [8. 0. 0. 0. 0. 0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3 8] -> size -> 11 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10.  9. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 3. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1] -> size -> 12 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 3. 3.] 
cards in discard: [8. 0. 0. 0. 0. 0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3 8] -> size -> 11 
action values: 0 
buys: 1 
player value: 2 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10.  9. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 3. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1] -> size -> 12 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 3. 3.] 
cards in discard: [8. 0. 0. 0. 0. 0. 8.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3 8 8] -> size -> 12 
action values: 0 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10.  8. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 3. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1] -> size -> 12 
adversary victory points: 3
player victory points: 3 





         -------------------- Turn: 3 -------------------- 
Player: 0 
cards in hand: [0. 0. 0. 3. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[14.329556]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 3. 3.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1] -> size -> 12 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10.  8. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 8 8] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 5.491186141967773





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[12.208406 ]
 [18.41576  ]
 [14.627347 ]
 [ 1.3342855]
 [18.100775 ]
 [14.94477  ]
 [14.664029 ]
 [13.656999 ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3. 3.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1] -> size -> 12 
action values: 0 
buys: 1 
player value: 3 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10.  8. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 8 8] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 13.325776100158691



buy possibilites: [-1] 
expected returns: [[12.69241]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3. 3.] 
cards in discard: [1.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1] -> size -> 13 
action values: 0 
buys: 0 
player value: 0 
card supply: [30. 28. 30. 30. 30.  8. 10. 10. 10.  8. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 8 8] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0 54  0] 
sum of rewards: 49 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 18.415769577026367






Player: 1 
cards in hand: [0. 0. 0. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3 8 8] -> size -> 12 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 28. 30. 30. 30.  8. 10. 10. 10.  8. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [29.  3.  0.  1.  0.] 
adversary cards in discard: [1. 0. 0. 0. 3. 3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1] -> size -> 13 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3 8 8] -> size -> 12 
action values: 0 
buys: 1 
player value: 4 
card supply: [30. 28. 30. 30. 30.  8. 10. 10. 10.  8. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [29.  3.  0.  1.  0.] 
adversary cards in discard: [1. 0. 0. 0. 3. 3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1] -> size -> 13 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [3.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3 8 8 3] -> size -> 13 
action values: 0 
buys: 0 
player value: 2 
card supply: [30. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [29.  3.  0.  1.  0.] 
adversary cards in discard: [1. 0. 0. 0. 3. 3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1] -> size -> 13 
adversary victory points: 3
player victory points: 4 





         -------------------- Turn: 4 -------------------- 
Player: 0 
cards in hand: [29.  3.  0.  1.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[-5.121644 ]
 [ 3.8195932]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  3.  0.  1.  0.] 
cards in discard: [1. 0. 0. 0. 3. 3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1] -> size -> 13 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [8. 0. 8. 3. 0.] 
adversary cards in discard: [3. 0. 0. 0. 0. 3.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 8 8 3] -> size -> 13 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -35 

action type: buy - action -1
Learning step: 0
desired expected reward: 12.692410469055176



action possibilites: [-1.] 
expected returns: [[19.641127]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 1. 0. 0.] 
cards in discard: [1. 0. 0. 0. 3. 3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1] -> size -> 13 
action values: 1 
buys: 0 
player value: 1 
card supply: [30. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [8. 0. 8. 3. 0.] 
adversary cards in discard: [3. 0. 0. 0. 0. 3.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 8 8 3] -> size -> 13 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 1.9755051136016846





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[21.126543]
 [28.02974 ]
 [14.238612]
 [23.845078]
 [17.022858]
 [ 8.885735]
 [25.75778 ]
 [27.733643]
 [24.215157]
 [31.71298 ]
 [32.788425]
 [21.338917]
 [28.057924]
 [23.880638]
 [17.425657]
 [28.822643]
 [22.707836]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 1. 0. 0.] 
cards in discard: [1. 0. 0. 0. 3. 3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1] -> size -> 13 
action values: 0 
buys: 1 
player value: 6 
card supply: [30. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [8. 0. 8. 3. 0.] 
adversary cards in discard: [3. 0. 0. 0. 0. 3.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 8 8 3] -> size -> 13 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 19.64112663269043



buy possibilites: [-1] 
expected returns: [[4.939331]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 1. 0. 0.] 
cards in discard: [ 1.  0.  0.  0.  3.  3. 29.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29] -> size -> 14 
action values: 0 
buys: 0 
player value: 2 
card supply: [30. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [8. 0. 8. 3. 0.] 
adversary cards in discard: [3. 0. 0. 0. 0. 3.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 8 8 3] -> size -> 13 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5.   0.   0. -30.   0.   0.  20.   0.   0.   0.   0.   0.   0.   0.
  32.   0.] 
sum of rewards: 17.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 32.788429260253906






Player: 1 
cards in hand: [8. 0. 8. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [8. 0. 8. 3. 0.] 
cards in discard: [3. 0. 0. 0. 0. 3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3 8 8 3] -> size -> 13 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 29.  3.  1.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29] -> size -> 14 
adversary victory points: 3
player victory points: 4 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [8. 3. 0.] 
cards in discard: [3. 0. 0. 0. 0. 3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [8.] 
owned cards: [0 0 0 0 0 0 3 3 3 8 8 3] -> size -> 12 
action values: 0 
buys: 0 
player value: 0 
card supply: [30. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 29.  3.  1.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29] -> size -> 14 
adversary victory points: 3
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [8. 3. 0.] 
cards in discard: [3. 0. 0. 0. 0. 3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [8.] 
owned cards: [0 0 0 0 0 0 3 3 3 8 8 3] -> size -> 12 
action values: 0 
buys: 1 
player value: 1 
card supply: [30. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 29.  3.  1.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29] -> size -> 14 
adversary victory points: 3
player victory points: 4 





         -------------------- Turn: 5 -------------------- 
Player: 0 
cards in hand: [ 0. 29.  3.  1.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[ 4.4575863]
 [12.943156 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29.  3.  1.  0.] 
cards in discard: [] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29] -> size -> 14 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [8. 0. 3. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 3 3 3 8 8 3] -> size -> 12 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -35 

action type: buy - action -1
Learning step: 0
desired expected reward: 4.9393310546875



action possibilites: [-1.] 
expected returns: [[10.8296]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 1. 0. 0.] 
cards in discard: [] 
cards in deck: 8 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29] -> size -> 14 
action values: 1 
buys: 0 
player value: 1 
card supply: [30. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [8. 0. 3. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 3 3 3 8 8 3] -> size -> 12 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 11.364123344421387





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 8.733523 ]
 [15.659413 ]
 [ 1.7233498]
 [11.417434 ]
 [ 4.4792833]
 [-3.671051 ]
 [13.410306 ]
 [15.363313 ]
 [11.864586 ]
 [19.176558 ]
 [20.198755 ]
 [ 8.919089 ]
 [15.621606 ]
 [11.539084 ]
 [ 4.8668203]
 [16.4445   ]
 [10.389642 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 1. 0. 0.] 
cards in discard: [] 
cards in deck: 8 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29] -> size -> 14 
action values: 0 
buys: 1 
player value: 6 
card supply: [30. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [8. 0. 3. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 3 3 3 8 8 3] -> size -> 12 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 10.82960033416748



buy possibilites: [-1] 
expected returns: [[21.291212]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 1. 0. 0.] 
cards in discard: [29.] 
cards in deck: 8 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29] -> size -> 15 
action values: 0 
buys: 0 
player value: 2 
card supply: [30. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [8. 0. 3. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 3 3 3 8 8 3] -> size -> 12 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5.   0.   0. -30.   0.   0.  20.   0.   0.   0.   0.   0.   0.   0.
  32.   0.] 
sum of rewards: 17.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 20.19875144958496






Player: 1 
cards in hand: [8. 0. 3. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [8. 0. 3. 3. 0.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 3 3 3 8 8 3] -> size -> 12 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  1. 29.  0.  0.] 
adversary cards in discard: [29. 29.  0.  3.  1.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29] -> size -> 15 
adversary victory points: 3
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [8. 0. 3. 3. 0.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 3 3 3 8 8 3] -> size -> 12 
action values: 0 
buys: 1 
player value: 2 
card supply: [30. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  1. 29.  0.  0.] 
adversary cards in discard: [29. 29.  0.  3.  1.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29] -> size -> 15 
adversary victory points: 3
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [8. 0. 3. 3. 0.] 
cards in discard: [0.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 3 3 3 8 8 3 0] -> size -> 13 
action values: 0 
buys: 0 
player value: 2 
card supply: [29. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  1. 29.  0.  0.] 
adversary cards in discard: [29. 29.  0.  3.  1.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29] -> size -> 15 
adversary victory points: 3
player victory points: 4 





         -------------------- Turn: 6 -------------------- 
Player: 0 
cards in hand: [ 0.  1. 29.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[-3.9885209]
 [ 3.9891942]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  1. 29.  0.  0.] 
cards in discard: [29. 29.  0.  3.  1.  0.  0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29] -> size -> 15 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 8. 0. 3. 0.] 
adversary cards in discard: [0. 8. 0. 3. 3. 0.] 
adversary owned cards: [0 0 0 0 0 0 3 3 3 8 8 3 0] -> size -> 13 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -35 

action type: buy - action -1
Learning step: 0
desired expected reward: 21.29121208190918



action possibilites: [-1.] 
expected returns: [[13.791341]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 1. 0. 0. 3.] 
cards in discard: [29. 29.  0.  3.  1.  0.  0.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29] -> size -> 15 
action values: 1 
buys: 0 
player value: 1 
card supply: [29. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 8. 0. 3. 0.] 
adversary cards in discard: [0. 8. 0. 3. 3. 0.] 
adversary owned cards: [0 0 0 0 0 0 3 3 3 8 8 3 0] -> size -> 13 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 3.5847694873809814





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[13.659423 ]
 [19.712053 ]
 [ 7.9758587]
 [16.12635  ]
 [10.338524 ]
 [ 3.535457 ]
 [17.728848 ]
 [19.338627 ]
 [16.00706  ]
 [22.782307 ]
 [23.699656 ]
 [13.9670725]
 [19.778967 ]
 [15.844031 ]
 [10.678967 ]
 [20.398127 ]
 [15.091174 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 0. 0. 3.] 
cards in discard: [29. 29.  0.  3.  1.  0.  0.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29] -> size -> 15 
action values: 0 
buys: 1 
player value: 6 
card supply: [29. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 8. 0. 3. 0.] 
adversary cards in discard: [0. 8. 0. 3. 3. 0.] 
adversary owned cards: [0 0 0 0 0 0 3 3 3 8 8 3 0] -> size -> 13 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 13.791340827941895



buy possibilites: [-1] 
expected returns: [[22.155127]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 0. 0. 3.] 
cards in discard: [29. 29.  0.  3.  1.  0.  0. 29.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29] -> size -> 16 
action values: 0 
buys: 0 
player value: 2 
card supply: [29. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 8. 0. 3. 0.] 
adversary cards in discard: [0. 8. 0. 3. 3. 0.] 
adversary owned cards: [0 0 0 0 0 0 3 3 3 8 8 3 0] -> size -> 13 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5.   0.   0. -30.   0.   0.  20.   0.   0.   0.   0.   0.   0.   0.
  32.   0.] 
sum of rewards: 17.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 23.69965934753418






Player: 1 
cards in hand: [0. 8. 0. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 8. 0. 3. 0.] 
cards in discard: [0. 8. 0. 3. 3. 0.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 3 3 3 8 8 3 0] -> size -> 13 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29] -> size -> 16 
adversary victory points: 3
player victory points: 4 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [] 
cards in discard: [0. 8. 0. 3. 3. 0.] 
cards in deck: 2 
card top of deck: [] 
played cards: [8.] 
owned cards: [0 0 0 3 3 8 8 3 0] -> size -> 9 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29] -> size -> 16 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [0. 8. 0. 3. 3. 0.] 
cards in deck: 2 
card top of deck: [] 
played cards: [8.] 
owned cards: [0 0 0 3 3 8 8 3 0] -> size -> 9 
action values: 0 
buys: 1 
player value: 0 
card supply: [29. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29] -> size -> 16 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [0. 8. 0. 3. 3. 0. 0.] 
cards in deck: 2 
card top of deck: [] 
played cards: [8.] 
owned cards: [0 0 0 3 3 8 8 3 0 0] -> size -> 10 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29] -> size -> 16 
adversary victory points: 3
player victory points: 3 





         -------------------- Turn: 7 -------------------- 
Player: 0 
cards in hand: [3. 0. 0. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[14.810414]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29] -> size -> 16 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [8. 3. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 3 3 8 8 3 0 0] -> size -> 10 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 22.155126571655273





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[12.949293  ]
 [19.849579  ]
 [15.67809   ]
 [ 0.92355704]
 [19.571194  ]
 [16.003563  ]
 [15.694852  ]
 [14.576598  ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29] -> size -> 16 
action values: 0 
buys: 1 
player value: 3 
card supply: [28. 28. 30. 29. 30.  8. 10. 10. 10.  8. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [8. 3. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 3 3 8 8 3 0 0] -> size -> 10 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 13.935233116149902



buy possibilites: [-1] 
expected returns: [[21.975931]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 3. 0.] 
cards in discard: [1.] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1] -> size -> 17 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [8. 3. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 3 3 8 8 3 0 0] -> size -> 10 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0 54  0] 
sum of rewards: 49 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 19.849586486816406






Player: 1 
cards in hand: [8. 3. 0. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [8. 3. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 3 3 8 8 3 0 0] -> size -> 10 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [29.  0. 29. 29.  1.] 
adversary cards in discard: [1. 3. 0. 0. 3. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1] -> size -> 17 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [8. 3. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 3 3 8 8 3 0 0] -> size -> 10 
action values: 0 
buys: 1 
player value: 2 
card supply: [28. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [29.  0. 29. 29.  1.] 
adversary cards in discard: [1. 3. 0. 0. 3. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1] -> size -> 17 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [8. 3. 0. 0. 3.] 
cards in discard: [0.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 3 3 8 8 3 0 0 0] -> size -> 11 
action values: 0 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [29.  0. 29. 29.  1.] 
adversary cards in discard: [1. 3. 0. 0. 3. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1] -> size -> 17 
adversary victory points: 3
player victory points: 3 





         -------------------- Turn: 8 -------------------- 
Player: 0 
cards in hand: [29.  0. 29. 29.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29.] 
expected returns: [[12.031653]
 [20.388037]
 [20.388037]
 [20.388037]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0. 29. 29.  1.] 
cards in discard: [1. 3. 0. 0. 3. 0.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1] -> size -> 17 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 8. 0.] 
adversary cards in discard: [0. 8. 3. 0. 0. 3.] 
adversary owned cards: [0 0 0 3 3 8 8 3 0 0 0] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 21.97593116760254



action possibilites: [-1. 29. 29.] 
expected returns: [[ 5.6236773]
 [14.578305 ]
 [14.578305 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29. 29.  1.  0.] 
cards in discard: [1. 3. 0. 0. 3. 0.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1] -> size -> 17 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 8. 0.] 
adversary cards in discard: [0. 8. 3. 0. 0. 3.] 
adversary owned cards: [0 0 0 3 3 8 8 3 0 0 0] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 17.79488182067871



action possibilites: [-1. 29.] 
expected returns: [[23.764202]
 [33.5699  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29.  1.  0.  1.] 
cards in discard: [1. 3. 0. 0. 3. 0.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1] -> size -> 17 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 8. 0.] 
adversary cards in discard: [0. 8. 3. 0. 0. 3.] 
adversary owned cards: [0 0 0 3 3 8 8 3 0 0 0] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 14.578312873840332



action possibilites: [-1.] 
expected returns: [[53.42308]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 1. 0. 1. 0.] 
cards in discard: [1. 3. 0. 0. 3. 0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1] -> size -> 17 
action values: 1 
buys: 0 
player value: 3 
card supply: [27. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 8. 0.] 
adversary cards in discard: [0. 8. 3. 0. 0. 3.] 
adversary owned cards: [0 0 0 3 3 8 8 3 0 0 0] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 33.56990432739258





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  5.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[55.454445]
 [62.736305]
 [48.377365]
 [58.244423]
 [51.183586]
 [62.233414]
 [42.8508  ]
 [60.302387]
 [62.356453]
 [58.390827]
 [66.61647 ]
 [67.71393 ]
 [55.724686]
 [62.67836 ]
 [58.17998 ]
 [51.558582]
 [63.60157 ]
 [57.26553 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 0. 1. 0.] 
cards in discard: [1. 3. 0. 0. 3. 0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1] -> size -> 17 
action values: 0 
buys: 1 
player value: 10 
card supply: [27. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 8. 0.] 
adversary cards in discard: [0. 8. 3. 0. 0. 3.] 
adversary owned cards: [0 0 0 3 3 8 8 3 0 0 0] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 53.42308044433594



buy possibilites: [-1] 
expected returns: [[36.377506]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 0. 1. 0.] 
cards in discard: [ 1.  3.  0.  0.  3.  0. 29.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29] -> size -> 18 
action values: 0 
buys: 0 
player value: 6 
card supply: [27. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  5. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 8. 0.] 
adversary cards in discard: [0. 8. 3. 0. 0. 3.] 
adversary owned cards: [0 0 0 3 3 8 8 3 0 0 0] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5.  0.  0.  0.  0.  0. 60.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 87.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 67.71392059326172






Player: 1 
cards in hand: [0. 0. 3. 8. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3. 8. 0.] 
cards in discard: [0. 8. 3. 0. 0. 3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 3 3 8 8 3 0 0 0] -> size -> 11 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  5. 10. 10. 10. 10. 10.] 
adversary cards in hand: [29.  0. 29.  3.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29] -> size -> 18 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 8. 0.] 
cards in discard: [0. 8. 3. 0. 0. 3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 3 3 8 8 3 0 0 0] -> size -> 11 
action values: 0 
buys: 1 
player value: 3 
card supply: [27. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  5. 10. 10. 10. 10. 10.] 
adversary cards in hand: [29.  0. 29.  3.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29] -> size -> 18 
adversary victory points: 3
player victory points: 3 


 --------- NOTHING HAPPENED --------- 



         -------------------- Turn: 9 -------------------- 
Player: 0 
cards in hand: [29.  0. 29.  3.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[-4.1475296]
 [ 3.3662884]
 [ 3.3662884]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0. 29.  3.  0.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29] -> size -> 18 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  5. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 3 3 8 8 3 0 0 0] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 36.377506256103516



action possibilites: [-1. 29.] 
expected returns: [[ 6.492917]
 [16.595465]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29.  3.  0.  0.] 
cards in discard: [] 
cards in deck: 12 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29] -> size -> 18 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  5. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 3 3 8 8 3 0 0 0] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 2.0805933475494385



action possibilites: [-1.] 
expected returns: [[37.49669]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29] -> size -> 18 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  5. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 3 3 8 8 3 0 0 0] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 16.595468521118164





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[37.510834]
 [44.73146 ]
 [30.242697]
 [40.30835 ]
 [33.116055]
 [24.611666]
 [42.401943]
 [44.36628 ]
 [40.64927 ]
 [48.347313]
 [49.42166 ]
 [37.733322]
 [44.671913]
 [40.368073]
 [33.49368 ]
 [45.53903 ]
 [39.296757]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29] -> size -> 18 
action values: 0 
buys: 1 
player value: 6 
card supply: [27. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  5. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 3 3 8 8 3 0 0 0] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 37.49668884277344



buy possibilites: [-1] 
expected returns: [[42.53028]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [29.] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29] -> size -> 19 
action values: 0 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  4. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 3 3 8 8 3 0 0 0] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5.  0.  0.  0.  0.  0. 40.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 67.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 49.42165756225586






Player: 1 
cards in hand: [0. 0. 0. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 3 3 8 8 3 0 0 0] -> size -> 11 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  4. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 1. 0. 1. 1.] 
adversary cards in discard: [29. 29. 29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29] -> size -> 19 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: 11.0 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 3 3 8 8 3 0 0 0] -> size -> 11 
action values: 0 
buys: 1 
player value: 4 
card supply: [27. 27. 30. 29. 30.  8. 10. 10. 10.  8. 10.  4. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 1. 0. 1. 1.] 
adversary cards in discard: [29. 29. 29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29] -> size -> 19 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [11.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  8  8  3  0  0  0 11] -> size -> 12 
action values: 0 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  8. 10.  4. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 1. 0. 1. 1.] 
adversary cards in discard: [29. 29. 29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29] -> size -> 19 
adversary victory points: 3
player victory points: 3 





         -------------------- Turn: 10 -------------------- 
Player: 0 
cards in hand: [0. 1. 0. 1. 1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[41.61712]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 1. 0. 1. 1.] 
cards in discard: [29. 29. 29.  0.  3.  0.  0.  0.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29] -> size -> 19 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  8. 10.  4. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 8. 3. 0.] 
adversary cards in discard: [11.  0.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  3  3  8  8  3  0  0  0 11] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 42.53028106689453





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  5.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[39.474987]
 [45.82736 ]
 [33.214115]
 [41.945724]
 [35.754936]
 [45.492886]
 [28.340452]
 [43.75464 ]
 [45.434433]
 [42.110203]
 [48.86983 ]
 [49.83847 ]
 [39.705894]
 [45.75197 ]
 [41.903942]
 [36.05066 ]
 [46.521942]
 [41.05142 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 0. 1. 1.] 
cards in discard: [29. 29. 29.  0.  3.  0.  0.  0.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29] -> size -> 19 
action values: 0 
buys: 1 
player value: 8 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  8. 10.  4. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 8. 3. 0.] 
adversary cards in discard: [11.  0.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  3  3  8  8  3  0  0  0 11] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 40.67066955566406



buy possibilites: [-1] 
expected returns: [[27.006987]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 0. 1. 1.] 
cards in discard: [29. 29. 29.  0.  3.  0.  0.  0. 29.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29] -> size -> 20 
action values: 0 
buys: 0 
player value: 4 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  8. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 8. 3. 0.] 
adversary cards in discard: [11.  0.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  3  3  8  8  3  0  0  0 11] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 27.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 49.838478088378906






Player: 1 
cards in hand: [3. 0. 8. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 8. 3. 0.] 
cards in discard: [11.  0.  0.  0.  0.  3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  8  8  3  0  0  0 11] -> size -> 12 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  8. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [29. 29.  0. 29.  3.] 
adversary cards in discard: [29. 29. 29.  0.  3.  0.  0.  0. 29.  0.  1.  0.  1.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29] -> size -> 20 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 8. 3. 0.] 
cards in discard: [11.  0.  0.  0.  0.  3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  8  8  3  0  0  0 11] -> size -> 12 
action values: 0 
buys: 1 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  8. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [29. 29.  0. 29.  3.] 
adversary cards in discard: [29. 29. 29.  0.  3.  0.  0.  0. 29.  0.  1.  0.  1.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29] -> size -> 20 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 8. 3. 0.] 
cards in discard: [11.  0.  0.  0.  0.  3.  8.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  8  8  3  0  0  0 11  8] -> size -> 13 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [29. 29.  0. 29.  3.] 
adversary cards in discard: [29. 29. 29.  0.  3.  0.  0.  0. 29.  0.  1.  0.  1.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29] -> size -> 20 
adversary victory points: 3
player victory points: 3 





         -------------------- Turn: 11 -------------------- 
Player: 0 
cards in hand: [29. 29.  0. 29.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29.] 
expected returns: [[78.37402]
 [85.8257 ]
 [85.8257 ]
 [85.8257 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29.  0. 29.  3.] 
cards in discard: [29. 29. 29.  0.  3.  0.  0.  0. 29.  0.  1.  0.  1.  1.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29] -> size -> 20 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  3.  0.  3.  8.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  8  8  3  0  0  0 11  8] -> size -> 13 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 27.006986618041992



action possibilites: [-1. 29. 29.] 
expected returns: [[80.04261]
 [87.54384]
 [87.54384]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0. 29.  3.  3.] 
cards in discard: [29. 29. 29.  0.  3.  0.  0.  0. 29.  0.  1.  0.  1.  1.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29] -> size -> 20 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  3.  0.  3.  8.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  8  8  3  0  0  0 11  8] -> size -> 13 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 82.21463012695312



action possibilites: [-1. 29. 29.] 
expected returns: [[22.818678]
 [31.805529]
 [31.805529]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29.  3.  3. 29.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29] -> size -> 20 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  3.  0.  3.  8.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  8  8  3  0  0  0 11  8] -> size -> 13 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 87.54383850097656



action possibilites: [-1. 29. 29.] 
expected returns: [[38.058   ]
 [47.033875]
 [47.033875]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3.  3. 29. 29.] 
cards in discard: [] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29] -> size -> 20 
action values: 1 
buys: 0 
player value: 3 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  3.  0.  3.  8.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  8  8  3  0  0  0 11  8] -> size -> 13 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 31.805532455444336



action possibilites: [-1. 29.] 
expected returns: [[44.359566]
 [54.320625]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3.  3. 29.  0.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29] -> size -> 20 
action values: 1 
buys: 0 
player value: 4 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  3.  0.  3.  8.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  8  8  3  0  0  0 11  8] -> size -> 13 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 47.03386688232422



action possibilites: [-1.] 
expected returns: [[49.4769]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 3. 0. 1.] 
cards in discard: [] 
cards in deck: 10 
card top of deck: [] 
played cards: [29. 29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29] -> size -> 20 
action values: 1 
buys: 0 
player value: 5 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  3.  0.  3.  8.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  8  8  3  0  0  0 11  8] -> size -> 13 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0   0   0   0 100   0   0   0   0   0   0   0   0   0] 
sum of rewards: 95 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 54.32062911987305





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  5.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[49.24534 ]
 [56.37857 ]
 [42.35011 ]
 [52.113365]
 [45.13218 ]
 [55.974594]
 [36.969955]
 [54.079258]
 [55.986286]
 [52.118164]
 [59.987324]
 [61.026295]
 [49.564636]
 [56.387966]
 [51.906815]
 [45.545822]
 [57.183876]
 [50.98169 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 3. 0. 1.] 
cards in discard: [] 
cards in deck: 10 
card top of deck: [] 
played cards: [29. 29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29] -> size -> 20 
action values: 0 
buys: 1 
player value: 9 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  3.  0.  3.  8.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  8  8  3  0  0  0 11  8] -> size -> 13 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0   0   0   0 100   0   0   0   0   0   0   0   0   0] 
sum of rewards: 95 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 49.476898193359375



buy possibilites: [-1] 
expected returns: [[81.55559]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 3. 0. 1.] 
cards in discard: [29.] 
cards in deck: 10 
card top of deck: [] 
played cards: [29. 29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29] -> size -> 21 
action values: 0 
buys: 0 
player value: 5 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  2. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  3.  0.  3.  8.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  8  8  3  0  0  0 11  8] -> size -> 13 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5.   0.   0.   0.   0.   0. 100.   0.   0.   0.   0.   0.   0.   0.
  32.   0.] 
sum of rewards: 127.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 61.02629470825195






Player: 1 
cards in hand: [11.  3.  0.  3.  8.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [11.  3.  0.  3.  8.] 
cards in discard: [] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  8  8  3  0  0  0 11  8] -> size -> 13 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  2. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 29.  0.  3.  0.] 
adversary cards in discard: [29. 29. 29. 29. 29. 29.  0.  3.  3.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29] -> size -> 21 
adversary victory points: 3
player victory points: 3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [11.  0.  3.] 
cards in discard: [] 
cards in deck: 8 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  0  3  8  8  3  0  0  0 11  8] -> size -> 12 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  2. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 29.  0.  3.  0.] 
adversary cards in discard: [29. 29. 29. 29. 29. 29.  0.  3.  3.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29] -> size -> 21 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [11.  0.  3.] 
cards in discard: [] 
cards in deck: 8 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  0  3  8  8  3  0  0  0 11  8] -> size -> 12 
action values: 0 
buys: 1 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  2. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 29.  0.  3.  0.] 
adversary cards in discard: [29. 29. 29. 29. 29. 29.  0.  3.  3.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29] -> size -> 21 
adversary victory points: 3
player victory points: 2 





         -------------------- Turn: 12 -------------------- 
Player: 0 
cards in hand: [ 0. 29.  0.  3.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[88.106865]
 [96.66114 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29.  0.  3.  0.] 
cards in discard: [29. 29. 29. 29. 29. 29.  0.  3.  3.  0.  1.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29] -> size -> 21 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  2. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 8. 0. 8.] 
adversary cards in discard: [ 8. 11.  0.  3.] 
adversary owned cards: [ 0  0  0  3  8  8  3  0  0  0 11  8] -> size -> 12 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 81.55558776855469



action possibilites: [-1.] 
expected returns: [[90.869736]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3. 0. 1.] 
cards in discard: [29. 29. 29. 29. 29. 29.  0.  3.  3.  0.  1.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29] -> size -> 21 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  2. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 8. 0. 8.] 
adversary cards in discard: [ 8. 11.  0.  3.] 
adversary owned cards: [ 0  0  0  3  8  8  3  0  0  0 11  8] -> size -> 12 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 93.59074401855469





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 91.94927 ]
 [ 98.24429 ]
 [ 85.38696 ]
 [ 94.33294 ]
 [ 87.887344]
 [ 80.266075]
 [ 96.28331 ]
 [ 97.92799 ]
 [ 94.68833 ]
 [101.320724]
 [102.20855 ]
 [ 92.11606 ]
 [ 98.07191 ]
 [ 94.48329 ]
 [ 88.20299 ]
 [ 98.93064 ]
 [ 93.643265]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 1.] 
cards in discard: [29. 29. 29. 29. 29. 29.  0.  3.  3.  0.  1.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29] -> size -> 21 
action values: 0 
buys: 1 
player value: 6 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  2. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 8. 0. 8.] 
adversary cards in discard: [ 8. 11.  0.  3.] 
adversary owned cards: [ 0  0  0  3  8  8  3  0  0  0 11  8] -> size -> 12 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 90.86973571777344



buy possibilites: [-1] 
expected returns: [[207.51688]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 1.] 
cards in discard: [29. 29. 29. 29. 29. 29.  0.  3.  3.  0.  1. 29.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
action values: 0 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 8. 0. 8.] 
adversary cards in discard: [ 8. 11.  0.  3.] 
adversary owned cards: [ 0  0  0  3  8  8  3  0  0  0 11  8] -> size -> 12 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5.  0.  0. 30.  0.  0. 20.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 77.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 102.20854187011719






Player: 1 
cards in hand: [0. 0. 8. 0. 8.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 8. 0. 8.] 
cards in discard: [ 8. 11.  0.  3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  8  8  3  0  0  0 11  8] -> size -> 12 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 1. 29.  1.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
adversary victory points: 3
player victory points: 2 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [8.] 
cards in discard: [ 8. 11.  0.  3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 3  8  8  3  0  0  0 11  8] -> size -> 9 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 1. 29.  1.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [8.] 
cards in discard: [ 8. 11.  0.  3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 3  8  8  3  0  0  0 11  8] -> size -> 9 
action values: 0 
buys: 1 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 1. 29.  1.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
adversary victory points: 3
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [8.] 
cards in discard: [ 8. 11.  0.  3.  0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 3  8  8  3  0  0  0 11  8  0] -> size -> 10 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 1. 29.  1.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
adversary victory points: 3
player victory points: 2 





         -------------------- Turn: 13 -------------------- 
Player: 0 
cards in hand: [ 1. 29.  1.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[17.68712 ]
 [27.622057]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 29.  1.  0.  0.] 
cards in discard: [] 
cards in deck: 17 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  8  3  0  0  0 11  8  0] -> size -> 10 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 207.51687622070312



action possibilites: [-1. 29.] 
expected returns: [[39.081615]
 [48.556595]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  1.  0.  0. 29.] 
cards in discard: [] 
cards in deck: 16 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 1 
card supply: [26. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  8  3  0  0  0 11  8  0] -> size -> 10 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 25.707674026489258



action possibilites: [-1.] 
expected returns: [[48.71569]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 1. 0. 0. 1.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 2 
card supply: [26. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  8  3  0  0  0 11  8  0] -> size -> 10 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 48.556602478027344





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  5.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[52.761494]
 [59.899876]
 [45.86557 ]
 [55.633892]
 [48.64818 ]
 [59.495693]
 [40.48513 ]
 [57.59961 ]
 [59.50845 ]
 [55.635406]
 [63.513245]
 [64.55251 ]
 [53.081703]
 [59.91048 ]
 [55.423782]
 [49.063286]
 [60.70582 ]
 [54.497272]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 1. 0. 0. 1.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29] -> size -> 22 
action values: 0 
buys: 1 
player value: 10 
card supply: [26. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  8  3  0  0  0 11  8  0] -> size -> 10 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 48.71569061279297



buy possibilites: [-1] 
expected returns: [[76.38306]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 1. 0. 0. 1.] 
cards in discard: [29.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29] -> size -> 23 
action values: 0 
buys: 0 
player value: 6 
card supply: [26. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  8  3  0  0  0 11  8  0] -> size -> 10 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5.  0.  0. 30.  0.  0. 40.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 97.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 64.55252075195312






Player: 1 
cards in hand: [3. 0. 0. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  8  3  0  0  0 11  8  0] -> size -> 10 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [29.  0. 29.  3.  0.] 
adversary cards in discard: [29. 29. 29.  1.  1.  0.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29] -> size -> 23 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  8  3  0  0  0 11  8  0] -> size -> 10 
action values: 0 
buys: 1 
player value: 3 
card supply: [26. 27. 30. 29. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [29.  0. 29.  3.  0.] 
adversary cards in discard: [29. 29. 29.  1.  1.  0.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29] -> size -> 23 
adversary victory points: 3
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 3.] 
cards in discard: [1.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  8  3  0  0  0 11  8  0  1] -> size -> 11 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 29. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [29.  0. 29.  3.  0.] 
adversary cards in discard: [29. 29. 29.  1.  1.  0.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29] -> size -> 23 
adversary victory points: 3
player victory points: 2 





         -------------------- Turn: 14 -------------------- 
Player: 0 
cards in hand: [29.  0. 29.  3.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[64.17828]
 [72.8457 ]
 [72.8457 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0. 29.  3.  0.] 
cards in discard: [29. 29. 29.  1.  1.  0.  0.  1.] 
cards in deck: 10 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29] -> size -> 23 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 29. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  0.  8.  8.  8.] 
adversary cards in discard: [1. 3. 0. 0. 0. 3.] 
adversary owned cards: [ 3  8  8  3  0  0  0 11  8  0  1] -> size -> 11 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 76.383056640625



action possibilites: [-1. 29.] 
expected returns: [[123.69815]
 [132.33078]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3.  0. 29.] 
cards in discard: [29. 29. 29.  1.  1.  0.  0.  1. 29.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29] -> size -> 23 
action values: 1 
buys: 0 
player value: 1 
card supply: [26. 26. 30. 29. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  0.  8.  8.  8.] 
adversary cards in discard: [1. 3. 0. 0. 0. 3.] 
adversary owned cards: [ 3  8  8  3  0  0  0 11  8  0  1] -> size -> 11 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 63.70884704589844



action possibilites: [-1.] 
expected returns: [[135.22368]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 0.] 
cards in discard: [29. 29. 29.  1.  1.  0.  0.  1. 29. 29.] 
cards in deck: 8 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29] -> size -> 23 
action values: 1 
buys: 0 
player value: 2 
card supply: [26. 26. 30. 29. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  0.  8.  8.  8.] 
adversary cards in discard: [1. 3. 0. 0. 0. 3.] 
adversary owned cards: [ 3  8  8  3  0  0  0 11  8  0  1] -> size -> 11 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 125.84190368652344





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[139.60284]
 [146.3631 ]
 [142.0134 ]
 [126.08409]
 [144.30606]
 [146.16966]
 [143.0011 ]
 [139.60693]
 [142.65056]
 [147.09627]
 [141.47986]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0.] 
cards in discard: [29. 29. 29.  1.  1.  0.  0.  1. 29. 29.] 
cards in deck: 8 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29] -> size -> 23 
action values: 0 
buys: 1 
player value: 4 
card supply: [26. 26. 30. 29. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  0.  8.  8.  8.] 
adversary cards in discard: [1. 3. 0. 0. 0. 3.] 
adversary owned cards: [ 3  8  8  3  0  0  0 11  8  0  1] -> size -> 11 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 135.2236785888672



buy possibilites: [-1] 
expected returns: [[350.96243]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0.] 
cards in discard: [29. 29. 29.  1.  1.  0.  0.  1. 29. 29. 15.] 
cards in deck: 8 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15] -> size -> 24 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 29. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10.  9.] 
adversary cards in hand: [11.  0.  8.  8.  8.] 
adversary cards in discard: [1. 3. 0. 0. 0. 3.] 
adversary owned cards: [ 3  8  8  3  0  0  0 11  8  0  1] -> size -> 11 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[ -5   0   0  30   0   0  40   0   0   0   0   0   0   0 128   0] 
sum of rewards: 193 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 147.09628295898438






Player: 1 
cards in hand: [11.  0.  8.  8.  8.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.  8.  8.  8.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [11.  0.  8.  8.  8.] 
cards in discard: [1. 3. 0. 0. 0. 3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  8  3  0  0  0 11  8  0  1] -> size -> 11 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 29. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10.  9.] 
adversary cards in hand: [29.  3.  3. 29.  0.] 
adversary cards in discard: [29. 29. 29.  1.  1.  0.  0.  1. 29. 29. 15. 29. 29.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15] -> size -> 24 
adversary victory points: 3
player victory points: 2 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 8. 8. 8.] 
cards in discard: [1. 3. 0. 0. 0. 3. 3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 3  8  8  3  0  0  0 11  8  0  1  3] -> size -> 12 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10.  9.] 
adversary cards in hand: [29.  3.  3. 29.  0.] 
adversary cards in discard: [29. 29. 29.  1.  1.  0.  0.  1. 29. 29. 15. 29. 29.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15] -> size -> 24 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 8. 8. 8.] 
cards in discard: [1. 3. 0. 0. 0. 3. 3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 3  8  8  3  0  0  0 11  8  0  1  3] -> size -> 12 
action values: 0 
buys: 1 
player value: 1 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10.  9.] 
adversary cards in hand: [29.  3.  3. 29.  0.] 
adversary cards in discard: [29. 29. 29.  1.  1.  0.  0.  1. 29. 29. 15. 29. 29.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15] -> size -> 24 
adversary victory points: 3
player victory points: 3 





         -------------------- Turn: 15 -------------------- 
Player: 0 
cards in hand: [29.  3.  3. 29.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[147.51761]
 [156.06656]
 [156.06656]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  3.  3. 29.  0.] 
cards in discard: [29. 29. 29.  1.  1.  0.  0.  1. 29. 29. 15. 29. 29.  0.  3.  0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15] -> size -> 24 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10.  9.] 
adversary cards in hand: [0. 8. 8. 0. 1.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  8  3  0  0  0 11  8  0  1  3] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 350.9624328613281



action possibilites: [-1. 29.] 
expected returns: [[127.018295]
 [135.4266  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  3.  0. 29.] 
cards in discard: [29. 29. 29.  1.  1.  0.  0.  1. 29. 29. 15. 29. 29.  0.  3.  0. 29.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15] -> size -> 24 
action values: 1 
buys: 0 
player value: 1 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10.  9.] 
adversary cards in hand: [0. 8. 8. 0. 1.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  8  3  0  0  0 11  8  0  1  3] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 147.3681640625



action possibilites: [-1.] 
expected returns: [[71.58731]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0.] 
cards in discard: [29. 29. 29.  1.  1.  0.  0.  1. 29. 29. 15. 29. 29.  0.  3.  0. 29.  3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15] -> size -> 24 
action values: 1 
buys: 0 
player value: 2 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10.  9.] 
adversary cards in hand: [0. 8. 8. 0. 1.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  8  3  0  0  0 11  8  0  1  3] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 131.4500732421875





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[69.65841 ]
 [75.97681 ]
 [71.976494]
 [58.302917]
 [74.071655]
 [75.65285 ]
 [72.43479 ]
 [69.784836]
 [72.26207 ]
 [76.64525 ]
 [71.49963 ]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0.] 
cards in discard: [29. 29. 29.  1.  1.  0.  0.  1. 29. 29. 15. 29. 29.  0.  3.  0. 29.  3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15] -> size -> 24 
action values: 0 
buys: 1 
player value: 4 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10.  9.] 
adversary cards in hand: [0. 8. 8. 0. 1.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  8  3  0  0  0 11  8  0  1  3] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 71.58731079101562



buy possibilites: [-1] 
expected returns: [[35.148895]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0.] 
cards in discard: [29. 29. 29.  1.  1.  0.  0.  1. 29. 29. 15. 29. 29.  0.  3.  0. 29.  3.
 15.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15
 15] -> size -> 25 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [0. 8. 8. 0. 1.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  8  3  0  0  0 11  8  0  1  3] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0   0   0   0  40   0   0   0   0   0   0   0 128   0] 
sum of rewards: 163 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 76.645263671875






Player: 1 
cards in hand: [0. 8. 8. 0. 1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 8. 8. 0. 1.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  8  3  0  0  0 11  8  0  1  3] -> size -> 12 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [15. 29.  0.  3.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15
 15] -> size -> 25 
adversary victory points: 3
player victory points: 3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 1.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3] -> size -> 10 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [15. 29.  0.  3.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15
 15] -> size -> 25 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3] -> size -> 10 
action values: 0 
buys: 1 
player value: 3 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  7. 10.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [15. 29.  0.  3.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15
 15] -> size -> 25 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1.] 
cards in discard: [8.] 
cards in deck: 7 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3  8] -> size -> 11 
action values: 0 
buys: 0 
player value: 1 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  6. 10.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [15. 29.  0.  3.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15
 15] -> size -> 25 
adversary victory points: 3
player victory points: 3 





         -------------------- Turn: 16 -------------------- 
Player: 0 
cards in hand: [15. 29.  0.  3.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 29.] 
expected returns: [[24.947205]
 [30.972841]
 [34.732143]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 29.  0.  3.  0.] 
cards in discard: [] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15
 15] -> size -> 25 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  6. 10.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 3. 11.  3.  0.  8.] 
adversary cards in discard: [8. 8. 0. 1.] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 35.148895263671875



action possibilites: [-1. 15.] 
expected returns: [[57.346123]
 [63.360123]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0.  3.  0.] 
cards in discard: [1.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15
 15] -> size -> 25 
action values: 1 
buys: 0 
player value: 1 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  6. 10.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 3. 11.  3.  0.  8.] 
adversary cards in discard: [8. 8. 0. 1.] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 27.0458927154541



action possibilites: [-1] 
expected returns: [[7.9816656]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0.] 
cards in discard: [1.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15] -> size -> 24 
action values: 0 
buys: 0 
player value: 4 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  6. 10.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 3. 11.  3.  0.  8.] 
adversary cards in discard: [8. 8. 0. 1.] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 63.360130310058594





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 8.495734 ]
 [15.585124 ]
 [11.351533 ]
 [ 4.419242 ]
 [-3.6856267]
 [13.306649 ]
 [15.184918 ]
 [11.315633 ]
 [19.161312 ]
 [ 8.823022 ]
 [15.590199 ]
 [11.121154 ]
 [ 4.828577 ]
 [16.383266 ]
 [10.234945 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0.] 
cards in discard: [1.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15] -> size -> 24 
action values: 0 
buys: 1 
player value: 5 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  6. 10.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 3. 11.  3.  0.  8.] 
adversary cards in discard: [8. 8. 0. 1.] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action -1
Learning step: 0
desired expected reward: 7.98166561126709



buy possibilites: [-1] 
expected returns: [[30.506067]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0.] 
cards in discard: [ 1. 25.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25] -> size -> 25 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  6.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 3. 11.  3.  0.  8.] 
adversary cards in discard: [8. 8. 0. 1.] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0   0   0   0  40   0   0   0   0   0   0   0 250   0] 
sum of rewards: 285 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 19.161317825317383






Player: 1 
cards in hand: [ 3. 11.  3.  0.  8.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.  8.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 11.  3.  0.  8.] 
cards in discard: [8. 8. 0. 1.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3  8] -> size -> 11 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  6.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 3.  3. 29.  1. 29.] 
adversary cards in discard: [ 1. 25. 29. 15.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25] -> size -> 25 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 11.  3.  0.  8.] 
cards in discard: [8. 8. 0. 1.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3  8] -> size -> 11 
action values: 0 
buys: 1 
player value: 1 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  6.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 3.  3. 29.  1. 29.] 
adversary cards in discard: [ 1. 25. 29. 15.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25] -> size -> 25 
adversary victory points: 3
player victory points: 3 


 --------- NOTHING HAPPENED --------- 



         -------------------- Turn: 17 -------------------- 
Player: 0 
cards in hand: [ 3.  3. 29.  1. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[65.56569]
 [75.80693]
 [75.80693]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  3. 29.  1. 29.] 
cards in discard: [ 1. 25. 29. 15.  3.  0.] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25] -> size -> 25 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  6.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [8. 0. 1. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 30.506067276000977



action possibilites: [-1. 29. 15.] 
expected returns: [[ 98.80238 ]
 [108.065544]
 [104.475555]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  1. 29. 15.] 
cards in discard: [ 1. 25. 29. 15.  3.  0.  3.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25] -> size -> 25 
action values: 1 
buys: 0 
player value: 1 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  6.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [8. 0. 1. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 64.50666046142578



action possibilites: [-1. 15. 29.] 
expected returns: [[107.802505]
 [113.43346 ]
 [116.89904 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 15. 29.] 
cards in discard: [ 1. 25. 29. 15.  3.  0.  3.  3.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25] -> size -> 25 
action values: 1 
buys: 0 
player value: 2 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  6.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [8. 0. 1. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 101.74382019042969



action possibilites: [-1. 29.] 
expected returns: [[139.43105]
 [148.61041]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 29.] 
cards in discard: [ 1. 25. 29. 15.  3.  0.  3.  3. 15.] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25] -> size -> 25 
action values: 1 
buys: 0 
player value: 3 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  6.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [8. 0. 1. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 110.3050765991211



action possibilites: [-1.] 
expected returns: [[230.85689]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0.] 
cards in discard: [ 1. 25. 29. 15.  3.  0.  3.  3. 15.  1.] 
cards in deck: 10 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25] -> size -> 25 
action values: 1 
buys: 0 
player value: 4 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  6.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [8. 0. 1. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 141.85305786132812





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[238.26312]
 [245.66199]
 [240.91573]
 [232.61499]
 [223.10887]
 [243.47107]
 [245.61444]
 [242.20505]
 [249.35907]
 [238.21806]
 [245.23798]
 [241.74687]
 [233.09654]
 [246.48056]
 [240.29878]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [ 1. 25. 29. 15.  3.  0.  3.  3. 15.  1.] 
cards in deck: 10 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25] -> size -> 25 
action values: 0 
buys: 1 
player value: 5 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  6.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [8. 0. 1. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 230.8568878173828



buy possibilites: [-1] 
expected returns: [[229.99185]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [ 1. 25. 29. 15.  3.  0.  3.  3. 15.  1. 25.] 
cards in deck: 10 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25 25] -> size -> 26 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  6.  8.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [8. 0. 1. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0   0   0   0  80   0   0   0   0   0   0   0 250   0] 
sum of rewards: 325 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 249.35910034179688






Player: 1 
cards in hand: [8. 0. 1. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [8. 0. 1. 3. 0.] 
cards in discard: [] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3  8] -> size -> 11 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  6.  8.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [29.  0. 29. 29. 29.] 
adversary cards in discard: [ 1. 25. 29. 15.  3.  0.  3.  3. 15.  1. 25. 29. 29. 29. 29.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25 25] -> size -> 26 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: 16.0 : ['Remodel' '16' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [8. 0. 1. 3. 0.] 
cards in discard: [] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3  8] -> size -> 11 
action values: 0 
buys: 1 
player value: 4 
card supply: [26. 26. 30. 28. 30.  8. 10. 10.  9.  6.  8.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [29.  0. 29. 29. 29.] 
adversary cards in discard: [ 1. 25. 29. 15.  3.  0.  3.  3. 15.  1. 25. 29. 29. 29. 29.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25 25] -> size -> 26 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [8. 0. 1. 3. 0.] 
cards in discard: [16.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16] -> size -> 12 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 28. 30.  8. 10.  9.  9.  6.  8.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [29.  0. 29. 29. 29.] 
adversary cards in discard: [ 1. 25. 29. 15.  3.  0.  3.  3. 15.  1. 25. 29. 29. 29. 29.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25 25] -> size -> 26 
adversary victory points: 3
player victory points: 3 





         -------------------- Turn: 18 -------------------- 
Player: 0 
cards in hand: [29.  0. 29. 29. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29. 29.] 
expected returns: [[121.05645]
 [129.11838]
 [129.11838]
 [129.11838]
 [129.11838]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0. 29. 29. 29.] 
cards in discard: [ 1. 25. 29. 15.  3.  0.  3.  3. 15.  1. 25. 29. 29. 29. 29.  0.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25 25] -> size -> 26 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 28. 30.  8. 10.  9.  9.  6.  8.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  8. 11.  8.  3.] 
adversary cards in discard: [16.  8.  0.  1.  3.  0.] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 229.99185180664062



action possibilites: [-1. 29. 29.] 
expected returns: [[27.84545]
 [35.72531]
 [35.72531]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29. 29.  1.] 
cards in discard: [ 1. 25. 29. 15.  3.  0.  3.  3. 15.  1. 25. 29. 29. 29. 29.  0. 29.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25 25] -> size -> 26 
action values: 1 
buys: 0 
player value: 1 
card supply: [26. 26. 30. 28. 30.  8. 10.  9.  9.  6.  8.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  8. 11.  8.  3.] 
adversary cards in discard: [16.  8.  0.  1.  3.  0.] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 122.13565063476562



action possibilites: [-1.] 
expected returns: [[45.390297]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 1. 0.] 
cards in discard: [ 1. 25. 29. 15.  3.  0.  3.  3. 15.  1. 25. 29. 29. 29. 29.  0. 29. 29.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25 25] -> size -> 26 
action values: 1 
buys: 0 
player value: 2 
card supply: [26. 26. 30. 28. 30.  8. 10.  9.  9.  6.  8.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  8. 11.  8.  3.] 
adversary cards in discard: [16.  8.  0.  1.  3.  0.] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 32.08129119873047





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[40.721386]
 [46.49311 ]
 [34.861927]
 [42.912224]
 [37.126415]
 [30.163607]
 [44.772602]
 [46.01248 ]
 [42.761566]
 [49.121914]
 [40.985893]
 [46.239193]
 [42.797382]
 [37.326332]
 [47.081272]
 [42.516567]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 0.] 
cards in discard: [ 1. 25. 29. 15.  3.  0.  3.  3. 15.  1. 25. 29. 29. 29. 29.  0. 29. 29.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25 25] -> size -> 26 
action values: 0 
buys: 1 
player value: 6 
card supply: [26. 26. 30. 28. 30.  8. 10.  9.  9.  6.  8.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  8. 11.  8.  3.] 
adversary cards in discard: [16.  8.  0.  1.  3.  0.] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 45.390296936035156



buy possibilites: [-1] 
expected returns: [[55.95417]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 0.] 
cards in discard: [ 1. 25. 29. 15.  3.  0.  3.  3. 15.  1. 25. 29. 29. 29. 29.  0. 29. 29.
 25.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25 25 25] -> size -> 27 
action values: 0 
buys: 0 
player value: 1 
card supply: [26. 26. 30. 28. 30.  8. 10.  9.  9.  6.  7.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  8. 11.  8.  3.] 
adversary cards in discard: [16.  8.  0.  1.  3.  0.] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5.   0.   0.   0.   0.   0.  40.   0.   0.   0.   0.   0.   0.   0.
 62.5  0. ] 
sum of rewards: 97.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 49.12191390991211






Player: 1 
cards in hand: [ 0.  8. 11.  8.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8. 11.  8.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  8. 11.  8.  3.] 
cards in discard: [16.  8.  0.  1.  3.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16] -> size -> 12 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 26. 30. 28. 30.  8. 10.  9.  9.  6.  7.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [29. 25. 29.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25 25 25] -> size -> 27 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  8. 11.  8.  3.] 
cards in discard: [16.  8.  0.  1.  3.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16] -> size -> 12 
action values: 0 
buys: 1 
player value: 1 
card supply: [26. 26. 30. 28. 30.  8. 10.  9.  9.  6.  7.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [29. 25. 29.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25 25 25] -> size -> 27 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  8. 11.  8.  3.] 
cards in discard: [16.  8.  0.  1.  3.  0.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0] -> size -> 13 
action values: 0 
buys: 0 
player value: 1 
card supply: [25. 26. 30. 28. 30.  8. 10.  9.  9.  6.  7.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [29. 25. 29.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25 25 25] -> size -> 27 
adversary victory points: 3
player victory points: 3 





         -------------------- Turn: 19 -------------------- 
Player: 0 
cards in hand: [29. 25. 29.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 25. 29.] 
expected returns: [[20.226612]
 [29.04587 ]
 [28.027195]
 [29.04587 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 25. 29.  0.  0.] 
cards in discard: [] 
cards in deck: 22 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25 25 25] -> size -> 27 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 26. 30. 28. 30.  8. 10.  9.  9.  6.  7.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  3. 11.  8.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0] -> size -> 13 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 55.95417022705078



action possibilites: [-1. 25. 29.] 
expected returns: [[50.464615]
 [58.64682 ]
 [59.57572 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  0.  0. 29.] 
cards in discard: [29.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25 25 25] -> size -> 27 
action values: 1 
buys: 0 
player value: 1 
card supply: [25. 26. 30. 28. 30.  8. 10.  9.  9.  6.  7.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  3. 11.  8.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0] -> size -> 13 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 22.86602210998535



action possibilites: [-1. 15.] 
expected returns: [[57.505913]
 [63.16671 ]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0. 15.] 
cards in discard: [29. 25.] 
cards in deck: 20 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15
 25 25 25] -> size -> 27 
action values: 1 
buys: 0 
player value: 2 
card supply: [25. 26. 30. 28. 30.  8. 10.  9.  9.  6.  7.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  3. 11.  8.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0] -> size -> 13 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 55.153778076171875



action possibilites: [-1] 
expected returns: [[26.442295]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0.] 
cards in discard: [29. 25.] 
cards in deck: 20 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25] -> size -> 26 
action values: 0 
buys: 0 
player value: 5 
card supply: [25. 26. 30. 28. 30.  8. 10.  9.  9.  6.  7.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  3. 11.  8.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0] -> size -> 13 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 63.166709899902344





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[25.55517 ]
 [31.989412]
 [19.060545]
 [27.975906]
 [21.580433]
 [13.931764]
 [29.992907]
 [31.521051]
 [28.091198]
 [35.00687 ]
 [25.781673]
 [31.786089]
 [27.999388]
 [21.7873  ]
 [32.665737]
 [27.40125 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [29. 25.] 
cards in deck: 20 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25] -> size -> 26 
action values: 0 
buys: 1 
player value: 6 
card supply: [25. 26. 30. 28. 30.  8. 10.  9.  9.  6.  7.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  3. 11.  8.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0] -> size -> 13 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: take_action - action -1
Learning step: 0
desired expected reward: 26.44229507446289



buy possibilites: [-1] 
expected returns: [[30.687483]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [29. 25. 25.] 
cards in deck: 20 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25] -> size -> 27 
action values: 0 
buys: 0 
player value: 1 
card supply: [25. 26. 30. 28. 30.  8. 10.  9.  9.  6.  6.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  3. 11.  8.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0] -> size -> 13 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5.   0.   0.   0.   0.   0.  60.   0.   0.   0.   0.   0.   0.   0.
 62.5  0. ] 
sum of rewards: 117.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 35.006874084472656






Player: 1 
cards in hand: [ 0.  3. 11.  8.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.  8.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3. 11.  8.  3.] 
cards in discard: [] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0] -> size -> 13 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 26. 30. 28. 30.  8. 10.  9.  9.  6.  6.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 1. 29. 25. 29.  1.] 
adversary cards in discard: [29. 25. 25. 29. 29. 15.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25] -> size -> 27 
adversary victory points: 3
player victory points: 3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 8. 3.] 
cards in discard: [6.] 
cards in deck: 8 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0  6] -> size -> 14 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 26. 30. 28. 30.  8.  9.  9.  9.  6.  6.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 1. 29. 25. 29.  1.] 
adversary cards in discard: [29. 25. 25. 29. 29. 15.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25] -> size -> 27 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 8. 3.] 
cards in discard: [6.] 
cards in deck: 8 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0  6] -> size -> 14 
action values: 0 
buys: 1 
player value: 1 
card supply: [25. 26. 30. 28. 30.  8.  9.  9.  9.  6.  6.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 1. 29. 25. 29.  1.] 
adversary cards in discard: [29. 25. 25. 29. 29. 15.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25] -> size -> 27 
adversary victory points: 3
player victory points: 2 





         -------------------- Turn: 20 -------------------- 
Player: 0 
cards in hand: [ 1. 29. 25. 29.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 25. 29.] 
expected returns: [[ 91.531586]
 [100.00784 ]
 [ 99.10625 ]
 [100.00784 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 29. 25. 29.  1.] 
cards in discard: [29. 25. 25. 29. 29. 15.  0.] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25] -> size -> 27 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 26. 30. 28. 30.  8.  9.  9.  9.  6.  6.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  3.  1.  0. 16.] 
adversary cards in discard: [ 6. 11.  0.  3.  8.  3.] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0  6] -> size -> 14 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 30.687482833862305



action possibilites: [-1. 29. 29.] 
expected returns: [[199.3151 ]
 [208.09233]
 [208.09233]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 29.  1. 29.] 
cards in discard: [29. 25. 25. 29. 29. 15.  0. 25.] 
cards in deck: 14 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25] -> size -> 27 
action values: 1 
buys: 0 
player value: 1 
card supply: [25. 26. 30. 28. 30.  8.  9.  9.  9.  6.  6.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  3.  1.  0. 16.] 
adversary cards in discard: [ 6. 11.  0.  3.  8.  3.] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0  6] -> size -> 14 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 92.77597045898438



action possibilites: [-1. 29.] 
expected returns: [[206.68515]
 [215.47112]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  1. 29.] 
cards in discard: [29. 25. 25. 29. 29. 15.  0. 25. 15.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25] -> size -> 27 
action values: 1 
buys: 0 
player value: 2 
card supply: [25. 26. 30. 28. 30.  8.  9.  9.  9.  6.  6.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  3.  1.  0. 16.] 
adversary cards in discard: [ 6. 11.  0.  3.  8.  3.] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0  6] -> size -> 14 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 204.25469970703125



action possibilites: [-1.] 
expected returns: [[200.09464]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 1.] 
cards in discard: [29. 25. 25. 29. 29. 15.  0. 25. 15.  1.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25] -> size -> 27 
action values: 1 
buys: 0 
player value: 3 
card supply: [25. 26. 30. 28. 30.  8.  9.  9.  9.  6.  6.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  3.  1.  0. 16.] 
adversary cards in discard: [ 6. 11.  0.  3.  8.  3.] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0  6] -> size -> 14 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: discard_n_cards - action 0
Learning step: 0
desired expected reward: 204.75083923339844





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[198.20795]
 [205.70442]
 [189.9246 ]
 [200.91934]
 [192.75894]
 [183.8645 ]
 [203.45052]
 [205.51689]
 [201.93604]
 [209.3817 ]
 [198.23738]
 [205.32455]
 [201.56337]
 [193.18085]
 [206.52145]
 [200.29506]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 1.] 
cards in discard: [29. 25. 25. 29. 29. 15.  0. 25. 15.  1.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25] -> size -> 27 
action values: 0 
buys: 1 
player value: 7 
card supply: [25. 26. 30. 28. 30.  8.  9.  9.  9.  6.  6.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  3.  1.  0. 16.] 
adversary cards in discard: [ 6. 11.  0.  3.  8.  3.] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0  6] -> size -> 14 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 200.09463500976562



buy possibilites: [-1] 
expected returns: [[173.61165]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 1.] 
cards in discard: [29. 25. 25. 29. 29. 15.  0. 25. 15.  1. 25.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25] -> size -> 28 
action values: 0 
buys: 0 
player value: 2 
card supply: [25. 26. 30. 28. 30.  8.  9.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  3.  1.  0. 16.] 
adversary cards in discard: [ 6. 11.  0.  3.  8.  3.] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0  6] -> size -> 14 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5.   0.   0.  30.   0.   0.  60.   0.   0.   0.   0.   0.   0.   0.
 62.5  0. ] 
sum of rewards: 147.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 209.38168334960938






Player: 1 
cards in hand: [ 0.  3.  1.  0. 16.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3.  1.  0. 16.] 
cards in discard: [ 6. 11.  0.  3.  8.  3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0  6] -> size -> 14 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 26. 30. 28. 30.  8.  9.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [0. 0. 0. 3. 3.] 
adversary cards in discard: [29. 25. 25. 29. 29. 15.  0. 25. 15.  1. 25. 29. 29. 29.  1.  1.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25] -> size -> 28 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3.  1.  0. 16.] 
cards in discard: [ 6. 11.  0.  3.  8.  3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0  6] -> size -> 14 
action values: 0 
buys: 1 
player value: 4 
card supply: [25. 26. 30. 28. 30.  8.  9.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [0. 0. 0. 3. 3.] 
adversary cards in discard: [29. 25. 25. 29. 29. 15.  0. 25. 15.  1. 25. 29. 29. 29.  1.  1.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25] -> size -> 28 
adversary victory points: 3
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3.  1.  0. 16.] 
cards in discard: [ 6. 11.  0.  3.  8.  3.  1.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0  6  1] -> size -> 15 
action values: 0 
buys: 0 
player value: 1 
card supply: [25. 25. 30. 28. 30.  8.  9.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [0. 0. 0. 3. 3.] 
adversary cards in discard: [29. 25. 25. 29. 29. 15.  0. 25. 15.  1. 25. 29. 29. 29.  1.  1.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25] -> size -> 28 
adversary victory points: 3
player victory points: 2 





         -------------------- Turn: 21 -------------------- 
Player: 0 
cards in hand: [0. 0. 0. 3. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[36.17357]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 3. 3.] 
cards in discard: [29. 25. 25. 29. 29. 15.  0. 25. 15.  1. 25. 29. 29. 29.  1.  1.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25] -> size -> 28 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 25. 30. 28. 30.  8.  9.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [3. 6. 0. 8. 8.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0  6  1] -> size -> 15 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 173.6116485595703





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[32.179497]
 [37.739265]
 [34.31833 ]
 [22.043703]
 [37.30642 ]
 [34.17877 ]
 [34.184925]
 [33.8489  ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3. 3.] 
cards in discard: [29. 25. 25. 29. 29. 15.  0. 25. 15.  1. 25. 29. 29. 29.  1.  1.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25] -> size -> 28 
action values: 0 
buys: 1 
player value: 3 
card supply: [25. 25. 30. 28. 30.  8.  9.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [3. 6. 0. 8. 8.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0  6  1] -> size -> 15 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 36.17356872558594



buy possibilites: [-1] 
expected returns: [[37.1125]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3. 3.] 
cards in discard: [29. 25. 25. 29. 29. 15.  0. 25. 15.  1. 25. 29. 29. 29.  1.  1.  1.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1] -> size -> 29 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 24. 30. 28. 30.  8.  9.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [3. 6. 0. 8. 8.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0  6  1] -> size -> 15 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0 54  0] 
sum of rewards: 79 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 37.7392578125






Player: 1 
cards in hand: [3. 6. 0. 8. 8.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 6. 0. 8. 8.] 
cards in discard: [] 
cards in deck: 10 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  8  3  0  0 11  8  0  1  3  8 16  0  6  1] -> size -> 15 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 24. 30. 28. 30.  8.  9.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  3. 29. 29. 25.] 
adversary cards in discard: [29. 25. 25. 29. 29. 15.  0. 25. 15.  1. 25. 29. 29. 29.  1.  1.  1.  0.
  0.  0.  3.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1] -> size -> 29 
adversary victory points: 3
player victory points: 2 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0.] 
cards in discard: [] 
cards in deck: 10 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1] -> size -> 12 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 24. 30. 28. 30.  8.  9.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  3. 29. 29. 25.] 
adversary cards in discard: [29. 25. 25. 29. 29. 15.  0. 25. 15.  1. 25. 29. 29. 29.  1.  1.  1.  0.
  0.  0.  3.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1] -> size -> 29 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [] 
cards in deck: 10 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1] -> size -> 12 
action values: 0 
buys: 1 
player value: 1 
card supply: [25. 24. 30. 28. 30.  8.  9.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 0.  3. 29. 29. 25.] 
adversary cards in discard: [29. 25. 25. 29. 29. 15.  0. 25. 15.  1. 25. 29. 29. 29.  1.  1.  1.  0.
  0.  0.  3.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1] -> size -> 29 
adversary victory points: 3
player victory points: 2 





         -------------------- Turn: 22 -------------------- 
Player: 0 
cards in hand: [ 0.  3. 29. 29. 25.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 25.] 
expected returns: [[ 6.003419]
 [13.043438]
 [13.043438]
 [12.183644]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3. 29. 29. 25.] 
cards in discard: [29. 25. 25. 29. 29. 15.  0. 25. 15.  1. 25. 29. 29. 29.  1.  1.  1.  0.
  0.  0.  3.  3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1] -> size -> 29 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 24. 30. 28. 30.  8.  9.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [0. 1. 3. 3. 0.] 
adversary cards in discard: [8. 0.] 
adversary owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1] -> size -> 12 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 37.11249923706055



action possibilites: [-1. 25. 29.] 
expected returns: [[-3.9295912]
 [ 1.5579269]
 [ 2.3504798]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3. 25. 29.] 
cards in discard: [29. 25. 25. 29. 29. 15.  0. 25. 15.  1. 25. 29. 29. 29.  1.  1.  1.  0.
  0.  0.  3.  3. 29.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1] -> size -> 29 
action values: 1 
buys: 0 
player value: 1 
card supply: [25. 24. 30. 28. 30.  8.  9.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [0. 1. 3. 3. 0.] 
adversary cards in discard: [8. 0.] 
adversary owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1] -> size -> 12 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 9.760151863098145



action possibilites: [-1. 25.] 
expected returns: [[2.1303294]
 [7.661688 ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3. 25.] 
cards in discard: [29. 25. 25. 29. 29. 15.  0. 25. 15.  1. 25. 29. 29. 29.  1.  1.  1.  0.
  0.  0.  3.  3. 29. 29.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1] -> size -> 29 
action values: 1 
buys: 0 
player value: 2 
card supply: [25. 24. 30. 28. 30.  8.  9.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [0. 1. 3. 3. 0.] 
adversary cards in discard: [8. 0.] 
adversary owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1] -> size -> 12 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -0.5842785835266113



action possibilites: [-1] 
expected returns: [[39.255302]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3. 29. 29.] 
cards in discard: [] 
cards in deck: 22 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1] -> size -> 29 
action values: 0 
buys: 0 
player value: 2 
card supply: [25. 24. 30. 28. 30.  8.  8.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [0. 1. 3. 3. 0.] 
adversary cards in discard: [8. 0. 6.] 
adversary owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6] -> size -> 13 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 7.661694526672363





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[34.746098]
 [41.80638 ]
 [37.377075]
 [21.429949]
 [41.446632]
 [37.855522]
 [37.62885 ]
 [36.698883]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3. 29. 29.] 
cards in discard: [] 
cards in deck: 22 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1] -> size -> 29 
action values: 0 
buys: 1 
player value: 3 
card supply: [25. 24. 30. 28. 30.  8.  8.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [0. 1. 3. 3. 0.] 
adversary cards in discard: [8. 0. 6.] 
adversary owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6] -> size -> 13 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: take_action - action -1
Learning step: 0
desired expected reward: 39.25530242919922



buy possibilites: [-1] 
expected returns: [[22.684145]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3. 29. 29.] 
cards in discard: [1.] 
cards in deck: 22 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1] -> size -> 30 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 23. 30. 28. 30.  8.  8.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [0. 1. 3. 3. 0.] 
adversary cards in discard: [8. 0. 6.] 
adversary owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6] -> size -> 13 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 60  0  0  0  0  0  0  0 54  0] 
sum of rewards: 139 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 41.80638122558594






Player: 1 
cards in hand: [0. 1. 3. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 1. 3. 3. 0.] 
cards in discard: [8. 0. 6.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6] -> size -> 13 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 23. 30. 28. 30.  8.  8.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 1.  0. 29.  1. 29.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1] -> size -> 30 
adversary victory points: 3
player victory points: 1 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 3. 3. 0.] 
cards in discard: [8. 0. 6.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6] -> size -> 13 
action values: 0 
buys: 1 
player value: 4 
card supply: [25. 23. 30. 28. 30.  8.  8.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 1.  0. 29.  1. 29.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1] -> size -> 30 
adversary victory points: 3
player victory points: 1 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 3. 3. 0.] 
cards in discard: [8. 0. 6. 3.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6  3] -> size -> 14 
action values: 0 
buys: 0 
player value: 2 
card supply: [25. 23. 30. 27. 30.  8.  8.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 1.  0. 29.  1. 29.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1] -> size -> 30 
adversary victory points: 3
player victory points: 2 





         -------------------- Turn: 23 -------------------- 
Player: 0 
cards in hand: [ 1.  0. 29.  1. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[151.23221]
 [159.87851]
 [159.87851]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  0. 29.  1. 29.] 
cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.] 
cards in deck: 17 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1] -> size -> 30 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 23. 30. 27. 30.  8.  8.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 8. 16.  1.  0. 11.] 
adversary cards in discard: [8. 0. 6. 3. 0. 1. 3. 3. 0.] 
adversary owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6  3] -> size -> 14 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 22.684144973754883



action possibilites: [-1. 29. 25.] 
expected returns: [[133.96234]
 [142.56339]
 [141.67294]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  1. 29. 25.] 
cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.  1.] 
cards in deck: 16 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1] -> size -> 30 
action values: 1 
buys: 0 
player value: 1 
card supply: [25. 23. 30. 27. 30.  8.  8.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 8. 16.  1.  0. 11.] 
adversary cards in discard: [8. 0. 6. 3. 0. 1. 3. 3. 0.] 
adversary owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6  3] -> size -> 14 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 151.30601501464844



action possibilites: [-1. 25. 29.] 
expected returns: [[85.392044]
 [92.99233 ]
 [93.99074 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 25. 29.] 
cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.  1.  1.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1] -> size -> 30 
action values: 1 
buys: 0 
player value: 2 
card supply: [25. 23. 30. 27. 30.  8.  8.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 8. 16.  1.  0. 11.] 
adversary cards in discard: [8. 0. 6. 3. 0. 1. 3. 3. 0.] 
adversary owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6  3] -> size -> 14 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 138.47518920898438



action possibilites: [-1. 25.] 
expected returns: [[64.40245]
 [71.82583]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 25.] 
cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.  1.  1. 25.] 
cards in deck: 14 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1] -> size -> 30 
action values: 1 
buys: 0 
player value: 3 
card supply: [25. 23. 30. 27. 30.  8.  8.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 8. 16.  1.  0. 11.] 
adversary cards in discard: [8. 0. 6. 3. 0. 1. 3. 3. 0.] 
adversary owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6  3] -> size -> 14 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 89.76158905029297



action possibilites: [-1] 
expected returns: [[2.3406389]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3. 15.] 
cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.  1.  1. 25.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1] -> size -> 30 
action values: 0 
buys: 0 
player value: 3 
card supply: [25. 23. 30. 27. 30.  8.  7.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 8. 16.  1.  0. 11.] 
adversary cards in discard: [8. 0. 6. 3. 0. 1. 3. 3. 0. 6.] 
adversary owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6  3  6] -> size -> 15 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 105 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 71.82582092285156





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[ -0.01558924]
 [  6.263999  ]
 [  2.3841588 ]
 [-11.166198  ]
 [  4.3338814 ]
 [  5.738516  ]
 [  2.2469027 ]
 [  0.27128148]
 [  2.242019  ]
 [  6.9146233 ]
 [  1.8393638 ]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3. 15.] 
cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.  1.  1. 25.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1] -> size -> 30 
action values: 0 
buys: 1 
player value: 4 
card supply: [25. 23. 30. 27. 30.  8.  7.  9.  9.  6.  5.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 8. 16.  1.  0. 11.] 
adversary cards in discard: [8. 0. 6. 3. 0. 1. 3. 3. 0. 6.] 
adversary owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6  3  6] -> size -> 15 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 105 

action type: take_action - action -1
Learning step: 0
desired expected reward: 2.3406388759613037



buy possibilites: [-1] 
expected returns: [[32.860283]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3. 15.] 
cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.  1.  1. 25. 15.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15] -> size -> 31 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 23. 30. 27. 30.  8.  7.  9.  9.  6.  5.  0. 10. 10. 10. 10.  7.] 
adversary cards in hand: [ 8. 16.  1.  0. 11.] 
adversary cards in discard: [8. 0. 6. 3. 0. 1. 3. 3. 0. 6.] 
adversary owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6  3  6] -> size -> 15 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[ -5   0   0  30   0   0  80   0   0   0   0   0   0   0 128   0] 
sum of rewards: 233 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 6.9146270751953125






Player: 1 
cards in hand: [ 8. 16.  1.  0. 11.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8. 16. 11.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 8. 16.  1.  0. 11.] 
cards in discard: [8. 0. 6. 3. 0. 1. 3. 3. 0. 6.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6  3  6] -> size -> 15 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 23. 30. 27. 30.  8.  7.  9.  9.  6.  5.  0. 10. 10. 10. 10.  7.] 
adversary cards in hand: [25.  1.  3.  0. 15.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.  1.  1. 25. 15. 29. 29. 29. 25.  0.  3.
 15.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15] -> size -> 31 
adversary victory points: 3
player victory points: 1 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 10.0 : ['Village' '10' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 8. 16.  1.  0. 11.] 
cards in discard: [8. 0. 6. 3. 0. 1. 3. 3. 0. 6.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6  3  6] -> size -> 15 
action values: 0 
buys: 1 
player value: 3 
card supply: [25. 23. 30. 27. 30.  8.  7.  9.  9.  6.  5.  0. 10. 10. 10. 10.  7.] 
adversary cards in hand: [25.  1.  3.  0. 15.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.  1.  1. 25. 15. 29. 29. 29. 25.  0.  3.
 15.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15] -> size -> 31 
adversary victory points: 3
player victory points: 1 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 8. 16.  1.  0. 11.] 
cards in discard: [ 8.  0.  6.  3.  0.  1.  3.  3.  0.  6. 10.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6  3  6 10] -> size -> 16 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 23. 30. 27. 30.  8.  7.  9.  9.  6.  5.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [25.  1.  3.  0. 15.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.  1.  1. 25. 15. 29. 29. 29. 25.  0.  3.
 15.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15] -> size -> 31 
adversary victory points: 3
player victory points: 1 





         -------------------- Turn: 24 -------------------- 
Player: 0 
cards in hand: [25.  1.  3.  0. 15.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 15.] 
expected returns: [[31.354914]
 [38.13938 ]
 [36.05995 ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  1.  3.  0. 15.] 
cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.  1.  1. 25. 15. 29. 29. 29. 25.  0.  3.
 15.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15] -> size -> 31 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 23. 30. 27. 30.  8.  7.  9.  9.  6.  5.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [0. 1. 3. 8. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6  3  6 10] -> size -> 16 
adversary victory points: 1
player victory points: 3 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: 32.86028289794922



action possibilites: [-1] 
expected returns: [[26.578579]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  3.  0. 15.  0. 29.] 
cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.  1.  1. 25. 15. 29. 29. 29. 25.  0.  3.
 15.] 
cards in deck: 5 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15] -> size -> 31 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 23. 30. 27. 30.  8.  6.  9.  9.  6.  5.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [0. 1. 3. 8. 0.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6  3  6 10  6] -> size -> 17 
adversary victory points: 1
player victory points: 3 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 38.13936996459961





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[23.20049 ]
 [28.835184]
 [25.369217]
 [13.125518]
 [27.129568]
 [28.281092]
 [25.012585]
 [23.515503]
 [25.104446]
 [29.403933]
 [24.952082]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  3.  0. 15.  0. 29.] 
cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.  1.  1. 25. 15. 29. 29. 29. 25.  0.  3.
 15.] 
cards in deck: 5 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15] -> size -> 31 
action values: 0 
buys: 1 
player value: 4 
card supply: [25. 23. 30. 27. 30.  8.  6.  9.  9.  6.  5.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [0. 1. 3. 8. 0.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6  3  6 10  6] -> size -> 17 
adversary victory points: 1
player victory points: 3 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action -1
Learning step: 0
desired expected reward: 26.57857894897461



buy possibilites: [-1] 
expected returns: [[16.223457]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  3.  0. 15.  0. 29.] 
cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.  1.  1. 25. 15. 29. 29. 29. 25.  0.  3.
 15. 15.] 
cards in deck: 5 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15] -> size -> 32 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 23. 30. 27. 30.  8.  6.  9.  9.  6.  5.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [0. 1. 3. 8. 0.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6  3  6 10  6] -> size -> 17 
adversary victory points: 1
player victory points: 3 

Reward from previous game state: 
[ -5   0   0  60   0   0  20   0   0   0   0   0   0   0 128   0] 
sum of rewards: 203 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 29.40391731262207






Player: 1 
cards in hand: [0. 1. 3. 8. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 1. 3. 8. 0.] 
cards in discard: [6.] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  0  0 11  8  0  1  3  8 16  0  1  6  3  6 10  6] -> size -> 17 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 23. 30. 27. 30.  8.  6.  9.  9.  6.  5.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [29. 29.  1. 25.  0.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.  1.  1. 25. 15. 29. 29. 29. 25.  0.  3.
 15. 15. 25.  1.  3.  0. 15.  0. 29.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15] -> size -> 32 
adversary victory points: 3
player victory points: 0 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0.] 
cards in discard: [6.] 
cards in deck: 11 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6] -> size -> 14 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 23. 30. 27. 30.  8.  6.  9.  9.  6.  5.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [29. 29.  1. 25.  0.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.  1.  1. 25. 15. 29. 29. 29. 25.  0.  3.
 15. 15. 25.  1.  3.  0. 15.  0. 29.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15] -> size -> 32 
adversary victory points: 3
player victory points: -1 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [6.] 
cards in deck: 11 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6] -> size -> 14 
action values: 0 
buys: 1 
player value: 1 
card supply: [25. 23. 30. 27. 30.  8.  6.  9.  9.  6.  5.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [29. 29.  1. 25.  0.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.  1.  1. 25. 15. 29. 29. 29. 25.  0.  3.
 15. 15. 25.  1.  3.  0. 15.  0. 29.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15] -> size -> 32 
adversary victory points: 3
player victory points: -1 





         -------------------- Turn: 25 -------------------- 
Player: 0 
cards in hand: [29. 29.  1. 25.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 25.] 
expected returns: [[27.629908]
 [33.90976 ]
 [33.90976 ]
 [33.119286]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29.  1. 25.  0.] 
cards in discard: [ 1. 29. 29. 25.  0.  3. 29. 29.  1.  1. 25. 15. 29. 29. 29. 25.  0.  3.
 15. 15. 25.  1.  3.  0. 15.  0. 29.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15] -> size -> 32 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 23. 30. 27. 30.  8.  6.  9.  9.  6.  5.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [ 1.  0.  3.  3. 10.] 
adversary cards in discard: [6. 8. 0.] 
adversary owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6] -> size -> 14 
adversary victory points: -1
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 120   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 115 

action type: buy - action -1
Learning step: 0
desired expected reward: 16.22345733642578



action possibilites: [-1. 25. 29.] 
expected returns: [[14.402822]
 [21.687101]
 [22.67284 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 25.  0. 29.] 
cards in discard: [29.] 
cards in deck: 26 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15] -> size -> 32 
action values: 1 
buys: 0 
player value: 1 
card supply: [25. 23. 30. 27. 30.  8.  6.  9.  9.  6.  5.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [ 1.  0.  3.  3. 10.] 
adversary cards in discard: [6. 8. 0.] 
adversary owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6] -> size -> 14 
adversary victory points: -1
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 120   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 135 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 30.97648811340332



action possibilites: [-1. 25. 29.] 
expected returns: [[ 9.3081255]
 [16.58838  ]
 [17.574802 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  0. 29.] 
cards in discard: [29.  1.] 
cards in deck: 25 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15] -> size -> 32 
action values: 1 
buys: 0 
player value: 2 
card supply: [25. 23. 30. 27. 30.  8.  6.  9.  9.  6.  5.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [ 1.  0.  3.  3. 10.] 
adversary cards in discard: [6. 8. 0.] 
adversary owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6] -> size -> 14 
adversary victory points: -1
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 120   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 155 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 18.801883697509766



action possibilites: [-1. 25.] 
expected returns: [[11.416701]
 [18.69663 ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  0.] 
cards in discard: [29.  1.  1.] 
cards in deck: 24 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15] -> size -> 32 
action values: 1 
buys: 0 
player value: 3 
card supply: [25. 23. 30. 27. 30.  8.  6.  9.  9.  6.  5.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [ 1.  0.  3.  3. 10.] 
adversary cards in discard: [6. 8. 0.] 
adversary owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6] -> size -> 14 
adversary victory points: -1
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 120   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 175 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 13.704493522644043



action possibilites: [-1] 
expected returns: [[57.26468]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0.] 
cards in discard: [29.  1.  1.] 
cards in deck: 22 
card top of deck: [] 
played cards: [29. 29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15] -> size -> 32 
action values: 0 
buys: 0 
player value: 3 
card supply: [25. 23. 30. 27. 30.  8.  5.  9.  9.  6.  5.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [ 1.  0.  3.  3. 10.] 
adversary cards in discard: [6. 8. 0. 6.] 
adversary owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6  6] -> size -> 15 
adversary victory points: -1
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 120   0   0  80   0   0   0   0   0   0   0   0   0] 
sum of rewards: 195 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 18.696638107299805





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[51.450493]
 [58.07553 ]
 [44.63086 ]
 [53.985764]
 [47.235043]
 [39.228233]
 [56.06179 ]
 [57.66006 ]
 [54.0738  ]
 [61.232254]
 [51.69726 ]
 [57.850605]
 [53.977417]
 [47.548183]
 [58.779785]
 [53.35121 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0.] 
cards in discard: [29.  1.  1.] 
cards in deck: 22 
card top of deck: [] 
played cards: [29. 29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15] -> size -> 32 
action values: 0 
buys: 1 
player value: 6 
card supply: [25. 23. 30. 27. 30.  8.  5.  9.  9.  6.  5.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [ 1.  0.  3.  3. 10.] 
adversary cards in discard: [6. 8. 0. 6.] 
adversary owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6  6] -> size -> 15 
adversary victory points: -1
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 120   0   0  80   0   0   0   0   0   0   0   0   0] 
sum of rewards: 195 

action type: take_action - action -1
Learning step: 0
desired expected reward: 57.264678955078125



buy possibilites: [-1] 
expected returns: [[57.321945]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0.] 
cards in discard: [29.  1.  1. 25.] 
cards in deck: 22 
card top of deck: [] 
played cards: [29. 29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15 25] -> size -> 33 
action values: 0 
buys: 0 
player value: 1 
card supply: [25. 23. 30. 27. 30.  8.  5.  9.  9.  6.  4.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [ 1.  0.  3.  3. 10.] 
adversary cards in discard: [6. 8. 0. 6.] 
adversary owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6  6] -> size -> 15 
adversary victory points: -1
player victory points: 3 

Reward from previous game state: 
[ -5.    0.    0.  120.    0.    0.   80.    0.    0.    0.    0.    0.
   0.    0.   62.5   0. ] 
sum of rewards: 257.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 61.23225402832031






Player: 1 
cards in hand: [ 1.  0.  3.  3. 10.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 10.] 
Chosen action: 10 : ['Village' '10' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  0.  3.  3. 10.] 
cards in discard: [6. 8. 0. 6.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6  6] -> size -> 15 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 23. 30. 27. 30.  8.  5.  9.  9.  6.  4.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [ 0.  1. 25. 15. 15.] 
adversary cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15 25] -> size -> 33 
adversary victory points: 3
player victory points: -2 


action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 0. 3. 3. 0.] 
cards in discard: [6. 8. 0. 6.] 
cards in deck: 5 
card top of deck: [] 
played cards: [10.] 
owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6  6] -> size -> 15 
action values: 2 
buys: 0 
player value: 0 
card supply: [25. 23. 30. 27. 30.  8.  5.  9.  9.  6.  4.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [ 0.  1. 25. 15. 15.] 
adversary cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15 25] -> size -> 33 
adversary victory points: 3
player victory points: -2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: 10.0 : ['Village' '10' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 0. 3. 3. 0.] 
cards in discard: [6. 8. 0. 6.] 
cards in deck: 5 
card top of deck: [] 
played cards: [10.] 
owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6  6] -> size -> 15 
action values: 0 
buys: 1 
player value: 4 
card supply: [25. 23. 30. 27. 30.  8.  5.  9.  9.  6.  4.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [ 0.  1. 25. 15. 15.] 
adversary cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15 25] -> size -> 33 
adversary victory points: 3
player victory points: -2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 0. 3. 3. 0.] 
cards in discard: [ 6.  8.  0.  6. 10.] 
cards in deck: 5 
card top of deck: [] 
played cards: [10.] 
owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6  6 10] -> size -> 16 
action values: 0 
buys: 0 
player value: 1 
card supply: [25. 23. 30. 27. 30.  8.  5.  9.  9.  6.  4.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 0.  1. 25. 15. 15.] 
adversary cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15 25] -> size -> 33 
adversary victory points: 3
player victory points: -2 





         -------------------- Turn: 26 -------------------- 
Player: 0 
cards in hand: [ 0.  1. 25. 15. 15.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 15. 15.] 
expected returns: [[87.149956]
 [95.00924 ]
 [92.55914 ]
 [92.55914 ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  1. 25. 15. 15.] 
cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.] 
cards in deck: 17 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15 25] -> size -> 33 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 23. 30. 27. 30.  8.  5.  9.  9.  6.  4.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 6. 16.  6.  8. 11.] 
adversary cards in discard: [ 6.  8.  0.  6. 10. 10.  1.  0.  3.  3.  0.] 
adversary owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6  6 10] -> size -> 16 
adversary victory points: -2
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 150   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 145 

action type: buy - action -1
Learning step: 0
desired expected reward: 57.32194519042969



action possibilites: [-1] 
expected returns: [[22.872156]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  1. 15. 15.  3.  3.] 
cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.] 
cards in deck: 15 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15 25] -> size -> 33 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 23. 30. 27. 30.  8.  4.  9.  9.  6.  4.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 6. 16.  6.  8. 11.] 
adversary cards in discard: [ 6.  8.  0.  6. 10. 10.  1.  0.  3.  3.  0.  6.] 
adversary owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6  6 10  6] -> size -> 17 
adversary victory points: -2
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 150   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 165 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 95.00923156738281





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[15.39169 ]
 [21.286901]
 [17.599962]
 [ 4.634268]
 [20.886173]
 [17.58704 ]
 [17.55957 ]
 [17.196175]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  1. 15. 15.  3.  3.] 
cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.] 
cards in deck: 15 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15 25] -> size -> 33 
action values: 0 
buys: 1 
player value: 3 
card supply: [25. 23. 30. 27. 30.  8.  4.  9.  9.  6.  4.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 6. 16.  6.  8. 11.] 
adversary cards in discard: [ 6.  8.  0.  6. 10. 10.  1.  0.  3.  3.  0.  6.] 
adversary owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6  6 10  6] -> size -> 17 
adversary victory points: -2
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 150   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 165 

action type: take_action - action -1
Learning step: 0
desired expected reward: 22.872156143188477



buy possibilites: [-1] 
expected returns: [[7.403303]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  1. 15. 15.  3.  3.] 
cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1.] 
cards in deck: 15 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15 25  1] -> size -> 34 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 22. 30. 27. 30.  8.  4.  9.  9.  6.  4.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 6. 16.  6.  8. 11.] 
adversary cards in discard: [ 6.  8.  0.  6. 10. 10.  1.  0.  3.  3.  0.  6.] 
adversary owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6  6 10  6] -> size -> 17 
adversary victory points: -2
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 150   0   0  20   0   0   0   0   0   0   0  54   0] 
sum of rewards: 219 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 21.28691291809082






Player: 1 
cards in hand: [ 6. 16.  6.  8. 11.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16.  8. 11.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 6. 16.  6.  8. 11.] 
cards in discard: [ 6.  8.  0.  6. 10. 10.  1.  0.  3.  3.  0.  6.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0 11  8  0  3  8 16  0  1  6  3  6 10  6  6 10  6] -> size -> 17 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 22. 30. 27. 30.  8.  4.  9.  9.  6.  4.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [29. 25. 15. 29.  1.] 
adversary cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1. 25.  0.  1. 15. 15.  3.
  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15 25  1] -> size -> 34 
adversary victory points: 3
player victory points: -3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 6.  6. 11.] 
cards in discard: [ 6.  8.  0.  6. 10. 10.  1.  0.  3.  3.  0.  6.] 
cards in deck: 0 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6] -> size -> 16 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 22. 30. 27. 30.  8.  4.  9.  9.  6.  4.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [29. 25. 15. 29.  1.] 
adversary cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1. 25.  0.  1. 15. 15.  3.
  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15 25  1] -> size -> 34 
adversary victory points: 3
player victory points: -3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 6.  6. 11.] 
cards in discard: [ 6.  8.  0.  6. 10. 10.  1.  0.  3.  3.  0.  6.] 
cards in deck: 0 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6] -> size -> 16 
action values: 0 
buys: 1 
player value: 0 
card supply: [25. 22. 30. 27. 30.  8.  4.  9.  9.  6.  4.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [29. 25. 15. 29.  1.] 
adversary cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1. 25.  0.  1. 15. 15.  3.
  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15 25  1] -> size -> 34 
adversary victory points: 3
player victory points: -3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 6.  6. 11.] 
cards in discard: [ 6.  8.  0.  6. 10. 10.  1.  0.  3.  3.  0.  6.  0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0] -> size -> 17 
action values: 0 
buys: 0 
player value: 0 
card supply: [24. 22. 30. 27. 30.  8.  4.  9.  9.  6.  4.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [29. 25. 15. 29.  1.] 
adversary cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1. 25.  0.  1. 15. 15.  3.
  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15 25  1] -> size -> 34 
adversary victory points: 3
player victory points: -3 





         -------------------- Turn: 27 -------------------- 
Player: 0 
cards in hand: [29. 25. 15. 29.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 25. 15. 29.] 
expected returns: [[-7.493064 ]
 [-1.379956 ]
 [-2.1810017]
 [-3.7765605]
 [-1.379956 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 25. 15. 29.  1.] 
cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1. 25.  0.  1. 15. 15.  3.
  3.] 
cards in deck: 10 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15 25  1] -> size -> 34 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 22. 30. 27. 30.  8.  4.  9.  9.  6.  4.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [10.  6. 11.  0. 10.] 
adversary cards in discard: [] 
adversary owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0] -> size -> 17 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 175 

action type: buy - action -1
Learning step: 0
desired expected reward: 7.403303146362305



action possibilites: [-1. 25. 15. 29. 15.] 
expected returns: [[-2.8031888]
 [ 3.142297 ]
 [ 1.3377931]
 [ 3.9734757]
 [ 1.3377931]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 15. 29. 15.] 
cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1. 25.  0.  1. 15. 15.  3.
  3.  1.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15 25  1] -> size -> 34 
action values: 1 
buys: 0 
player value: 1 
card supply: [24. 22. 30. 27. 30.  8.  4.  9.  9.  6.  4.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [10.  6. 11.  0. 10.] 
adversary cards in discard: [] 
adversary owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0] -> size -> 17 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 195 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -4.24240779876709



action possibilites: [-1. 15. 15. 29.] 
expected returns: [[-2.549736 ]
 [ 1.2112238]
 [ 1.2112238]
 [ 3.6240904]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 15. 29.] 
cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1. 25.  0.  1. 15. 15.  3.
  3.  1. 25.] 
cards in deck: 8 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15 25  1] -> size -> 34 
action values: 1 
buys: 0 
player value: 2 
card supply: [24. 22. 30. 27. 30.  8.  4.  9.  9.  6.  4.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [10.  6. 11.  0. 10.] 
adversary cards in discard: [] 
adversary owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0] -> size -> 17 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 215 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 0.8129491806030273



action possibilites: [-1. 15.] 
expected returns: [[-2.7010698]
 [ 1.4025996]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0.] 
cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1. 25.  0.  1. 15. 15.  3.
  3.  1. 25. 15.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25
 25 25 25 25  1  1 15 15 25  1] -> size -> 34 
action values: 1 
buys: 0 
player value: 3 
card supply: [24. 22. 30. 27. 30.  8.  4.  9.  9.  6.  4.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [10.  6. 11.  0. 10.] 
adversary cards in discard: [] 
adversary owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0] -> size -> 17 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 235 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 0.7379248142242432



action possibilites: [-1] 
expected returns: [[12.629655]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [] 
cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1. 25.  0.  1. 15. 15.  3.
  3.  1. 25. 15.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 29. 29. 15.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1] -> size -> 33 
action values: 0 
buys: 0 
player value: 6 
card supply: [24. 22. 30. 27. 30.  8.  4.  9.  9.  6.  4.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [10.  6. 11.  0. 10.] 
adversary cards in discard: [] 
adversary owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0] -> size -> 17 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0  80   0   0   0   0   0   0   0   0   0] 
sum of rewards: 255 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 1.4025919437408447





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[10.118786 ]
 [15.206733 ]
 [ 5.0518446]
 [11.949602 ]
 [ 7.0225363]
 [ 0.8881781]
 [13.6506605]
 [14.684932 ]
 [11.662533 ]
 [17.516071 ]
 [10.370678 ]
 [14.960511 ]
 [11.795317 ]
 [ 7.111353 ]
 [15.7250185]
 [11.817773 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1. 25.  0.  1. 15. 15.  3.
  3.  1. 25. 15.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 29. 29. 15.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1] -> size -> 33 
action values: 0 
buys: 1 
player value: 6 
card supply: [24. 22. 30. 27. 30.  8.  4.  9.  9.  6.  4.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [10.  6. 11.  0. 10.] 
adversary cards in discard: [] 
adversary owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0] -> size -> 17 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0  80   0   0   0   0   0   0   0   0   0] 
sum of rewards: 255 

action type: take_action - action -1
Learning step: 0
desired expected reward: 12.629654884338379



buy possibilites: [-1] 
expected returns: [[-6.0957546]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1. 25.  0.  1. 15. 15.  3.
  3.  1. 25. 15. 25.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 29. 29. 15.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25] -> size -> 34 
action values: 0 
buys: 0 
player value: 1 
card supply: [24. 22. 30. 27. 30.  8.  4.  9.  9.  6.  3.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [10.  6. 11.  0. 10.] 
adversary cards in discard: [] 
adversary owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0] -> size -> 17 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5.    0.    0.  180.    0.    0.   80.    0.    0.    0.    0.    0.
   0.    0.   62.5   0. ] 
sum of rewards: 317.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 17.51606559753418






Player: 1 
cards in hand: [10.  6. 11.  0. 10.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 10. 11. 10.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [10.  6. 11.  0. 10.] 
cards in discard: [] 
cards in deck: 12 
card top of deck: [] 
played cards: [] 
owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0] -> size -> 17 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 22. 30. 27. 30.  8.  4.  9.  9.  6.  3.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [29. 25. 29. 25. 29.] 
adversary cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1. 25.  0.  1. 15. 15.  3.
  3.  1. 25. 15. 25. 29. 29. 29. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25] -> size -> 34 
adversary victory points: 3
player victory points: -3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [10.  6.  0. 10.] 
cards in discard: [1.] 
cards in deck: 12 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0  1] -> size -> 18 
action values: 0 
buys: 0 
player value: 0 
card supply: [24. 21. 30. 27. 30.  8.  4.  9.  9.  6.  3.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [29. 25. 29. 25. 29.] 
adversary cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1. 25.  0.  1. 15. 15.  3.
  3.  1. 25. 15. 25. 29. 29. 29. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25] -> size -> 34 
adversary victory points: 3
player victory points: -3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [10.  6.  0. 10.] 
cards in discard: [1.] 
cards in deck: 12 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0  1] -> size -> 18 
action values: 0 
buys: 1 
player value: 1 
card supply: [24. 21. 30. 27. 30.  8.  4.  9.  9.  6.  3.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [29. 25. 29. 25. 29.] 
adversary cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1. 25.  0.  1. 15. 15.  3.
  3.  1. 25. 15. 25. 29. 29. 29. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25] -> size -> 34 
adversary victory points: 3
player victory points: -3 





         -------------------- Turn: 28 -------------------- 
Player: 0 
cards in hand: [29. 25. 29. 25. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 25. 29. 25. 29.] 
expected returns: [[-3.9977422]
 [ 3.2162626]
 [ 2.275081 ]
 [ 3.2162626]
 [ 2.275081 ]
 [ 3.2162626]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 25. 29. 25. 29.] 
cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1. 25.  0.  1. 15. 15.  3.
  3.  1. 25. 15. 25. 29. 29. 29. 15.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25] -> size -> 34 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 21. 30. 27. 30.  8.  4.  9.  9.  6.  3.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [8. 8. 0. 1. 6.] 
adversary cards in discard: [ 1. 11. 10.  6.  0. 10.] 
adversary owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0  1] -> size -> 18 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 175 

action type: buy - action -1
Learning step: 0
desired expected reward: -6.095754623413086



action possibilites: [-1. 25. 29. 25. 29.] 
expected returns: [[30.566545]
 [36.804623]
 [37.673534]
 [36.804623]
 [37.673534]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 29. 25. 29.] 
cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1. 25.  0.  1. 15. 15.  3.
  3.  1. 25. 15. 25. 29. 29. 29. 15.  1.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25] -> size -> 34 
action values: 1 
buys: 0 
player value: 1 
card supply: [24. 21. 30. 27. 30.  8.  4.  9.  9.  6.  3.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [8. 8. 0. 1. 6.] 
adversary cards in discard: [ 1. 11. 10.  6.  0. 10.] 
adversary owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0  1] -> size -> 18 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 195 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -0.20566749572753906



action possibilites: [-1. 25. 25. 29.] 
expected returns: [[12.462939]
 [19.035498]
 [19.035498]
 [19.987112]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 25. 29.] 
cards in discard: [29.  1.  1. 25. 29. 29. 29. 25.  0.  0.  0.  1. 25.  0.  1. 15. 15.  3.
  3.  1. 25. 15. 25. 29. 29. 29. 15.  1.  3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25] -> size -> 34 
action values: 1 
buys: 0 
player value: 2 
card supply: [24. 21. 30. 27. 30.  8.  4.  9.  9.  6.  3.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [8. 8. 0. 1. 6.] 
adversary cards in discard: [ 1. 11. 10.  6.  0. 10.] 
adversary owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0  1] -> size -> 18 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 215 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 34.358924865722656



action possibilites: [-1. 25. 29.] 
expected returns: [[-9.002807 ]
 [-3.1590195]
 [-2.305599 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 29.] 
cards in discard: [25.] 
cards in deck: 28 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25] -> size -> 34 
action values: 1 
buys: 0 
player value: 3 
card supply: [24. 21. 30. 27. 30.  8.  4.  9.  9.  6.  3.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [8. 8. 0. 1. 6.] 
adversary cards in discard: [ 1. 11. 10.  6.  0. 10.] 
adversary owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0  1] -> size -> 18 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 235 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 16.471139907836914



action possibilites: [-1. 29.] 
expected returns: [[-0.4494977]
 [ 6.2422857]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.] 
cards in discard: [25. 25.] 
cards in deck: 27 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25] -> size -> 34 
action values: 1 
buys: 0 
player value: 4 
card supply: [24. 21. 30. 27. 30.  8.  4.  9.  9.  6.  3.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [8. 8. 0. 1. 6.] 
adversary cards in discard: [ 1. 11. 10.  6.  0. 10.] 
adversary owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0  1] -> size -> 18 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0  80   0   0   0   0   0   0   0   0   0] 
sum of rewards: 255 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -5.428628921508789



action possibilites: [-1.] 
expected returns: [[-5.213027]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [] 
cards in discard: [25. 25.  1.] 
cards in deck: 26 
card top of deck: [] 
played cards: [29. 29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25] -> size -> 34 
action values: 1 
buys: 0 
player value: 5 
card supply: [24. 21. 30. 27. 30.  8.  4.  9.  9.  6.  3.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [8. 8. 0. 1. 6.] 
adversary cards in discard: [ 1. 11. 10.  6.  0. 10.] 
adversary owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0  1] -> size -> 18 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0 100   0   0   0   0   0   0   0   0   0] 
sum of rewards: 275 

action type: discard_n_cards - action 0
Learning step: 0
desired expected reward: -2.152949571609497





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ -6.8447437 ]
 [ -0.940686  ]
 [ -4.6779194 ]
 [-10.562742  ]
 [-17.862917  ]
 [ -2.742266  ]
 [ -1.476884  ]
 [ -4.675245  ]
 [  1.6947138 ]
 [ -6.623912  ]
 [ -1.2058723 ]
 [ -4.6671147 ]
 [-10.420316  ]
 [ -0.34837937]
 [ -5.0067415 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [25. 25.  1.] 
cards in deck: 26 
card top of deck: [] 
played cards: [29. 29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25] -> size -> 34 
action values: 1 
buys: 1 
player value: 5 
card supply: [24. 21. 30. 27. 30.  8.  4.  9.  9.  6.  3.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [8. 8. 0. 1. 6.] 
adversary cards in discard: [ 1. 11. 10.  6.  0. 10.] 
adversary owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0  1] -> size -> 18 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0 100   0   0   0   0   0   0   0   0   0] 
sum of rewards: 275 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -5.213027000427246



buy possibilites: [-1] 
expected returns: [[22.438112]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [25. 25.  1. 25.] 
cards in deck: 26 
card top of deck: [] 
played cards: [29. 29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25] -> size -> 35 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 21. 30. 27. 30.  8.  4.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [8. 8. 0. 1. 6.] 
adversary cards in discard: [ 1. 11. 10.  6.  0. 10.] 
adversary owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0  1] -> size -> 18 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0 100   0   0   0   0   0   0   0 250   0] 
sum of rewards: 525 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 1.6947219371795654






Player: 1 
cards in hand: [8. 8. 0. 1. 6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [8. 8. 0. 1. 6.] 
cards in discard: [ 1. 11. 10.  6.  0. 10.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0 11  8  0  3  8  0  1  6  3  6 10  6  6 10  6  0  1] -> size -> 18 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 21. 30. 27. 30.  8.  4.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [15.  1. 25. 29.  0.] 
adversary cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25] -> size -> 35 
adversary victory points: 3
player victory points: -3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [8. 0. 6.] 
cards in discard: [ 1. 11. 10.  6.  0. 10.] 
cards in deck: 7 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1] -> size -> 17 
action values: 0 
buys: 0 
player value: 0 
card supply: [24. 21. 30. 27. 30.  8.  4.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [15.  1. 25. 29.  0.] 
adversary cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25] -> size -> 35 
adversary victory points: 3
player victory points: -3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [8. 0. 6.] 
cards in discard: [ 1. 11. 10.  6.  0. 10.] 
cards in deck: 7 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1] -> size -> 17 
action values: 0 
buys: 1 
player value: 1 
card supply: [24. 21. 30. 27. 30.  8.  4.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [15.  1. 25. 29.  0.] 
adversary cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25] -> size -> 35 
adversary victory points: 3
player victory points: -3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [8. 0. 6.] 
cards in discard: [ 1. 11. 10.  6.  0. 10.  0.] 
cards in deck: 7 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0] -> size -> 18 
action values: 0 
buys: 0 
player value: 1 
card supply: [23. 21. 30. 27. 30.  8.  4.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [15.  1. 25. 29.  0.] 
adversary cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25] -> size -> 35 
adversary victory points: 3
player victory points: -3 





         -------------------- Turn: 29 -------------------- 
Player: 0 
cards in hand: [15.  1. 25. 29.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 25. 29.] 
expected returns: [[38.34444 ]
 [43.036156]
 [45.113045]
 [45.994072]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  1. 25. 29.  0.] 
cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.] 
cards in deck: 21 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25] -> size -> 35 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 21. 30. 27. 30.  8.  4.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [3. 0. 6. 6. 6.] 
adversary cards in discard: [ 1. 11. 10.  6.  0. 10.  0.  8.  8.  0.  6.] 
adversary owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0] -> size -> 18 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 175 

action type: buy - action -1
Learning step: 0
desired expected reward: 22.438112258911133



action possibilites: [-1. 15. 25.] 
expected returns: [[41.312637]
 [46.165756]
 [48.30671 ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 25.  0.  0.] 
cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.] 
cards in deck: 20 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25] -> size -> 35 
action values: 1 
buys: 0 
player value: 1 
card supply: [23. 21. 30. 27. 30.  8.  4.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [3. 0. 6. 6. 6.] 
adversary cards in discard: [ 1. 11. 10.  6.  0. 10.  0.  8.  8.  0.  6.] 
adversary owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0] -> size -> 18 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 195 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 42.43555450439453



action possibilites: [-1] 
expected returns: [[-3.2124782]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0.  0. 29. 15.] 
cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.] 
cards in deck: 18 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25] -> size -> 35 
action values: 0 
buys: 0 
player value: 1 
card supply: [23. 21. 30. 27. 30.  8.  3.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [3. 0. 6. 6. 6.] 
adversary cards in discard: [ 1. 11. 10.  6.  0. 10.  0.  8.  8.  0.  6.  6.] 
adversary owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0  6] -> size -> 19 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 215 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 48.30672073364258





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[ -5.3067007 ]
 [ -0.43755245]
 [ -3.4762523 ]
 [-14.150455  ]
 [ -1.0487211 ]
 [ -3.9851599 ]
 [ -3.773644  ]
 [ -3.6104755 ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  0.  0. 29. 15.] 
cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.] 
cards in deck: 18 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25] -> size -> 35 
action values: 0 
buys: 1 
player value: 3 
card supply: [23. 21. 30. 27. 30.  8.  3.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [3. 0. 6. 6. 6.] 
adversary cards in discard: [ 1. 11. 10.  6.  0. 10.  0.  8.  8.  0.  6.  6.] 
adversary owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0  6] -> size -> 19 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 215 

action type: take_action - action -1
Learning step: 0
desired expected reward: -3.2124781608581543



buy possibilites: [-1] 
expected returns: [[15.590207]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  0.  0. 29. 15.] 
cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1.] 
cards in deck: 18 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1] -> size -> 36 
action values: 0 
buys: 0 
player value: 0 
card supply: [23. 20. 30. 27. 30.  8.  3.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [3. 0. 6. 6. 6.] 
adversary cards in discard: [ 1. 11. 10.  6.  0. 10.  0.  8.  8.  0.  6.  6.] 
adversary owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0  6] -> size -> 19 
adversary victory points: -3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 180   0   0  40   0   0   0   0 -10   0   0  54   0] 
sum of rewards: 259 

action type: buy - action 1.0
Learning step: 0
desired expected reward: -0.4375479221343994






Player: 1 
cards in hand: [3. 0. 6. 6. 6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 6. 6. 6.] 
cards in discard: [ 1. 11. 10.  6.  0. 10.  0.  8.  8.  0.  6.  6.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0  6] -> size -> 19 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 20. 30. 27. 30.  8.  3.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 1.  3. 15. 15.  3.] 
adversary cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1. 29. 25. 15.  0.  0. 29. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1] -> size -> 36 
adversary victory points: 3
player victory points: -4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 6. 6. 6.] 
cards in discard: [ 1. 11. 10.  6.  0. 10.  0.  8.  8.  0.  6.  6.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0  6] -> size -> 19 
action values: 0 
buys: 1 
player value: 1 
card supply: [23. 20. 30. 27. 30.  8.  3.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 1.  3. 15. 15.  3.] 
adversary cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1. 29. 25. 15.  0.  0. 29. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1] -> size -> 36 
adversary victory points: 3
player victory points: -4 


 --------- NOTHING HAPPENED --------- 



         -------------------- Turn: 30 -------------------- 
Player: 0 
cards in hand: [ 1.  3. 15. 15.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 15.] 
expected returns: [[-9.704089 ]
 [-6.0736427]
 [-6.0736427]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  3. 15. 15.  3.] 
cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1. 29. 25. 15.  0.  0. 29. 15.] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1] -> size -> 36 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 20. 30. 27. 30.  8.  3.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 0.  6. 10.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0  6] -> size -> 19 
adversary victory points: -4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 210   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 205 

action type: buy - action -1
Learning step: 0
desired expected reward: 15.59020709991455



action possibilites: [-1] 
expected returns: [[-14.2944765]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  3. 15.  3.] 
cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1. 29. 25. 15.  0.  0. 29. 15.] 
cards in deck: 13 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1] -> size -> 36 
action values: 0 
buys: 0 
player value: 0 
card supply: [23. 20. 30. 27. 30.  8.  3.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 0.  6. 10.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0  6] -> size -> 19 
adversary victory points: -4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 210   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 225 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: -6.073651313781738





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
expected returns: [[-16.97664 ]
 [-15.150918]
 [-26.032455]
 [-15.53646 ]
 [-15.273654]]
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  3. 15.  3.] 
cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1. 29. 25. 15.  0.  0. 29. 15.] 
cards in deck: 13 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1] -> size -> 36 
action values: 0 
buys: 1 
player value: 2 
card supply: [23. 20. 30. 27. 30.  8.  3.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 0.  6. 10.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0  6] -> size -> 19 
adversary victory points: -4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 210   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 225 

action type: take_action - action -1
Learning step: 0
desired expected reward: -14.294476509094238



buy possibilites: [-1] 
expected returns: [[-16.205297]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  3. 15.  3.] 
cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1. 29. 25. 15.  0.  0. 29. 15.
  3.] 
cards in deck: 13 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3] -> size -> 37 
action values: 0 
buys: 0 
player value: 0 
card supply: [23. 20. 30. 26. 30.  8.  3.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 0.  6. 10.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0  6] -> size -> 19 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0  20   0   0   0   0 -20   0   0  16   0] 
sum of rewards: 251 

action type: buy - action 3.0
Learning step: 0
desired expected reward: -15.150924682617188






Player: 1 
cards in hand: [ 0.  6. 10.  0.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 10.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  6. 10.  0.  3.] 
cards in discard: [] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0  6] -> size -> 19 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 20. 30. 26. 30.  8.  3.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [29. 29.  3.  0.  0.] 
adversary cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1. 29. 25. 15.  0.  0. 29. 15.
  3. 15.  1.  3. 15.  3.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3] -> size -> 37 
adversary victory points: 4
player victory points: -4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  6. 10.  0.  3.] 
cards in discard: [] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0  6] -> size -> 19 
action values: 0 
buys: 1 
player value: 2 
card supply: [23. 20. 30. 26. 30.  8.  3.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [29. 29.  3.  0.  0.] 
adversary cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1. 29. 25. 15.  0.  0. 29. 15.
  3. 15.  1.  3. 15.  3.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3] -> size -> 37 
adversary victory points: 4
player victory points: -4 


 --------- NOTHING HAPPENED --------- 



         -------------------- Turn: 31 -------------------- 
Player: 0 
cards in hand: [29. 29.  3.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[-7.892296 ]
 [-1.7742765]
 [-1.7742765]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29.  3.  0.  0.] 
cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1. 29. 25. 15.  0.  0. 29. 15.
  3. 15.  1.  3. 15.  3.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3] -> size -> 37 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 20. 30. 26. 30.  8.  3.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 8.  0. 10.  6.  3.] 
adversary cards in discard: [ 0.  6. 10.  0.  3.] 
adversary owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0  6] -> size -> 19 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 235 

action type: buy - action -1
Learning step: 0
desired expected reward: -16.205297470092773



action possibilites: [-1. 25.] 
expected returns: [[-0.2366612]
 [ 5.6550226]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0.  0. 25.] 
cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1. 29. 25. 15.  0.  0. 29. 15.
  3. 15.  1.  3. 15.  3. 29.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3] -> size -> 37 
action values: 1 
buys: 0 
player value: 1 
card supply: [23. 20. 30. 26. 30.  8.  3.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 8.  0. 10.  6.  3.] 
adversary cards in discard: [ 0.  6. 10.  0.  3.] 
adversary owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0  6] -> size -> 19 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 255 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -4.63995361328125



action possibilites: [-1] 
expected returns: [[-20.328403]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0.  0. 25. 25.] 
cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1. 29. 25. 15.  0.  0. 29. 15.
  3. 15.  1.  3. 15.  3. 29.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3] -> size -> 37 
action values: 0 
buys: 0 
player value: 1 
card supply: [23. 20. 30. 26. 30.  8.  2.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 8.  0. 10.  6.  3.] 
adversary cards in discard: [ 0.  6. 10.  0.  3.  6.] 
adversary owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0  6  6] -> size -> 20 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 275 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 5.655026435852051





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[-21.885057]
 [-17.020674]
 [-20.01844 ]
 [-30.318687]
 [-17.666035]
 [-20.622513]
 [-20.41759 ]
 [-20.27231 ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0.  0. 25. 25.] 
cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1. 29. 25. 15.  0.  0. 29. 15.
  3. 15.  1.  3. 15.  3. 29.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3] -> size -> 37 
action values: 0 
buys: 1 
player value: 3 
card supply: [23. 20. 30. 26. 30.  8.  2.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 8.  0. 10.  6.  3.] 
adversary cards in discard: [ 0.  6. 10.  0.  3.  6.] 
adversary owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0  6  6] -> size -> 20 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 275 

action type: take_action - action -1
Learning step: 0
desired expected reward: -20.32840347290039



buy possibilites: [-1] 
expected returns: [[-19.674955]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0.  0. 25. 25.] 
cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1. 29. 25. 15.  0.  0. 29. 15.
  3. 15.  1.  3. 15.  3. 29.  1.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1] -> size -> 38 
action values: 0 
buys: 0 
player value: 0 
card supply: [23. 19. 30. 26. 30.  8.  2.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 8.  0. 10.  6.  3.] 
adversary cards in discard: [ 0.  6. 10.  0.  3.  6.] 
adversary owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0  6  6] -> size -> 20 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0  40   0   0   0   0 -30   0   0  54   0] 
sum of rewards: 299 

action type: buy - action 1.0
Learning step: 0
desired expected reward: -17.02067756652832






Player: 1 
cards in hand: [ 8.  0. 10.  6.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8. 10.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 8.  0. 10.  6.  3.] 
cards in discard: [ 0.  6. 10.  0.  3.  6.] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0 11  8  0  3  8  0  6  3  6 10  6  6 10  6  0  1  0  6  6] -> size -> 20 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 19. 30. 26. 30.  8.  2.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [25.  1.  1.  1. 29.] 
adversary cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1. 29. 25. 15.  0.  0. 29. 15.
  3. 15.  1.  3. 15.  3. 29.  1. 29. 25.  3.  0.  0. 25. 25.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1] -> size -> 38 
adversary victory points: 4
player victory points: -5 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3.] 
cards in discard: [ 0.  6. 10.  0.  3.  6.] 
cards in deck: 9 
card top of deck: [] 
played cards: [8.] 
owned cards: [11  8  0  3  8  0  3  6  6  6 10  6  0  1  0  6  6] -> size -> 17 
action values: 0 
buys: 0 
player value: 0 
card supply: [23. 19. 30. 26. 30.  8.  2.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [25.  1.  1.  1. 29.] 
adversary cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1. 29. 25. 15.  0.  0. 29. 15.
  3. 15.  1.  3. 15.  3. 29.  1. 29. 25.  3.  0.  0. 25. 25.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1] -> size -> 38 
adversary victory points: 4
player victory points: -4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3.] 
cards in discard: [ 0.  6. 10.  0.  3.  6.] 
cards in deck: 9 
card top of deck: [] 
played cards: [8.] 
owned cards: [11  8  0  3  8  0  3  6  6  6 10  6  0  1  0  6  6] -> size -> 17 
action values: 0 
buys: 1 
player value: 0 
card supply: [23. 19. 30. 26. 30.  8.  2.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [25.  1.  1.  1. 29.] 
adversary cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1. 29. 25. 15.  0.  0. 29. 15.
  3. 15.  1.  3. 15.  3. 29.  1. 29. 25.  3.  0.  0. 25. 25.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1] -> size -> 38 
adversary victory points: 4
player victory points: -4 





         -------------------- Turn: 32 -------------------- 
Player: 0 
cards in hand: [25.  1.  1.  1. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 29.] 
expected returns: [[-22.59461 ]
 [-17.276783]
 [-16.49519 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  1.  1.  1. 29.] 
cards in discard: [25. 25.  1. 25. 29. 29. 29. 29. 29.  1.  1. 29. 25. 15.  0.  0. 29. 15.
  3. 15.  1.  3. 15.  3. 29.  1. 29. 25.  3.  0.  0. 25. 25.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1] -> size -> 38 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 19. 30. 26. 30.  8.  2.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [0. 6. 6. 1. 8.] 
adversary cards in discard: [ 0.  6. 10.  0.  3.  6.  8.  3.] 
adversary owned cards: [11  8  0  3  8  0  3  6  6  6 10  6  0  1  0  6  6] -> size -> 17 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 235 

action type: buy - action -1
Learning step: 0
desired expected reward: -19.674955368041992



action possibilites: [-1. 29.] 
expected returns: [[46.68654 ]
 [55.135788]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  1.  1. 29.] 
cards in discard: [25.] 
cards in deck: 32 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1] -> size -> 38 
action values: 1 
buys: 0 
player value: 1 
card supply: [23. 19. 30. 26. 30.  8.  2.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [0. 6. 6. 1. 8.] 
adversary cards in discard: [ 0.  6. 10.  0.  3.  6.  8.  3.] 
adversary owned cards: [11  8  0  3  8  0  3  6  6  6 10  6  0  1  0  6  6] -> size -> 17 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 255 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -19.34228515625



action possibilites: [-1.] 
expected returns: [[55.82435]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 1. 1.] 
cards in discard: [25. 29.] 
cards in deck: 31 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1] -> size -> 38 
action values: 1 
buys: 0 
player value: 2 
card supply: [23. 19. 30. 26. 30.  8.  2.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [0. 6. 6. 1. 8.] 
adversary cards in discard: [ 0.  6. 10.  0.  3.  6.  8.  3.] 
adversary owned cards: [11  8  0  3  8  0  3  6  6  6 10  6  0  1  0  6  6] -> size -> 17 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 275 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 51.1918830871582





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  5.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[51.488075]
 [57.950127]
 [44.65117 ]
 [53.87466 ]
 [47.23726 ]
 [57.413567]
 [39.233593]
 [55.993412]
 [57.515587]
 [54.112976]
 [60.94962 ]
 [51.67575 ]
 [57.659973]
 [54.018337]
 [47.483868]
 [58.621002]
 [53.416946]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 1. 1.] 
cards in discard: [25. 29.] 
cards in deck: 31 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1] -> size -> 38 
action values: 0 
buys: 1 
player value: 8 
card supply: [23. 19. 30. 26. 30.  8.  2.  9.  9.  6.  2.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [0. 6. 6. 1. 8.] 
adversary cards in discard: [ 0.  6. 10.  0.  3.  6.  8.  3.] 
adversary owned cards: [11  8  0  3  8  0  3  6  6  6 10  6  0  1  0  6  6] -> size -> 17 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 275 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 55.82434844970703



buy possibilites: [-1] 
expected returns: [[30.422235]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 1. 1.] 
cards in discard: [25. 29. 25.] 
cards in deck: 31 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25] -> size -> 39 
action values: 0 
buys: 0 
player value: 3 
card supply: [23. 19. 30. 26. 30.  8.  2.  9.  9.  6.  1.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [0. 6. 6. 1. 8.] 
adversary cards in discard: [ 0.  6. 10.  0.  3.  6.  8.  3.] 
adversary owned cards: [11  8  0  3  8  0  3  6  6  6 10  6  0  1  0  6  6] -> size -> 17 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5.    0.    0.  240.    0.    0.   40.    0.    0.    0.    0.  -40.
   0.    0.   62.5   0. ] 
sum of rewards: 297.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 60.94962692260742






Player: 1 
cards in hand: [0. 6. 6. 1. 8.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 6. 6. 1. 8.] 
cards in discard: [ 0.  6. 10.  0.  3.  6.  8.  3.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [11  8  0  3  8  0  3  6  6  6 10  6  0  1  0  6  6] -> size -> 17 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 19. 30. 26. 30.  8.  2.  9.  9.  6.  1.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [15.  1. 29. 25. 15.] 
adversary cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25] -> size -> 39 
adversary victory points: 4
player victory points: -4 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [6. 6. 1.] 
cards in discard: [ 0.  6. 10.  0.  3.  6.  8.  3.] 
cards in deck: 4 
card top of deck: [] 
played cards: [8.] 
owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6] -> size -> 16 
action values: 0 
buys: 0 
player value: 0 
card supply: [23. 19. 30. 26. 30.  8.  2.  9.  9.  6.  1.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [15.  1. 29. 25. 15.] 
adversary cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25] -> size -> 39 
adversary victory points: 4
player victory points: -4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [6. 6. 1.] 
cards in discard: [ 0.  6. 10.  0.  3.  6.  8.  3.] 
cards in deck: 4 
card top of deck: [] 
played cards: [8.] 
owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6] -> size -> 16 
action values: 0 
buys: 1 
player value: 2 
card supply: [23. 19. 30. 26. 30.  8.  2.  9.  9.  6.  1.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [15.  1. 29. 25. 15.] 
adversary cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25] -> size -> 39 
adversary victory points: 4
player victory points: -4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [6. 6. 1.] 
cards in discard: [ 0.  6. 10.  0.  3.  6.  8.  3.  3.] 
cards in deck: 4 
card top of deck: [] 
played cards: [8.] 
owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3] -> size -> 17 
action values: 0 
buys: 0 
player value: 0 
card supply: [23. 19. 30. 25. 30.  8.  2.  9.  9.  6.  1.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [15.  1. 29. 25. 15.] 
adversary cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25] -> size -> 39 
adversary victory points: 4
player victory points: -3 





         -------------------- Turn: 33 -------------------- 
Player: 0 
cards in hand: [15.  1. 29. 25. 15.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 29. 25. 15.] 
expected returns: [[59.800713]
 [64.55874 ]
 [67.57468 ]
 [66.65054 ]
 [64.55874 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  1. 29. 25. 15.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.] 
cards in deck: 26 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25] -> size -> 39 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 19. 30. 25. 30.  8.  2.  9.  9.  6.  1.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 0.  6.  6.  0. 11.] 
adversary cards in discard: [] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3] -> size -> 17 
adversary victory points: -3
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 210   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 205 

action type: buy - action -1
Learning step: 0
desired expected reward: 30.4222354888916



action possibilites: [-1. 15. 25. 15.] 
expected returns: [[44.797714]
 [49.59512 ]
 [51.71018 ]
 [49.59512 ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  1. 25. 15.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.] 
cards in deck: 25 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25] -> size -> 39 
action values: 1 
buys: 0 
player value: 1 
card supply: [23. 19. 30. 25. 30.  8.  2.  9.  9.  6.  1.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 0.  6.  6.  0. 11.] 
adversary cards in discard: [] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3] -> size -> 17 
adversary victory points: -3
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 210   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 225 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 63.95245361328125



action possibilites: [-1] 
expected returns: [[44.91997]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  1. 15. 25. 25.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.] 
cards in deck: 23 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25] -> size -> 39 
action values: 0 
buys: 0 
player value: 1 
card supply: [23. 19. 30. 25. 30.  8.  1.  9.  9.  6.  1.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 0.  6.  6.  0. 11.] 
adversary cards in discard: [6.] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6] -> size -> 18 
adversary victory points: -3
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 210   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 245 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 51.710182189941406





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[42.950233]
 [48.596043]
 [45.124382]
 [32.962696]
 [48.00717 ]
 [44.72065 ]
 [44.82457 ]
 [44.698273]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  1. 15. 25. 25.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.] 
cards in deck: 23 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25] -> size -> 39 
action values: 0 
buys: 1 
player value: 3 
card supply: [23. 19. 30. 25. 30.  8.  1.  9.  9.  6.  1.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 0.  6.  6.  0. 11.] 
adversary cards in discard: [6.] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6] -> size -> 18 
adversary victory points: -3
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 210   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 245 

action type: take_action - action -1
Learning step: 0
desired expected reward: 44.91997146606445



buy possibilites: [-1] 
expected returns: [[30.920118]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  1. 15. 25. 25.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1.] 
cards in deck: 23 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1] -> size -> 40 
action values: 0 
buys: 0 
player value: 0 
card supply: [23. 18. 30. 25. 30.  8.  1.  9.  9.  6.  1.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 0.  6.  6.  0. 11.] 
adversary cards in discard: [6.] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6] -> size -> 18 
adversary victory points: -3
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 210   0   0  40   0   0   0   0 -50   0   0  54   0] 
sum of rewards: 249 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 48.596046447753906






Player: 1 
cards in hand: [ 0.  6.  6.  0. 11.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  6.  6.  0. 11.] 
cards in discard: [6.] 
cards in deck: 12 
card top of deck: [] 
played cards: [] 
owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6] -> size -> 18 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 18. 30. 25. 30.  8.  1.  9.  9.  6.  1.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 3. 29.  3. 25.  0.] 
adversary cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1] -> size -> 40 
adversary victory points: 4
player victory points: -4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  6.  6.  0. 11.] 
cards in discard: [6.] 
cards in deck: 12 
card top of deck: [] 
played cards: [] 
owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6] -> size -> 18 
action values: 0 
buys: 1 
player value: 2 
card supply: [23. 18. 30. 25. 30.  8.  1.  9.  9.  6.  1.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [ 3. 29.  3. 25.  0.] 
adversary cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1] -> size -> 40 
adversary victory points: 4
player victory points: -4 


 --------- NOTHING HAPPENED --------- 



         -------------------- Turn: 34 -------------------- 
Player: 0 
cards in hand: [ 3. 29.  3. 25.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 25.] 
expected returns: [[-0.24807143]
 [ 5.762622  ]
 [ 5.024275  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29.  3. 25.  0.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.] 
cards in deck: 18 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1] -> size -> 40 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 18. 30. 25. 30.  8.  1.  9.  9.  6.  1.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [10.  3.  6.  6.  6.] 
adversary cards in discard: [ 6.  0.  6.  6.  0. 11.] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6] -> size -> 18 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 235 

action type: buy - action -1
Learning step: 0
desired expected reward: 30.92011833190918



action possibilites: [-1. 25. 15.] 
expected returns: [[-2.6699867]
 [ 2.6411374]
 [ 1.0466974]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 25.  0. 15.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.] 
cards in deck: 17 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1] -> size -> 40 
action values: 1 
buys: 0 
player value: 1 
card supply: [23. 18. 30. 25. 30.  8.  1.  9.  9.  6.  1.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [10.  3.  6.  6.  6.] 
adversary cards in discard: [ 6.  0.  6.  6.  0. 11.] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6] -> size -> 18 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 255 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 2.9591944217681885



action possibilites: [-1] 
expected returns: [[-5.7789674]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 15.  1. 29.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1] -> size -> 40 
action values: 0 
buys: 0 
player value: 1 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  9.  6.  1.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [10.  3.  6.  6.  6.] 
adversary cards in discard: [ 6.  0.  6.  6.  0. 11.  6.] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6] -> size -> 19 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 275 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 2.6411349773406982





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[-7.3548927]
 [-2.4857855]
 [-5.524514 ]
 [-3.9114554]
 [-3.0969205]
 [-6.0333247]
 [-7.0450287]
 [-5.8218184]
 [-2.0272884]
 [-5.658674 ]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0. 15.  1. 29.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1] -> size -> 40 
action values: 0 
buys: 1 
player value: 4 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  9.  6.  1.  0. 10. 10.  8. 10.  6.] 
adversary cards in hand: [10.  3.  6.  6.  6.] 
adversary cards in discard: [ 6.  0.  6.  6.  0. 11.  6.] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6] -> size -> 19 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 275 

action type: take_action - action -1
Learning step: 0
desired expected reward: -5.778967380523682



buy possibilites: [-1] 
expected returns: [[3.445635]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0. 15.  1. 29.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.
 15.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15] -> size -> 41 
action values: 0 
buys: 0 
player value: 0 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  9.  6.  1.  0. 10. 10.  8. 10.  5.] 
adversary cards in hand: [10.  3.  6.  6.  6.] 
adversary cards in discard: [ 6.  0.  6.  6.  0. 11.  6.] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6] -> size -> 19 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0  40   0   0   0   0 -60   0   0 128   0] 
sum of rewards: 343 

action type: buy - action 15.0
Learning step: 0
desired expected reward: -2.0272908210754395






Player: 1 
cards in hand: [10.  3.  6.  6.  6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 10.] 
Chosen action: 10 : ['Village' '10' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [10.  3.  6.  6.  6.] 
cards in discard: [ 6.  0.  6.  6.  0. 11.  6.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6] -> size -> 19 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  9.  6.  1.  0. 10. 10.  8. 10.  5.] 
adversary cards in hand: [25. 25.  1.  1. 29.] 
adversary cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.
 15. 29. 25.  3.  0. 15.  1. 29.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15] -> size -> 41 
adversary victory points: 4
player victory points: -5 


action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 6. 6. 6. 3.] 
cards in discard: [ 6.  0.  6.  6.  0. 11.  6.] 
cards in deck: 6 
card top of deck: [] 
played cards: [10.] 
owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6] -> size -> 19 
action values: 2 
buys: 0 
player value: 0 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  9.  6.  1.  0. 10. 10.  8. 10.  5.] 
adversary cards in hand: [25. 25.  1.  1. 29.] 
adversary cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.
 15. 29. 25.  3.  0. 15.  1. 29.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15] -> size -> 41 
adversary victory points: 4
player victory points: -5 




----------- BUY PHASE ----------- 
buy possibilites: [ 0. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 6. 6. 6. 3.] 
cards in discard: [ 6.  0.  6.  6.  0. 11.  6.] 
cards in deck: 6 
card top of deck: [] 
played cards: [10.] 
owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6] -> size -> 19 
action values: 2 
buys: 1 
player value: 0 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  9.  6.  1.  0. 10. 10.  8. 10.  5.] 
adversary cards in hand: [25. 25.  1.  1. 29.] 
adversary cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.
 15. 29. 25.  3.  0. 15.  1. 29.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15] -> size -> 41 
adversary victory points: 4
player victory points: -5 





         -------------------- Turn: 35 -------------------- 
Player: 0 
cards in hand: [25. 25.  1.  1. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 25. 29.] 
expected returns: [[1.975518 ]
 [7.2908125]
 [7.2908125]
 [8.090724 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 25.  1.  1. 29.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.
 15. 29. 25.  3.  0. 15.  1. 29.] 
cards in deck: 10 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15] -> size -> 41 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  9.  6.  1.  0. 10. 10.  8. 10.  5.] 
adversary cards in hand: [3. 1. 0. 6. 8.] 
adversary cards in discard: [ 6.  0.  6.  6.  0. 11.  6. 10.  3.  6.  6.  6.  3.] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6] -> size -> 19 
adversary victory points: -5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 270   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 265 

action type: buy - action -1
Learning step: 0
desired expected reward: 3.4456350803375244



action possibilites: [-1. 25. 25. 29.] 
expected returns: [[15.449346]
 [20.76638 ]
 [20.76638 ]
 [21.56629 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 25. 29.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.
 15. 29. 25.  3.  0. 15.  1. 29.  1.  1.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15] -> size -> 41 
action values: 1 
buys: 0 
player value: 1 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  9.  6.  1.  0. 10. 10.  8. 10.  5.] 
adversary cards in hand: [3. 1. 0. 6. 8.] 
adversary cards in discard: [ 6.  0.  6.  6.  0. 11.  6. 10.  3.  6.  6.  6.  3.] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6] -> size -> 19 
adversary victory points: -5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 270   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 285 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 5.228754997253418



action possibilites: [-1. 25.] 
expected returns: [[0.43498135]
 [5.750186  ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.
 15. 29. 25.  3.  0. 15.  1. 29.  1.  1. 25.  3.] 
cards in deck: 8 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15] -> size -> 41 
action values: 1 
buys: 0 
player value: 2 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  9.  6.  1.  0. 10. 10.  8. 10.  5.] 
adversary cards in hand: [3. 1. 0. 6. 8.] 
adversary cards in discard: [ 6.  0.  6.  6.  0. 11.  6. 10.  3.  6.  6.  6.  3.] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6] -> size -> 19 
adversary victory points: -5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 270   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 305 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 18.702775955200195



action possibilites: [-1] 
expected returns: [[-4.8676705]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.
 15. 29. 25.  3.  0. 15.  1. 29.  1.  1. 25.  3.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15] -> size -> 41 
action values: 0 
buys: 0 
player value: 2 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  9.  6.  1.  0. 10. 10.  8. 10.  5.] 
adversary cards in hand: [3. 1. 0. 6. 8.] 
adversary cards in discard: [ 6.  0.  6.  6.  0. 11.  6. 10.  3.  6.  6.  6.  3.] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6] -> size -> 19 
adversary victory points: -5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 270   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 325 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 5.750185012817383





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[-6.4804134]
 [-1.6150079]
 [-4.613492 ]
 [-3.0944552]
 [-2.259711 ]
 [-5.217872 ]
 [-6.1384687]
 [-5.012948 ]
 [-1.14889  ]
 [-4.8676724]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.
 15. 29. 25.  3.  0. 15.  1. 29.  1.  1. 25.  3.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15] -> size -> 41 
action values: 0 
buys: 1 
player value: 4 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  9.  6.  1.  0. 10. 10.  8. 10.  5.] 
adversary cards in hand: [3. 1. 0. 6. 8.] 
adversary cards in discard: [ 6.  0.  6.  6.  0. 11.  6. 10.  3.  6.  6.  6.  3.] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6] -> size -> 19 
adversary victory points: -5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 270   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 325 

action type: take_action - action -1
Learning step: 0
desired expected reward: -4.86767053604126



buy possibilites: [-1] 
expected returns: [[-3.1003318]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.
 15. 29. 25.  3.  0. 15.  1. 29.  1.  1. 25.  3. 15.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15 15] -> size -> 42 
action values: 0 
buys: 0 
player value: 0 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  9.  6.  1.  0. 10. 10.  8. 10.  4.] 
adversary cards in hand: [3. 1. 0. 6. 8.] 
adversary cards in discard: [ 6.  0.  6.  6.  0. 11.  6. 10.  3.  6.  6.  6.  3.] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6] -> size -> 19 
adversary victory points: -5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 270   0   0  60   0   0   0   0 -70   0   0 128   0] 
sum of rewards: 383 

action type: buy - action 15.0
Learning step: 0
desired expected reward: -1.1488893032073975






Player: 1 
cards in hand: [3. 1. 0. 6. 8.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 1. 0. 6. 8.] 
cards in discard: [ 6.  0.  6.  6.  0. 11.  6. 10.  3.  6.  6.  6.  3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6] -> size -> 19 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  9.  6.  1.  0. 10. 10.  8. 10.  4.] 
adversary cards in hand: [ 1. 29. 29. 15.  0.] 
adversary cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.
 15. 29. 25.  3.  0. 15.  1. 29.  1.  1. 25.  3. 15. 29. 29. 25.  0.  0.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15 15] -> size -> 42 
adversary victory points: 4
player victory points: -5 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3. 11.  8. 10. -1.] 
Chosen buy action: 11.0 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 1. 0. 6. 8.] 
cards in discard: [ 6.  0.  6.  6.  0. 11.  6. 10.  3.  6.  6.  6.  3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6] -> size -> 19 
action values: 0 
buys: 1 
player value: 3 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  9.  6.  1.  0. 10. 10.  8. 10.  4.] 
adversary cards in hand: [ 1. 29. 29. 15.  0.] 
adversary cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.
 15. 29. 25.  3.  0. 15.  1. 29.  1.  1. 25.  3. 15. 29. 29. 25.  0.  0.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15 15] -> size -> 42 
adversary victory points: 4
player victory points: -5 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 1. 0. 6. 8.] 
cards in discard: [ 6.  0.  6.  6.  0. 11.  6. 10.  3.  6.  6.  6.  3. 11.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6 11] -> size -> 20 
action values: 0 
buys: 0 
player value: 0 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  8.  6.  1.  0. 10. 10.  8. 10.  4.] 
adversary cards in hand: [ 1. 29. 29. 15.  0.] 
adversary cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.
 15. 29. 25.  3.  0. 15.  1. 29.  1.  1. 25.  3. 15. 29. 29. 25.  0.  0.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15 15] -> size -> 42 
adversary victory points: 4
player victory points: -5 





         -------------------- Turn: 36 -------------------- 
Player: 0 
cards in hand: [ 1. 29. 29. 15.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 15.] 
expected returns: [[-19.399015]
 [-12.229413]
 [-12.229413]
 [-15.109821]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 29. 29. 15.  0.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.
 15. 29. 25.  3.  0. 15.  1. 29.  1.  1. 25.  3. 15. 29. 29. 25.  0.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15 15] -> size -> 42 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  8.  6.  1.  0. 10. 10.  8. 10.  4.] 
adversary cards in hand: [6. 6. 1. 6. 8.] 
adversary cards in discard: [] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6 11] -> size -> 20 
adversary victory points: -5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 270   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 265 

action type: buy - action -1
Learning step: 0
desired expected reward: -3.1003317832946777



action possibilites: [-1. 29. 25.] 
expected returns: [[1.0133586]
 [7.7015944]
 [6.862797 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0. 25.] 
cards in discard: [25. 29. 25. 29. 29.  1.  1.  1.  3.  1. 29. 25. 15.  1. 15. 25. 25.  3.
 15. 29. 25.  3.  0. 15.  1. 29.  1.  1. 25.  3. 15. 29. 29. 25.  0.  0.
  1. 15.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15 15] -> size -> 42 
action values: 1 
buys: 0 
player value: 1 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  8.  6.  1.  0. 10. 10.  8. 10.  4.] 
adversary cards in hand: [6. 6. 1. 6. 8.] 
adversary cards in discard: [] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6 11] -> size -> 20 
adversary victory points: -5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 270   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 285 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -15.67081356048584



action possibilites: [-1. 29.] 
expected returns: [[-1.7057467]
 [ 5.329174 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.] 
cards in discard: [ 0. 25.] 
cards in deck: 37 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15 15] -> size -> 42 
action values: 1 
buys: 0 
player value: 2 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  8.  6.  1.  0. 10. 10.  8. 10.  4.] 
adversary cards in hand: [6. 6. 1. 6. 8.] 
adversary cards in discard: [] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6 11] -> size -> 20 
adversary victory points: -5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 270   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 305 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 4.5785064697265625



action possibilites: [-1.] 
expected returns: [[-5.760709]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [] 
cards in discard: [ 0. 25. 15.] 
cards in deck: 36 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15 15] -> size -> 42 
action values: 1 
buys: 0 
player value: 3 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  8.  6.  1.  0. 10. 10.  8. 10.  4.] 
adversary cards in hand: [6. 6. 1. 6. 8.] 
adversary cards in discard: [] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6 11] -> size -> 20 
adversary victory points: -5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 270   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 325 

action type: discard_n_cards - action 0
Learning step: 0
desired expected reward: -3.4295318126678467





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3. 11.  8. 10. -1.] 
expected returns: [[-7.3319254]
 [-1.7715895]
 [-5.171798 ]
 [-2.3610451]
 [-5.648534 ]
 [-5.520919 ]
 [-5.5926204]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [ 0. 25. 15.] 
cards in deck: 36 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15 15] -> size -> 42 
action values: 1 
buys: 1 
player value: 3 
card supply: [23. 18. 30. 25. 30.  8.  0.  9.  8.  6.  1.  0. 10. 10.  8. 10.  4.] 
adversary cards in hand: [6. 6. 1. 6. 8.] 
adversary cards in discard: [] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6 11] -> size -> 20 
adversary victory points: -5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 270   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 325 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -5.760708808898926



buy possibilites: [-1] 
expected returns: [[17.762156]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [ 0. 25. 15.  1.] 
cards in deck: 36 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15 15  1] -> size -> 43 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 17. 30. 25. 30.  8.  0.  9.  8.  6.  1.  0. 10. 10.  8. 10.  4.] 
adversary cards in hand: [6. 6. 1. 6. 8.] 
adversary cards in discard: [] 
adversary owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6 11] -> size -> 20 
adversary victory points: -5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 270   0   0  60   0   0   0   0 -80   0   0  54   0] 
sum of rewards: 299 

action type: buy - action 1.0
Learning step: 0
desired expected reward: -1.7715911865234375






Player: 1 
cards in hand: [6. 6. 1. 6. 8.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [6. 6. 1. 6. 8.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [11  8  3  8  0  3  6  6  6 10  6  0  1  0  6  6  3  6  6 11] -> size -> 20 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 17. 30. 25. 30.  8.  0.  9.  8.  6.  1.  0. 10. 10.  8. 10.  4.] 
adversary cards in hand: [15.  3.  1. 25.  3.] 
adversary cards in discard: [ 0. 25. 15.  1. 29. 29. 29.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15 15  1] -> size -> 43 
adversary victory points: 4
player victory points: -5 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [6. 6.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [8.] 
owned cards: [11  8  3  8  0  3  6  6 10  6  0  0  6  6  3  6  6 11] -> size -> 18 
action values: 0 
buys: 0 
player value: 0 
card supply: [23. 17. 30. 25. 30.  8.  0.  9.  8.  6.  1.  0. 10. 10.  8. 10.  4.] 
adversary cards in hand: [15.  3.  1. 25.  3.] 
adversary cards in discard: [ 0. 25. 15.  1. 29. 29. 29.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15 15  1] -> size -> 43 
adversary victory points: 4
player victory points: -4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [6. 6.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [8.] 
owned cards: [11  8  3  8  0  3  6  6 10  6  0  0  6  6  3  6  6 11] -> size -> 18 
action values: 0 
buys: 1 
player value: 0 
card supply: [23. 17. 30. 25. 30.  8.  0.  9.  8.  6.  1.  0. 10. 10.  8. 10.  4.] 
adversary cards in hand: [15.  3.  1. 25.  3.] 
adversary cards in discard: [ 0. 25. 15.  1. 29. 29. 29.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15 15  1] -> size -> 43 
adversary victory points: 4
player victory points: -4 





         -------------------- Turn: 37 -------------------- 
Player: 0 
cards in hand: [15.  3.  1. 25.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 25.] 
expected returns: [[14.279174]
 [19.371843]
 [21.630358]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  3.  1. 25.  3.] 
cards in discard: [ 0. 25. 15.  1. 29. 29. 29.] 
cards in deck: 31 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15 15  1] -> size -> 43 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 17. 30. 25. 30.  8.  0.  9.  8.  6.  1.  0. 10. 10.  8. 10.  4.] 
adversary cards in hand: [ 3. 10.  3.  3.  6.] 
adversary cards in discard: [8. 6. 6.] 
adversary owned cards: [11  8  3  8  0  3  6  6 10  6  0  0  6  6  3  6  6 11] -> size -> 18 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 235 

action type: buy - action -1
Learning step: 0
desired expected reward: 17.762155532836914



action possibilites: [-1] 
expected returns: [[6.2585154]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  3.  1.  3.  1.  0.] 
cards in discard: [ 0. 25. 15.  1. 29. 29. 29.] 
cards in deck: 29 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15 15  1] -> size -> 43 
action values: 0 
buys: 0 
player value: 0 
card supply: [23. 17. 30. 25. 30.  8.  0.  9.  8.  6.  1.  0. 10. 10.  8. 10.  4.] 
adversary cards in hand: [ 3. 10.  3.  3.  6.] 
adversary cards in discard: [8. 6. 6.] 
adversary owned cards: [11  8  3  8  0  3  6  6 10  6  0  0  6  6  3  6  6 11] -> size -> 18 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 255 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 21.630353927612305





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 3.7811892]
 [ 9.361168 ]
 [ 5.9134016]
 [ 0.6292317]
 [ 7.6202765]
 [ 8.741778 ]
 [ 5.5283566]
 [11.841323 ]
 [ 4.0999327]
 [ 9.2086315]
 [ 5.624592 ]
 [ 0.7430606]
 [ 9.920654 ]
 [ 5.488353 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  3.  1.  3.  1.  0.] 
cards in discard: [ 0. 25. 15.  1. 29. 29. 29.] 
cards in deck: 29 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15 15  1] -> size -> 43 
action values: 0 
buys: 1 
player value: 5 
card supply: [23. 17. 30. 25. 30.  8.  0.  9.  8.  6.  1.  0. 10. 10.  8. 10.  4.] 
adversary cards in hand: [ 3. 10.  3.  3.  6.] 
adversary cards in discard: [8. 6. 6.] 
adversary owned cards: [11  8  3  8  0  3  6  6 10  6  0  0  6  6  3  6  6 11] -> size -> 18 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 240   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 255 

action type: take_action - action -1
Learning step: 0
desired expected reward: 6.258515357971191



Player 0 won the game! 



Player 0 bought cards:
Copper: 0 
Silver: 10 
Gold: 0 
Estate: 1 
Duchy: 0 
Province: 0 
Curse: 0 

Remodel: 0 
Workshop: 0 
Chapel: 0 
Witch: 10 
Poacher: 10 
Militia: 0 
Market: 0 
Village: 0 
Library: 0 
Moneylender: 6 


 --- GAME STATE ---
Action that is taken: None 
cards in hand: [15.  3.  1.  3.  1.  0.] 
cards in discard: [ 0. 25. 15.  1. 29. 29. 29. 25.] 
cards in deck: 29 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29  1 29 29 29 29 29 29 15 15 25 25
 25 25 25  1  1 15 15 25  1 25 25  1  3  1 25  1 15 15  1 25] -> size -> 44 
action values: 0 
buys: 0 
player value: 0 
card supply: [23. 17. 30. 25. 30.  8.  0.  9.  8.  6.  0.  0. 10. 10.  8. 10.  4.] 
adversary cards in hand: [ 3. 10.  3.  3.  6.] 
adversary cards in discard: [8. 6. 6.] 
adversary owned cards: [11  8  3  8  0  3  6  6 10  6  0  0  6  6  3  6  6 11] -> size -> 18 
adversary victory points: -4
player victory points: 4 

Reward from previous game state: 
[     -5 3000000       0     240       0       0      20       0       0
       0       0     -90       0       0     125       0] 
sum of rewards: 3000290 

action type: buy - action 25.0
Learning step: 120011.125
desired expected reward: 120022.96875



