 --------- CARDS IN THIS GAME --------- 
Game cards: [['Copper' '0' '0']
 ['Silver' '1' '3']
 ['Gold' '2' '6']
 ['Estate' '3' '2']
 ['Duchy' '4' '5']
 ['Province' '5' '8']
 ['Curse' '6' '0']
 ['Remodel' '16' '4']
 ['Workshop' '11' '3']
 ['Chapel' '8' '2']
 ['Witch' '25' '5']
 ['Poacher' '29' '4']
 ['Militia' '14' '4']
 ['Market' '23' '5']
 ['Village' '10' '3']
 ['Library' '22' '5']
 ['Moneylender' '15' '4']]






         -------------------- Turn: 1 -------------------- 
Player: 0 
cards in hand: [0. 3. 3. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[13.455385]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 3. 3. 0.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[      -5 -3000000        0     -390        0        0       20        0
        0        0        0        0        0        0        0        0] 
sum of rewards: -3000375 

action type: buy - action 0.0
Learning step: -120008.859375
desired expected reward: -120162.3671875





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
expected returns: [[ 11.746346]
 [ 17.555931]
 [-41.364044]
 [ 15.13434 ]
 [ 12.806818]]
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 3. 3. 0.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 0 
buys: 1 
player value: 2 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 13.51034927368164



buy possibilites: [-1] 
expected returns: [[11.430384]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 3. 3. 0.] 
cards in discard: [3.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3 3] -> size -> 11 
action values: 0 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0 16  0] 
sum of rewards: 41 

action type: buy - action 3.0
Learning step: 0
desired expected reward: 17.555925369262695






Player: 1 
cards in hand: [0. 0. 0. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 0.] 
adversary cards in discard: [3. 0. 3. 3. 3. 0.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 3] -> size -> 11 
adversary victory points: 4
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 0 
buys: 1 
player value: 4 
card supply: [30. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 0.] 
adversary cards in discard: [3. 0. 3. 3. 3. 0.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 3] -> size -> 11 
adversary victory points: 4
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [29.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
action values: 0 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 0.] 
adversary cards in discard: [3. 0. 3. 3. 3. 0.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 3] -> size -> 11 
adversary victory points: 4
player victory points: 3 





         -------------------- Turn: 2 -------------------- 
Player: 0 
cards in hand: [0. 0. 0. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[18.467926]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [3. 0. 3. 3. 3. 0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3 3] -> size -> 11 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 3. 0.] 
adversary cards in discard: [29.  0.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 11.430383682250977





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 17.266459]
 [ 28.985876]
 [ 22.750177]
 [ -1.318893]
 [-31.588156]
 [ 23.02603 ]
 [ 28.295822]
 [ 20.102993]
 [ 31.451754]
 [ 35.009567]
 [ 24.11208 ]
 [ 29.751417]
 [ 25.471796]
 [ 10.48099 ]
 [ 31.186884]
 [ 18.525715]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [3. 0. 3. 3. 3. 0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3 3] -> size -> 11 
action values: 0 
buys: 1 
player value: 5 
card supply: [30. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 3. 0.] 
adversary cards in discard: [29.  0.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 18.007091522216797



buy possibilites: [-1] 
expected returns: [[-15.87953]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [ 3.  0.  3.  3.  3.  0. 29.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29] -> size -> 12 
action values: 0 
buys: 0 
player value: 1 
card supply: [30. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 3. 0.] 
adversary cards in discard: [29.  0.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5.  0.  0. 30.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 57.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 35.00956726074219






Player: 1 
cards in hand: [3. 0. 0. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 3. 0.] 
cards in discard: [29.  0.  0.  0.  0.  3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29] -> size -> 12 
adversary victory points: 4
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 3. 0.] 
cards in discard: [29.  0.  0.  0.  0.  3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
action values: 0 
buys: 1 
player value: 3 
card supply: [30. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29] -> size -> 12 
adversary victory points: 4
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 3. 0.] 
cards in discard: [29.  0.  0.  0.  0.  3.  0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0] -> size -> 12 
action values: 0 
buys: 0 
player value: 3 
card supply: [29. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29] -> size -> 12 
adversary victory points: 4
player victory points: 3 





         -------------------- Turn: 3 -------------------- 
Player: 0 
cards in hand: [0. 3. 0. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[22.94095]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29] -> size -> 12 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0] -> size -> 12 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: -15.87952995300293





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
expected returns: [[ 18.73292 ]
 [ 30.633493]
 [ 24.368364]
 [-33.041996]
 [ 24.71603 ]
 [ 30.177889]
 [ 21.891813]
 [ 36.742516]
 [ 25.816288]
 [ 27.252693]
 [ 33.013855]
 [ 20.03758 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29] -> size -> 12 
action values: 0 
buys: 1 
player value: 4 
card supply: [29. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0] -> size -> 12 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 21.841087341308594



buy possibilites: [-1] 
expected returns: [[25.90831]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [29.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29] -> size -> 13 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0] -> size -> 12 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  30   0   0   0   0   0   0   0   0   0   0 128   0] 
sum of rewards: 153 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 36.742523193359375






Player: 1 
cards in hand: [0. 0. 3. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3. 0. 0.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0] -> size -> 12 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3.  0.  3.  0. 29.] 
adversary cards in discard: [29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29] -> size -> 13 
adversary victory points: 4
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: 14.0 : ['Militia' '14' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 0.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0] -> size -> 12 
action values: 0 
buys: 1 
player value: 4 
card supply: [29. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3.  0.  3.  0. 29.] 
adversary cards in discard: [29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29] -> size -> 13 
adversary victory points: 4
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 0.] 
cards in discard: [14.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14] -> size -> 13 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  7.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 3.  0.  3.  0. 29.] 
adversary cards in discard: [29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29] -> size -> 13 
adversary victory points: 4
player victory points: 3 





         -------------------- Turn: 4 -------------------- 
Player: 0 
cards in hand: [ 3.  0.  3.  0. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[ 0.01767159]
 [16.497227  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0.  3.  0. 29.] 
cards in discard: [29.  0.  3.  0.  0.  0.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29] -> size -> 13 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  7.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  3.  0.  0. 29.] 
adversary cards in discard: [14.  0.  0.  3.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14] -> size -> 13 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 25.908309936523438



action possibilites: [-1.] 
expected returns: [[8.803373]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 3. 0. 0.] 
cards in discard: [29.  0.  3.  0.  0.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29] -> size -> 13 
action values: 1 
buys: 0 
player value: 1 
card supply: [29. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  7.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  3.  0.  0. 29.] 
adversary cards in discard: [14.  0.  0.  3.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14] -> size -> 13 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 15.425554275512695





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
expected returns: [[  9.544687]
 [ 21.715061]
 [ 15.235085]
 [-44.311516]
 [ 15.749323]
 [ 21.114117]
 [ 12.623289]
 [ 27.80227 ]
 [ 16.741354]
 [ 18.2398  ]
 [ 24.005816]
 [ 11.150206]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 3. 0. 0.] 
cards in discard: [29.  0.  3.  0.  0.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29] -> size -> 13 
action values: 0 
buys: 1 
player value: 4 
card supply: [29. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  7.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  3.  0.  0. 29.] 
adversary cards in discard: [14.  0.  0.  3.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14] -> size -> 13 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 8.803373336791992



buy possibilites: [-1] 
expected returns: [[17.649149]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 3. 0. 0.] 
cards in discard: [29.  0.  3.  0.  0.  0. 29.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29] -> size -> 14 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  6.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  3.  0.  0. 29.] 
adversary cards in discard: [14.  0.  0.  3.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14] -> size -> 13 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  30   0   0  20   0   0   0   0   0   0   0 128   0] 
sum of rewards: 173 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 27.802270889282227






Player: 1 
cards in hand: [ 0.  3.  0.  0. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3.  0.  0. 29.] 
cards in discard: [14.  0.  0.  3.  0.  0.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14] -> size -> 13 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  6.  9. 10. 10. 10. 10.] 
adversary cards in hand: [29.  3. 29. 29.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29] -> size -> 14 
adversary victory points: 4
player victory points: 3 


action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 0. 0. 3.] 
cards in discard: [14.  0.  0.  3.  0.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14] -> size -> 13 
action values: 1 
buys: 0 
player value: 1 
card supply: [29. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  6.  9. 10. 10. 10. 10.] 
adversary cards in hand: [29.  3. 29. 29.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29] -> size -> 14 
adversary victory points: 4
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 3.] 
cards in discard: [14.  0.  0.  3.  0.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14] -> size -> 13 
action values: 0 
buys: 1 
player value: 4 
card supply: [29. 30. 30. 29. 30.  8. 10. 10. 10. 10. 10.  6.  9. 10. 10. 10. 10.] 
adversary cards in hand: [29.  3. 29. 29.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29] -> size -> 14 
adversary victory points: 4
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 3.] 
cards in discard: [14.  0.  0.  3.  0.  0.  3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3] -> size -> 14 
action values: 0 
buys: 0 
player value: 2 
card supply: [29. 30. 30. 28. 30.  8. 10. 10. 10. 10. 10.  6.  9. 10. 10. 10. 10.] 
adversary cards in hand: [29.  3. 29. 29.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29] -> size -> 14 
adversary victory points: 4
player victory points: 4 





         -------------------- Turn: 5 -------------------- 
Player: 0 
cards in hand: [29.  3. 29. 29.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29.] 
expected returns: [[27.377453]
 [38.96395 ]
 [38.96395 ]
 [38.96395 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  3. 29. 29.  3.] 
cards in discard: [] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29] -> size -> 14 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 30. 30. 28. 30.  8. 10. 10. 10. 10. 10.  6.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3] -> size -> 14 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 17.64914894104004



action possibilites: [-1. 29. 29.] 
expected returns: [[13.430689]
 [25.9433  ]
 [25.9433  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29. 29.  3.  0.] 
cards in discard: [] 
cards in deck: 8 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29] -> size -> 14 
action values: 1 
buys: 0 
player value: 1 
card supply: [29. 30. 30. 28. 30.  8. 10. 10. 10. 10. 10.  6.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3] -> size -> 14 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 34.68273162841797



action possibilites: [-1. 29.] 
expected returns: [[13.028116]
 [29.121584]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29.  3.  0.  0.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29] -> size -> 14 
action values: 1 
buys: 0 
player value: 2 
card supply: [29. 30. 30. 28. 30.  8. 10. 10. 10. 10. 10.  6.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3] -> size -> 14 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 25.943307876586914



action possibilites: [-1.] 
expected returns: [[3.2247686]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29] -> size -> 14 
action values: 1 
buys: 0 
player value: 3 
card supply: [29. 30. 30. 28. 30.  8. 10. 10. 10. 10. 10.  6.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3] -> size -> 14 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 29.121583938598633





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[  3.2499795]
 [ 15.403376 ]
 [-24.972813 ]
 [  8.87582  ]
 [-18.386917 ]
 [-51.175552 ]
 [  9.6175165]
 [ 14.754169 ]
 [  6.2617245]
 [ 17.832079 ]
 [ 21.273455 ]
 [ 10.442635 ]
 [ 16.163776 ]
 [ 12.001552 ]
 [ -4.781221 ]
 [ 17.573957 ]
 [  5.212125 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29] -> size -> 14 
action values: 0 
buys: 1 
player value: 6 
card supply: [29. 30. 30. 28. 30.  8. 10. 10. 10. 10. 10.  6.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3] -> size -> 14 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 3.22476863861084



buy possibilites: [-1] 
expected returns: [[21.272396]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 0. 0. 0.] 
cards in discard: [29.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29] -> size -> 15 
action values: 0 
buys: 0 
player value: 2 
card supply: [29. 30. 30. 28. 30.  8. 10. 10. 10. 10. 10.  5.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3] -> size -> 14 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5.  0.  0.  0.  0.  0. 60.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 87.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 21.273447036743164






Player: 1 
cards in hand: [0. 3. 0. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3] -> size -> 14 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 30. 30. 28. 30.  8. 10. 10. 10. 10. 10.  5.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 3. 0.] 
adversary cards in discard: [29. 29. 29. 29.  3.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29] -> size -> 15 
adversary victory points: 4
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3] -> size -> 14 
action values: 0 
buys: 1 
player value: 3 
card supply: [29. 30. 30. 28. 30.  8. 10. 10. 10. 10. 10.  5.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 3. 0.] 
adversary cards in discard: [29. 29. 29. 29.  3.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29] -> size -> 15 
adversary victory points: 4
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 3. 0.] 
cards in discard: [1.] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1] -> size -> 15 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 28. 30.  8. 10. 10. 10. 10. 10.  5.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0. 3. 0.] 
adversary cards in discard: [29. 29. 29. 29.  3.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29] -> size -> 15 
adversary victory points: 4
player victory points: 4 





         -------------------- Turn: 6 -------------------- 
Player: 0 
cards in hand: [0. 0. 0. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[16.718935]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 3. 0.] 
cards in discard: [29. 29. 29. 29.  3.  3.  0.  0.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29] -> size -> 15 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 28. 30.  8. 10. 10. 10. 10. 10.  5.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 0.] 
adversary cards in discard: [1. 0. 3. 0. 3. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1] -> size -> 15 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 21.272396087646484





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
expected returns: [[ 15.124748]
 [ 27.238535]
 [ 20.634005]
 [-40.248894]
 [ 21.259584]
 [ 26.324787]
 [ 17.946648]
 [ 33.18161 ]
 [ 22.11943 ]
 [ 23.608501]
 [ 29.294584]
 [ 16.908663]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3. 0.] 
cards in discard: [29. 29. 29. 29.  3.  3.  0.  0.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29] -> size -> 15 
action values: 0 
buys: 1 
player value: 4 
card supply: [29. 29. 30. 28. 30.  8. 10. 10. 10. 10. 10.  5.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 0.] 
adversary cards in discard: [1. 0. 3. 0. 3. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1] -> size -> 15 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 16.427064895629883



buy possibilites: [-1] 
expected returns: [[-4.7207117]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3. 0.] 
cards in discard: [29. 29. 29. 29.  3.  3.  0.  0.  0. 29.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29] -> size -> 16 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 28. 30.  8. 10. 10. 10. 10. 10.  4.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 0.] 
adversary cards in discard: [1. 0. 3. 0. 3. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1] -> size -> 15 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0   0   0   0   0   0   0   0   0   0   0   0 128   0] 
sum of rewards: 123 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 33.181610107421875






Player: 1 
cards in hand: [0. 0. 3. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3. 0. 0.] 
cards in discard: [1. 0. 3. 0. 3. 0.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1] -> size -> 15 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 28. 30.  8. 10. 10. 10. 10. 10.  4.  9. 10. 10. 10. 10.] 
adversary cards in hand: [29.  0.  3.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29] -> size -> 16 
adversary victory points: 4
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 0.] 
cards in discard: [1. 0. 3. 0. 3. 0.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1] -> size -> 15 
action values: 0 
buys: 1 
player value: 4 
card supply: [29. 29. 30. 28. 30.  8. 10. 10. 10. 10. 10.  4.  9. 10. 10. 10. 10.] 
adversary cards in hand: [29.  0.  3.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29] -> size -> 16 
adversary victory points: 4
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 0.] 
cards in discard: [1. 0. 3. 0. 3. 0. 0.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0] -> size -> 16 
action values: 0 
buys: 0 
player value: 4 
card supply: [28. 29. 30. 28. 30.  8. 10. 10. 10. 10. 10.  4.  9. 10. 10. 10. 10.] 
adversary cards in hand: [29.  0.  3.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29] -> size -> 16 
adversary victory points: 4
player victory points: 4 





         -------------------- Turn: 7 -------------------- 
Player: 0 
cards in hand: [29.  0.  3.  0.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[-9.743471]
 [ 4.940343]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0.  3.  0.  3.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29] -> size -> 16 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 29. 30. 28. 30.  8. 10. 10. 10. 10. 10.  4.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  0.  3. 14. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0] -> size -> 16 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: -4.720711708068848



action possibilites: [-1.] 
expected returns: [[3.9364247]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 10 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29] -> size -> 16 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 29. 30. 28. 30.  8. 10. 10. 10. 10. 10.  4.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  0.  3. 14. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0] -> size -> 16 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 1.4279890060424805





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
expected returns: [[  5.2116756]
 [ 17.059332 ]
 [ 10.60361  ]
 [-49.631348 ]
 [ 11.510253 ]
 [ 16.359684 ]
 [  8.224223 ]
 [ 22.652592 ]
 [ 12.18541  ]
 [ 13.760353 ]
 [ 19.081806 ]
 [  7.3490353]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 10 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29] -> size -> 16 
action values: 0 
buys: 1 
player value: 4 
card supply: [28. 29. 30. 28. 30.  8. 10. 10. 10. 10. 10.  4.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  0.  3. 14. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0] -> size -> 16 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 3.936424732208252



buy possibilites: [-1] 
expected returns: [[22.537132]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 3. 0.] 
cards in discard: [29.] 
cards in deck: 10 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29] -> size -> 17 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 29. 30. 28. 30.  8. 10. 10. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  0.  3. 14. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0] -> size -> 16 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0   0   0   0  20   0   0   0   0   0   0   0 128   0] 
sum of rewards: 143 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 22.652597427368164






Player: 1 
cards in hand: [ 0.  0.  3. 14. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 14. 29.] 
Chosen action: 14 : ['Militia' '14' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  3. 14. 29.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0] -> size -> 16 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 29. 30. 28. 30.  8. 10. 10. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 0.] 
adversary cards in discard: [29. 29.  0.  3.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29] -> size -> 17 
adversary victory points: 4
player victory points: 4 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  3. 29.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [14.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0] -> size -> 16 
action values: 0 
buys: 0 
player value: 2 
card supply: [28. 29. 30. 28. 30.  8. 10. 10. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0.] 
adversary cards in discard: [29. 29.  0.  3.  0.  3.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29] -> size -> 17 
adversary victory points: 4
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: 16.0 : ['Remodel' '16' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0.  3. 29.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [14.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0] -> size -> 16 
action values: 0 
buys: 1 
player value: 4 
card supply: [28. 29. 30. 28. 30.  8. 10. 10. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0.] 
adversary cards in discard: [29. 29.  0.  3.  0.  3.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29] -> size -> 17 
adversary victory points: 4
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0.  3. 29.] 
cards in discard: [16.] 
cards in deck: 11 
card top of deck: [] 
played cards: [14.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16] -> size -> 17 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 29. 30. 28. 30.  8. 10.  9. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 0.] 
adversary cards in discard: [29. 29.  0.  3.  0.  3.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29] -> size -> 17 
adversary victory points: 4
player victory points: 4 





         -------------------- Turn: 8 -------------------- 
Player: 0 
cards in hand: [0. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[28.07665]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0.] 
cards in discard: [29. 29.  0.  3.  0.  3.  0.  3.  0.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29] -> size -> 17 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 29. 30. 28. 30.  8. 10.  9. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 0. 3.] 
adversary cards in discard: [16. 14.  0.  0.  3. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16] -> size -> 17 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0   0   0   0   0   0   0   0   0   0   0   0 320   0] 
sum of rewards: 315 

action type: discard_down_to_3_cards - action 1
Learning step: 0
desired expected reward: 41.82758331298828





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[ 24.975588]
 [ 37.017746]
 [ 29.321806]
 [-51.523766]
 [ 35.128586]
 [ 28.161024]
 [ 33.64186 ]
 [ 28.19025 ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0.] 
cards in discard: [29. 29.  0.  3.  0.  3.  0.  3.  0.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29] -> size -> 17 
action values: 0 
buys: 1 
player value: 3 
card supply: [28. 29. 30. 28. 30.  8. 10.  9. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 0. 3.] 
adversary cards in discard: [16. 14.  0.  0.  3. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16] -> size -> 17 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 26.002735137939453



buy possibilites: [-1] 
expected returns: [[33.11792]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0.] 
cards in discard: [29. 29.  0.  3.  0.  3.  0.  3.  0.  1.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1] -> size -> 18 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 28. 30. 28. 30.  8. 10.  9. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 0. 3.] 
adversary cards in discard: [16. 14.  0.  0.  3. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16] -> size -> 17 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0 54  0] 
sum of rewards: 49 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 37.017738342285156






Player: 1 
cards in hand: [0. 3. 0. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 0. 0. 3.] 
cards in discard: [16. 14.  0.  0.  3. 29.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16] -> size -> 17 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 28. 30. 28. 30.  8. 10.  9. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [29.  3. 29. 29. 29.] 
adversary cards in discard: [29. 29.  0.  3.  0.  3.  0.  3.  0.  1.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1] -> size -> 18 
adversary victory points: 4
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 3.] 
cards in discard: [16. 14.  0.  0.  3. 29.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16] -> size -> 17 
action values: 0 
buys: 1 
player value: 3 
card supply: [28. 28. 30. 28. 30.  8. 10.  9. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [29.  3. 29. 29. 29.] 
adversary cards in discard: [29. 29.  0.  3.  0.  3.  0.  3.  0.  1.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1] -> size -> 18 
adversary victory points: 4
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 3.] 
cards in discard: [16. 14.  0.  0.  3. 29.  1.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1] -> size -> 18 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [29.  3. 29. 29. 29.] 
adversary cards in discard: [29. 29.  0.  3.  0.  3.  0.  3.  0.  1.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1] -> size -> 18 
adversary victory points: 4
player victory points: 4 





         -------------------- Turn: 9 -------------------- 
Player: 0 
cards in hand: [29.  3. 29. 29. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29. 29.] 
expected returns: [[36.26729 ]
 [51.233414]
 [51.233414]
 [51.233414]
 [51.233414]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  3. 29. 29. 29.] 
cards in discard: [29. 29.  0.  3.  0.  3.  0.  3.  0.  1.  0.  0.  0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1] -> size -> 18 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 1. 0. 0. 3.] 
adversary cards in discard: [16. 14.  0.  0.  3. 29.  1.  0.  3.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1] -> size -> 18 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 33.117919921875



action possibilites: [-1. 29. 29. 29.] 
expected returns: [[35.827415]
 [48.71943 ]
 [48.71943 ]
 [48.71943 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29. 29. 29.  0.] 
cards in discard: [] 
cards in deck: 12 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1] -> size -> 18 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 1. 0. 0. 3.] 
adversary cards in discard: [16. 14.  0.  0.  3. 29.  1.  0.  3.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1] -> size -> 18 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 50.01525115966797



action possibilites: [-1. 29. 29.] 
expected returns: [[-2.689298]
 [11.883235]
 [11.883235]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29. 29.  0.  0.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1] -> size -> 18 
action values: 1 
buys: 0 
player value: 2 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 1. 0. 0. 3.] 
adversary cards in discard: [16. 14.  0.  0.  3. 29.  1.  0.  3.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1] -> size -> 18 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 48.71943664550781



action possibilites: [-1. 29. 29.] 
expected returns: [[24.604897]
 [39.8574  ]
 [39.8574  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29.  0.  0. 29.] 
cards in discard: [] 
cards in deck: 10 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1] -> size -> 18 
action values: 1 
buys: 0 
player value: 3 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 1. 0. 0. 3.] 
adversary cards in discard: [16. 14.  0.  0.  3. 29.  1.  0.  3.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1] -> size -> 18 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 11.883237838745117



action possibilites: [-1. 29.] 
expected returns: [[15.192831]
 [31.090979]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0.  0. 29.  0.] 
cards in discard: [] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1] -> size -> 18 
action values: 1 
buys: 0 
player value: 4 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 1. 0. 0. 3.] 
adversary cards in discard: [16. 14.  0.  0.  3. 29.  1.  0.  3.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1] -> size -> 18 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 39.857398986816406



action possibilites: [-1.] 
expected returns: [[36.40732]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 8 
card top of deck: [] 
played cards: [29. 29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1] -> size -> 18 
action values: 1 
buys: 0 
player value: 5 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 1. 0. 0. 3.] 
adversary cards in discard: [16. 14.  0.  0.  3. 29.  1.  0.  3.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1] -> size -> 18 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0   0   0   0 100   0   0   0   0   0   0   0   0   0] 
sum of rewards: 95 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 31.090978622436523





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  5.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 36.09973 ]
 [ 48.17865 ]
 [  8.431771]
 [ 41.851776]
 [ 14.887928]
 [ 51.540268]
 [-17.307915]
 [ 42.39904 ]
 [ 47.795624]
 [ 39.232536]
 [ 50.815094]
 [ 54.176704]
 [ 43.38703 ]
 [ 49.04219 ]
 [ 44.90586 ]
 [ 28.364517]
 [ 50.551254]
 [ 37.778183]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 8 
card top of deck: [] 
played cards: [29. 29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1] -> size -> 18 
action values: 0 
buys: 1 
player value: 9 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10. 10. 10.  3.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 1. 0. 0. 3.] 
adversary cards in discard: [16. 14.  0.  0.  3. 29.  1.  0.  3.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1] -> size -> 18 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[ -5   0   0   0   0   0 100   0   0   0   0   0   0   0   0   0] 
sum of rewards: 95 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 36.407318115234375



buy possibilites: [-1] 
expected returns: [[38.84961]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 0.] 
cards in discard: [29.] 
cards in deck: 8 
card top of deck: [] 
played cards: [29. 29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29] -> size -> 19 
action values: 0 
buys: 0 
player value: 5 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10. 10. 10.  2.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 1. 0. 0. 3.] 
adversary cards in discard: [16. 14.  0.  0.  3. 29.  1.  0.  3.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1] -> size -> 18 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[ -5.   0.   0.   0.   0.   0. 100.   0.   0.   0.   0.   0.   0.   0.
  32.   0.] 
sum of rewards: 127.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 54.17671203613281






Player: 1 
cards in hand: [0. 1. 0. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 1. 0. 0. 3.] 
cards in discard: [16. 14.  0.  0.  3. 29.  1.  0.  3.  0.  0.  3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1] -> size -> 18 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10. 10. 10.  2.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 1.] 
adversary cards in discard: [29. 29. 29. 29. 29. 29.  3.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29] -> size -> 19 
adversary victory points: 4
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 0. 0. 3.] 
cards in discard: [16. 14.  0.  0.  3. 29.  1.  0.  3.  0.  0.  3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1] -> size -> 18 
action values: 0 
buys: 1 
player value: 5 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10. 10. 10.  2.  9. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 1.] 
adversary cards in discard: [29. 29. 29. 29. 29. 29.  3.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29] -> size -> 19 
adversary victory points: 4
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 0. 0. 3.] 
cards in discard: [16. 14.  0.  0.  3. 29.  1.  0.  3.  0.  0.  3. 15.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15] -> size -> 19 
action values: 0 
buys: 0 
player value: 1 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10. 10. 10.  2.  9. 10. 10. 10.  9.] 
adversary cards in hand: [0. 0. 3. 0. 1.] 
adversary cards in discard: [29. 29. 29. 29. 29. 29.  3.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29] -> size -> 19 
adversary victory points: 4
player victory points: 4 





         -------------------- Turn: 10 -------------------- 
Player: 0 
cards in hand: [0. 0. 3. 0. 1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[14.177921]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3. 0. 1.] 
cards in discard: [29. 29. 29. 29. 29. 29.  3.  0.  0.  0.  0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29] -> size -> 19 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10. 10. 10.  2.  9. 10. 10. 10.  9.] 
adversary cards in hand: [3. 0. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15] -> size -> 19 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 38.849609375





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 13.415485 ]
 [ 24.94351  ]
 [ 18.159365 ]
 [-14.385008 ]
 [-55.79454  ]
 [ 19.258314 ]
 [ 23.384863 ]
 [ 16.543545 ]
 [ 26.235435 ]
 [ 29.544527 ]
 [ 20.070608 ]
 [ 25.620668 ]
 [ 21.538134 ]
 [  2.6379018]
 [ 26.06889  ]
 [ 15.039902 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 1.] 
cards in discard: [29. 29. 29. 29. 29. 29.  3.  0.  0.  0.  0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29] -> size -> 19 
action values: 0 
buys: 1 
player value: 5 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10. 10. 10.  2.  9. 10. 10. 10.  9.] 
adversary cards in hand: [3. 0. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15] -> size -> 19 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 12.727991104125977



buy possibilites: [-1] 
expected returns: [[36.789963]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 1.] 
cards in discard: [29. 29. 29. 29. 29. 29.  3.  0.  0.  0.  0. 29.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29] -> size -> 20 
action values: 0 
buys: 0 
player value: 1 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10. 10. 10.  1.  9. 10. 10. 10.  9.] 
adversary cards in hand: [3. 0. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15] -> size -> 19 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 27.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 29.54453468322754






Player: 1 
cards in hand: [3. 0. 0. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15] -> size -> 19 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10. 10. 10.  1.  9. 10. 10. 10.  9.] 
adversary cards in hand: [29.  0. 29.  3.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29] -> size -> 20 
adversary victory points: 4
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15] -> size -> 19 
action values: 0 
buys: 1 
player value: 3 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10. 10. 10.  1.  9. 10. 10. 10.  9.] 
adversary cards in hand: [29.  0. 29.  3.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29] -> size -> 20 
adversary victory points: 4
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 3. 0.] 
cards in discard: [8.] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8] -> size -> 20 
action values: 0 
buys: 0 
player value: 1 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10.  9. 10.  1.  9. 10. 10. 10.  9.] 
adversary cards in hand: [29.  0. 29.  3.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29] -> size -> 20 
adversary victory points: 4
player victory points: 4 





         -------------------- Turn: 11 -------------------- 
Player: 0 
cards in hand: [29.  0. 29.  3.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[ 1.8117113]
 [14.466692 ]
 [14.466692 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0. 29.  3.  3.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29] -> size -> 20 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10.  9. 10.  1.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 0.  0. 14.  0.  0.] 
adversary cards in discard: [8. 3. 0. 0. 3. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8] -> size -> 20 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 36.78996276855469



action possibilites: [-1. 29.] 
expected returns: [[18.890818]
 [34.460938]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29.  3.  3.  3.] 
cards in discard: [] 
cards in deck: 14 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29] -> size -> 20 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10.  9. 10.  1.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 0.  0. 14.  0.  0.] 
adversary cards in discard: [8. 3. 0. 0. 3. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8] -> size -> 20 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 13.186307907104492



action possibilites: [-1.] 
expected returns: [[2.423925]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 3. 3. 3.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29] -> size -> 20 
action values: 1 
buys: 0 
player value: 2 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10.  9. 10.  1.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 0.  0. 14.  0.  0.] 
adversary cards in discard: [8. 3. 0. 0. 3. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8] -> size -> 20 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 34.46092987060547





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[  1.378253 ]
 [ 13.456089 ]
 [  7.0469627]
 [-50.202255 ]
 [ 12.897871 ]
 [  4.4380445]
 [ 10.038268 ]
 [  2.9849586]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 3. 3. 3.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29] -> size -> 20 
action values: 0 
buys: 1 
player value: 3 
card supply: [28. 27. 30. 28. 30.  8. 10.  9. 10.  9. 10.  1.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 0.  0. 14.  0.  0.] 
adversary cards in discard: [8. 3. 0. 0. 3. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8] -> size -> 20 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 2.4239249229431152



buy possibilites: [-1] 
expected returns: [[21.850805]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 3. 3. 3.] 
cards in discard: [1.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1] -> size -> 21 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 26. 30. 28. 30.  8. 10.  9. 10.  9. 10.  1.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 0.  0. 14.  0.  0.] 
adversary cards in discard: [8. 3. 0. 0. 3. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8] -> size -> 20 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0 54  0] 
sum of rewards: 89 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 13.456087112426758






Player: 1 
cards in hand: [ 0.  0. 14.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 14.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0. 14.  0.  0.] 
cards in discard: [8. 3. 0. 0. 3. 0.] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8] -> size -> 20 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 26. 30. 28. 30.  8. 10.  9. 10.  9. 10.  1.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 0.  0. 29. 29.  0.] 
adversary cards in discard: [ 1. 29. 29.  0.  3.  3.  3.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1] -> size -> 21 
adversary victory points: 4
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0. 14.  0.  0.] 
cards in discard: [8. 3. 0. 0. 3. 0.] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8] -> size -> 20 
action values: 0 
buys: 1 
player value: 4 
card supply: [28. 26. 30. 28. 30.  8. 10.  9. 10.  9. 10.  1.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 0.  0. 29. 29.  0.] 
adversary cards in discard: [ 1. 29. 29.  0.  3.  3.  3.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1] -> size -> 21 
adversary victory points: 4
player victory points: 4 


 --------- NOTHING HAPPENED --------- 



         -------------------- Turn: 12 -------------------- 
Player: 0 
cards in hand: [ 0.  0. 29. 29.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[-12.049383 ]
 [  2.2931843]
 [  2.2931843]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0. 29. 29.  0.] 
cards in discard: [ 1. 29. 29.  0.  3.  3.  3.  3.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1] -> size -> 21 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 26. 30. 28. 30.  8. 10.  9. 10.  9. 10.  1.  9. 10. 10. 10.  9.] 
adversary cards in hand: [16. 29.  3.  0.  1.] 
adversary cards in discard: [ 8.  3.  0.  0.  3.  0.  0.  0. 14.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8] -> size -> 20 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 21.850805282592773



action possibilites: [-1. 29. 29.] 
expected returns: [[-4.670062]
 [ 8.703693]
 [ 8.703693]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0. 29.  0. 29.] 
cards in discard: [ 1. 29. 29.  0.  3.  3.  3.  3.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1] -> size -> 21 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 26. 30. 28. 30.  8. 10.  9. 10.  9. 10.  1.  9. 10. 10. 10.  9.] 
adversary cards in hand: [16. 29.  3.  0.  1.] 
adversary cards in discard: [ 8.  3.  0.  0.  3.  0.  0.  0. 14.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8] -> size -> 20 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 1.929351806640625



action possibilites: [-1. 29.] 
expected returns: [[36.74716]
 [51.02957]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  0. 29.  1.] 
cards in discard: [ 1. 29. 29.  0.  3.  3.  3.  3.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1] -> size -> 21 
action values: 1 
buys: 0 
player value: 2 
card supply: [28. 26. 30. 28. 30.  8. 10.  9. 10.  9. 10.  1.  9. 10. 10. 10.  9.] 
adversary cards in hand: [16. 29.  3.  0.  1.] 
adversary cards in discard: [ 8.  3.  0.  0.  3.  0.  0.  0. 14.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8] -> size -> 20 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 8.703688621520996



action possibilites: [-1. 29.] 
expected returns: [[49.936195]
 [64.87977 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  0.  1. 29.] 
cards in discard: [ 1. 29. 29.  0.  3.  3.  3.  3.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1] -> size -> 21 
action values: 1 
buys: 0 
player value: 3 
card supply: [28. 26. 30. 28. 30.  8. 10.  9. 10.  9. 10.  1.  9. 10. 10. 10.  9.] 
adversary cards in hand: [16. 29.  3.  0.  1.] 
adversary cards in discard: [ 8.  3.  0.  0.  3.  0.  0.  0. 14.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8] -> size -> 20 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 51.029579162597656



action possibilites: [-1.] 
expected returns: [[47.741158]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 1. 0.] 
cards in discard: [ 1. 29. 29.  0.  3.  3.  3.  3.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1] -> size -> 21 
action values: 1 
buys: 0 
player value: 4 
card supply: [28. 26. 30. 28. 30.  8. 10.  9. 10.  9. 10.  1.  9. 10. 10. 10.  9.] 
adversary cards in hand: [16. 29.  3.  0.  1.] 
adversary cards in discard: [ 8.  3.  0.  0.  3.  0.  0.  0. 14.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8] -> size -> 20 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 64.87975311279297





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  5.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[47.679565]
 [58.39113 ]
 [22.191988]
 [52.579567]
 [28.290636]
 [61.419838]
 [-1.854275]
 [53.387726]
 [57.645676]
 [50.018967]
 [60.345543]
 [63.374245]
 [53.971664]
 [58.976624]
 [55.362434]
 [40.547546]
 [60.126488]
 [49.730774]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 1. 0.] 
cards in discard: [ 1. 29. 29.  0.  3.  3.  3.  3.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1] -> size -> 21 
action values: 0 
buys: 1 
player value: 10 
card supply: [28. 26. 30. 28. 30.  8. 10.  9. 10.  9. 10.  1.  9. 10. 10. 10.  9.] 
adversary cards in hand: [16. 29.  3.  0.  1.] 
adversary cards in discard: [ 8.  3.  0.  0.  3.  0.  0.  0. 14.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8] -> size -> 20 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5  0  0  0  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 47.74115753173828



buy possibilites: [-1] 
expected returns: [[95.27587]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 1. 0.] 
cards in discard: [ 1. 29. 29.  0.  3.  3.  3.  3. 29.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29] -> size -> 22 
action values: 0 
buys: 0 
player value: 6 
card supply: [28. 26. 30. 28. 30.  8. 10.  9. 10.  9. 10.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [16. 29.  3.  0.  1.] 
adversary cards in discard: [ 8.  3.  0.  0.  3.  0.  0.  0. 14.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8] -> size -> 20 
adversary victory points: 4
player victory points: 4 

Reward from previous game state: 
[-5.  0.  0.  0.  0.  0. 80.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 107.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 63.374244689941406






Player: 1 
cards in hand: [16. 29.  3.  0.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16. 29.] 
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [16. 29.  3.  0.  1.] 
cards in discard: [ 8.  3.  0.  0.  3.  0.  0.  0. 14.  0.  0.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8] -> size -> 20 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 26. 30. 28. 30.  8. 10.  9. 10.  9. 10.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [29. 29. 29.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29] -> size -> 22 
adversary victory points: 4
player victory points: 4 


action possibilites: [-1. 16.] 
Chosen action: 16 : ['Remodel' '16' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [16.  0.  1.  3.] 
cards in discard: [ 8.  3.  0.  0.  3.  0.  0.  0. 14.  0.  0.  3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8] -> size -> 20 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 26. 30. 28. 30.  8. 10.  9. 10.  9. 10.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [29. 29. 29.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29] -> size -> 22 
adversary victory points: 4
player victory points: 4 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 3.] 
cards in discard: [ 8.  3.  0.  0.  3.  0.  0.  0. 14.  0.  0.  3.  6.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 16.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8  6] -> size -> 20 
action values: 0 
buys: 0 
player value: 1 
card supply: [28. 26. 30. 28. 30.  8.  9.  9. 10.  9. 10.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [29. 29. 29.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29] -> size -> 22 
adversary victory points: 4
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 11.0 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 3.] 
cards in discard: [ 8.  3.  0.  0.  3.  0.  0.  0. 14.  0.  0.  3.  6.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 16.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8  6] -> size -> 20 
action values: 0 
buys: 1 
player value: 3 
card supply: [28. 26. 30. 28. 30.  8.  9.  9. 10.  9. 10.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [29. 29. 29.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29] -> size -> 22 
adversary victory points: 4
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 3.] 
cards in discard: [ 8.  3.  0.  0.  3.  0.  0.  0. 14.  0.  0.  3.  6. 11.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 16.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8  6 11] -> size -> 21 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 26. 30. 28. 30.  8.  9.  9.  9.  9. 10.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [29. 29. 29.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29] -> size -> 22 
adversary victory points: 4
player victory points: 3 





         -------------------- Turn: 13 -------------------- 
Player: 0 
cards in hand: [29. 29. 29.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29.] 
expected returns: [[-0.57527876]
 [11.843878  ]
 [11.843878  ]
 [11.843878  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29. 29.  0.  0.] 
cards in discard: [] 
cards in deck: 17 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 26. 30. 28. 30.  8.  9.  9.  9.  9. 10.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 1.  0. 15.  1.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8  6 11] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 95.27587127685547



action possibilites: [-1. 29.] 
expected returns: [[-10.11676  ]
 [  2.3263578]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0.  0.  0.] 
cards in discard: [29.] 
cards in deck: 16 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 26. 30. 28. 30.  8.  9.  9.  9.  9. 10.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 1.  0. 15.  1.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8  6 11] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 4.564831733703613



action possibilites: [-1.] 
expected returns: [[16.972397]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0.] 
cards in discard: [29.  0.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 2 
card supply: [28. 26. 30. 28. 30.  8.  9.  9.  9.  9. 10.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 1.  0. 15.  1.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8  6 11] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: discard_n_cards - action 0
Learning step: 0
desired expected reward: -13.983427047729492





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 15.872156 ]
 [ 27.781425 ]
 [ 21.482359 ]
 [ -6.2182693]
 [-39.063698 ]
 [ 22.428444 ]
 [ 27.232456 ]
 [ 18.508432 ]
 [ 30.079355 ]
 [ 23.064322 ]
 [ 28.498007 ]
 [ 24.636547 ]
 [  7.899111 ]
 [ 29.83763  ]
 [ 18.348387 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0.] 
cards in discard: [29.  0.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29] -> size -> 22 
action values: 0 
buys: 1 
player value: 5 
card supply: [28. 26. 30. 28. 30.  8.  9.  9.  9.  9. 10.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 1.  0. 15.  1.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8  6 11] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 16.972396850585938



buy possibilites: [-1] 
expected returns: [[29.629885]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0.] 
cards in discard: [29.  0. 25.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25] -> size -> 23 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 26. 30. 28. 30.  8.  9.  9.  9.  9.  9.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 1.  0. 15.  1.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8  6 11] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  30   0   0  40   0   0   0   0   0   0   0 250   0] 
sum of rewards: 315 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 30.079370498657227






Player: 1 
cards in hand: [ 1.  0. 15.  1.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  0. 15.  1.  0.] 
cards in discard: [] 
cards in deck: 16 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8  6 11] -> size -> 21 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 26. 30. 28. 30.  8.  9.  9.  9.  9.  9.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 3.  3.  0. 29.  0.] 
adversary cards in discard: [29.  0. 25. 29. 29.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25] -> size -> 23 
adversary victory points: 4
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  0. 15.  1.  0.] 
cards in discard: [] 
cards in deck: 16 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8  6 11] -> size -> 21 
action values: 0 
buys: 1 
player value: 6 
card supply: [28. 26. 30. 28. 30.  8.  9.  9.  9.  9.  9.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 3.  3.  0. 29.  0.] 
adversary cards in discard: [29.  0. 25. 29. 29.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25] -> size -> 23 
adversary victory points: 4
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  0. 15.  1.  0.] 
cards in discard: [25.] 
cards in deck: 16 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8  6 11 25] -> size -> 22 
action values: 0 
buys: 0 
player value: 1 
card supply: [28. 26. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 3.  3.  0. 29.  0.] 
adversary cards in discard: [29.  0. 25. 29. 29.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25] -> size -> 23 
adversary victory points: 4
player victory points: 3 





         -------------------- Turn: 14 -------------------- 
Player: 0 
cards in hand: [ 3.  3.  0. 29.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[23.253397]
 [37.243217]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  3.  0. 29.  0.] 
cards in discard: [29.  0. 25. 29. 29.  0.  0.  0.] 
cards in deck: 10 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25] -> size -> 23 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 26. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 3. 11.  8.  0.  0.] 
adversary cards in discard: [25.  1.  0. 15.  1.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8  6 11 25] -> size -> 22 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 29.629884719848633



action possibilites: [-1.] 
expected returns: [[45.852814]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 3.] 
cards in discard: [29.  0. 25. 29. 29.  0.  0.  0.  3.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25] -> size -> 23 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 26. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 3. 11.  8.  0.  0.] 
adversary cards in discard: [25.  1.  0. 15.  1.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8  6 11 25] -> size -> 22 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 31.12322425842285





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[45.865425 ]
 [57.29239  ]
 [51.196884 ]
 [-2.9976654]
 [56.61119  ]
 [48.390343 ]
 [54.0483   ]
 [47.726868 ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 3.] 
cards in discard: [29.  0. 25. 29. 29.  0.  0.  0.  3.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25] -> size -> 23 
action values: 0 
buys: 1 
player value: 3 
card supply: [28. 26. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 3. 11.  8.  0.  0.] 
adversary cards in discard: [25.  1.  0. 15.  1.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8  6 11 25] -> size -> 22 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 45.852813720703125



buy possibilites: [-1] 
expected returns: [[67.0927]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 3.] 
cards in discard: [29.  0. 25. 29. 29.  0.  0.  0.  3.  1.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1] -> size -> 24 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 25. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 3. 11.  8.  0.  0.] 
adversary cards in discard: [25.  1.  0. 15.  1.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8  6 11 25] -> size -> 22 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0 54  0] 
sum of rewards: 99 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 57.29236602783203






Player: 1 
cards in hand: [ 3. 11.  8.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 11.  8.  0.  0.] 
cards in discard: [25.  1.  0. 15.  1.  0.] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  0 14  3  1  0 16  1 15  8  6 11 25] -> size -> 22 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 25. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [29. 29. 29. 29.  1.] 
adversary cards in discard: [29.  0. 25. 29. 29.  0.  0.  0.  3.  1. 29.  3.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1] -> size -> 24 
adversary victory points: 4
player victory points: 3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [11.  0.] 
cards in discard: [25.  1.  0. 15.  1.  0.] 
cards in deck: 11 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  0  0  0  3  3 29  0 14  3  1  0 16  1 15  8  6 11 25] -> size -> 20 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 25. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [29. 29. 29. 29.  1.] 
adversary cards in discard: [29.  0. 25. 29. 29.  0.  0.  0.  3.  1. 29.  3.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1] -> size -> 24 
adversary victory points: 4
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [11.  0.] 
cards in discard: [25.  1.  0. 15.  1.  0.] 
cards in deck: 11 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  0  0  0  3  3 29  0 14  3  1  0 16  1 15  8  6 11 25] -> size -> 20 
action values: 0 
buys: 1 
player value: 1 
card supply: [28. 25. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [29. 29. 29. 29.  1.] 
adversary cards in discard: [29.  0. 25. 29. 29.  0.  0.  0.  3.  1. 29.  3.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1] -> size -> 24 
adversary victory points: 4
player victory points: 2 





         -------------------- Turn: 15 -------------------- 
Player: 0 
cards in hand: [29. 29. 29. 29.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29. 29.] 
expected returns: [[81.07569]
 [92.79489]
 [92.79489]
 [92.79489]
 [92.79489]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29. 29. 29.  1.] 
cards in discard: [29.  0. 25. 29. 29.  0.  0.  0.  3.  1. 29.  3.  0.  0.  3.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1] -> size -> 24 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 25. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 0. 16. 29.  0.  3.] 
adversary cards in discard: [25.  1.  0. 15.  1.  0.  8. 11.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3 29  0 14  3  1  0 16  1 15  8  6 11 25] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: 67.09269714355469



action possibilites: [-1. 29. 29. 29.] 
expected returns: [[-15.776775 ]
 [ -4.2589965]
 [ -4.2589965]
 [ -4.2589965]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29. 29.  1.] 
cards in discard: [29.  0. 25. 29. 29.  0.  0.  0.  3.  1. 29.  3.  0.  0.  3.  1.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1] -> size -> 24 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 25. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 0. 16. 29.  0.  3.] 
adversary cards in discard: [25.  1.  0. 15.  1.  0.  8. 11.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3 29  0 14  3  1  0 16  1 15  8  6 11 25] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 88.51436614990234



action possibilites: [-1. 29. 29.] 
expected returns: [[ 6.9872  ]
 [21.866098]
 [21.866098]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29.  3.] 
cards in discard: [29.  0. 25. 29. 29.  0.  0.  0.  3.  1. 29.  3.  0.  0.  3.  1.  1.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1] -> size -> 24 
action values: 1 
buys: 0 
player value: 2 
card supply: [28. 25. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 0. 16. 29.  0.  3.] 
adversary cards in discard: [25.  1.  0. 15.  1.  0.  8. 11.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3 29  0 14  3  1  0 16  1 15  8  6 11 25] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 95 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -8.456350326538086



action possibilites: [-1. 29. 29.] 
expected returns: [[55.562065]
 [69.65465 ]
 [69.65465 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29.] 
cards in discard: [29.  0. 25. 29. 29.  0.  0.  0.  3.  1. 29.  3.  0.  0.  3.  1.  1.  3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1] -> size -> 24 
action values: 1 
buys: 0 
player value: 3 
card supply: [28. 25. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 0. 16. 29.  0.  3.] 
adversary cards in discard: [25.  1.  0. 15.  1.  0.  8. 11.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3 29  0 14  3  1  0 16  1 15  8  6 11 25] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 115 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 16.423799514770508



action possibilites: [-1.] 
expected returns: [[7.3538055]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0.] 
cards in discard: [29.  0. 25. 29. 29.  0.  0.  0.  3.  1. 29.  3.  0.  0.  3.  1.  1.  3.
 29.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1] -> size -> 24 
action values: 1 
buys: 0 
player value: 4 
card supply: [28. 25. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 0. 16. 29.  0.  3.] 
adversary cards in discard: [25.  1.  0. 15.  1.  0.  8. 11.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3 29  0 14  3  1  0 16  1 15  8  6 11 25] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 135 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 64.28697204589844





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[  1.1209002]
 [ 12.052733 ]
 [  4.4838257]
 [-33.36812  ]
 [-79.653534 ]
 [  7.8673162]
 [  9.121837 ]
 [  3.582996 ]
 [ 11.075516 ]
 [  6.845104 ]
 [ 11.368758 ]
 [  8.851838 ]
 [-12.655308 ]
 [ 11.017068 ]
 [  6.0941467]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [29.  0. 25. 29. 29.  0.  0.  0.  3.  1. 29.  3.  0.  0.  3.  1.  1.  3.
 29.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1] -> size -> 24 
action values: 0 
buys: 1 
player value: 5 
card supply: [28. 25. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 0. 16. 29.  0.  3.] 
adversary cards in discard: [25.  1.  0. 15.  1.  0.  8. 11.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3 29  0 14  3  1  0 16  1 15  8  6 11 25] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 135 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 7.3538055419921875



buy possibilites: [-1] 
expected returns: [[15.4858265]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [29.  0. 25. 29. 29.  0.  0.  0.  3.  1. 29.  3.  0.  0.  3.  1.  1.  3.
 29.  1.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1] -> size -> 25 
action values: 0 
buys: 0 
player value: 2 
card supply: [28. 24. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [ 0. 16. 29.  0.  3.] 
adversary cards in discard: [25.  1.  0. 15.  1.  0.  8. 11.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3 29  0 14  3  1  0 16  1 15  8  6 11 25] -> size -> 20 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5.   0.   0.  60.   0.   0.  80.   0.   0.   0.   0.   0.   0.   0.
 13.5  0. ] 
sum of rewards: 148.5 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 12.052728652954102






Player: 1 
cards in hand: [ 0. 16. 29.  0.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16. 29.] 
Chosen action: 16 : ['Remodel' '16' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 16. 29.  0.  3.] 
cards in discard: [25.  1.  0. 15.  1.  0.  8. 11.  0.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3 29  0 14  3  1  0 16  1 15  8  6 11 25] -> size -> 20 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 24. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  9.] 
adversary cards in hand: [3. 3. 0. 0. 1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1] -> size -> 25 
adversary victory points: 4
player victory points: 2 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3.] 
cards in discard: [25.  1.  0. 15.  1.  0.  8. 11.  0. 15.] 
cards in deck: 6 
card top of deck: [] 
played cards: [16.] 
owned cards: [ 0  0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15] -> size -> 20 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 24. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  8.] 
adversary cards in hand: [3. 3. 0. 0. 1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1] -> size -> 25 
adversary victory points: 4
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3.] 
cards in discard: [25.  1.  0. 15.  1.  0.  8. 11.  0. 15.] 
cards in deck: 6 
card top of deck: [] 
played cards: [16.] 
owned cards: [ 0  0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15] -> size -> 20 
action values: 0 
buys: 1 
player value: 2 
card supply: [28. 24. 30. 28. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  8.] 
adversary cards in hand: [3. 3. 0. 0. 1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1] -> size -> 25 
adversary victory points: 4
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3.] 
cards in discard: [25.  1.  0. 15.  1.  0.  8. 11.  0. 15.  3.] 
cards in deck: 6 
card top of deck: [] 
played cards: [16.] 
owned cards: [ 0  0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3] -> size -> 21 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 24. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  8.] 
adversary cards in hand: [3. 3. 0. 0. 1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1] -> size -> 25 
adversary victory points: 4
player victory points: 3 





         -------------------- Turn: 16 -------------------- 
Player: 0 
cards in hand: [3. 3. 0. 0. 1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[8.872728]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 3. 0. 0. 1.] 
cards in discard: [] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1] -> size -> 25 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 24. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  8.] 
adversary cards in hand: [ 3. 14.  0.  0.  3.] 
adversary cards in discard: [25.  1.  0. 15.  1.  0.  8. 11.  0. 15.  3. 16.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 15.48582649230957





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[  2.3721852]
 [ 12.837843 ]
 [  7.319456 ]
 [-47.391663 ]
 [  8.4011   ]
 [ 12.164894 ]
 [  3.9041185]
 [  8.735633 ]
 [ 10.146542 ]
 [ 14.399265 ]
 [  5.168212 ]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 0. 0. 1.] 
cards in discard: [] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1] -> size -> 25 
action values: 0 
buys: 1 
player value: 4 
card supply: [28. 24. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  8.] 
adversary cards in hand: [ 3. 14.  0.  0.  3.] 
adversary cards in discard: [25.  1.  0. 15.  1.  0.  8. 11.  0. 15.  3. 16.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 6.825679779052734



buy possibilites: [-1] 
expected returns: [[16.266943]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 0. 0. 1.] 
cards in discard: [15.] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15] -> size -> 26 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 24. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  7.] 
adversary cards in hand: [ 3. 14.  0.  0.  3.] 
adversary cards in discard: [25.  1.  0. 15.  1.  0.  8. 11.  0. 15.  3. 16.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  30   0   0   0   0   0   0   0   0   0   0 128   0] 
sum of rewards: 153 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 14.399251937866211






Player: 1 
cards in hand: [ 3. 14.  0.  0.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 14.] 
Chosen action: 14 : ['Militia' '14' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 14.  0.  0.  3.] 
cards in discard: [25.  1.  0. 15.  1.  0.  8. 11.  0. 15.  3. 16.  0.  0.  3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3] -> size -> 21 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 24. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  7.] 
adversary cards in hand: [ 0.  3. 29.  0. 29.] 
adversary cards in discard: [15.  3.  3.  0.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15] -> size -> 26 
adversary victory points: 4
player victory points: 3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 3.] 
cards in discard: [25.  1.  0. 15.  1.  0.  8. 11.  0. 15.  3. 16.  0.  0.  3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [14.] 
owned cards: [ 0  0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3] -> size -> 21 
action values: 0 
buys: 0 
player value: 2 
card supply: [28. 24. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  7.] 
adversary cards in hand: [29.  0. 29.] 
adversary cards in discard: [15.  3.  3.  0.  0.  1.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15] -> size -> 26 
adversary victory points: 4
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 3.] 
cards in discard: [25.  1.  0. 15.  1.  0.  8. 11.  0. 15.  3. 16.  0.  0.  3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [14.] 
owned cards: [ 0  0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3] -> size -> 21 
action values: 0 
buys: 1 
player value: 4 
card supply: [28. 24. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  7.] 
adversary cards in hand: [29.  0. 29.] 
adversary cards in discard: [15.  3.  3.  0.  0.  1.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15] -> size -> 26 
adversary victory points: 4
player victory points: 3 





         -------------------- Turn: 17 -------------------- 
Player: 0 
cards in hand: [29.  0. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[ 2.003336]
 [15.145117]
 [15.145117]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0. 29.] 
cards in discard: [15.  3.  3.  0.  0.  1.  3.  0.] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15] -> size -> 26 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 24. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  7.] 
adversary cards in hand: [ 0.  1.  0. 15.  6.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  30   0   0   0   0   0   0   0   0   0   0 630   0] 
sum of rewards: 655 

action type: discard_down_to_3_cards - action 5
Learning step: 0
desired expected reward: 32.24754333496094



action possibilites: [-1. 29.] 
expected returns: [[10.869875]
 [22.733664]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29.] 
cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.] 
cards in deck: 14 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15] -> size -> 26 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 24. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  7.] 
adversary cards in hand: [ 0.  1.  0. 15.  6.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 10.241010665893555



action possibilites: [-1.] 
expected returns: [[32.378227]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0.] 
cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.  1.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15] -> size -> 26 
action values: 1 
buys: 0 
player value: 2 
card supply: [28. 24. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  7.] 
adversary cards in hand: [ 0.  1.  0. 15.  6.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 18.861860275268555





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[ 30.630304]
 [ 41.76156 ]
 [ 35.835556]
 [-22.710587]
 [ 41.064034]
 [ 33.11608 ]
 [ 38.488228]
 [ 32.145653]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.  1.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15] -> size -> 26 
action values: 0 
buys: 1 
player value: 3 
card supply: [28. 24. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  7.] 
adversary cards in hand: [ 0.  1.  0. 15.  6.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 32.37822723388672



buy possibilites: [-1] 
expected returns: [[7.310669]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.  1.  1.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15  1] -> size -> 27 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 23. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  7.] 
adversary cards in hand: [ 0.  1.  0. 15.  6.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0 54  0] 
sum of rewards: 119 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 41.761566162109375






Player: 1 
cards in hand: [ 0.  1.  0. 15.  6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15.] 
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  1.  0. 15.  6.] 
cards in discard: [] 
cards in deck: 16 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3] -> size -> 21 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 23. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  7.] 
adversary cards in hand: [29. 29. 29.  1.  3.] 
adversary cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.  1.  1. 29. 29.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15  1] -> size -> 27 
adversary victory points: 4
player victory points: 3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 0. 6.] 
cards in discard: [] 
cards in deck: 16 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3] -> size -> 20 
action values: 0 
buys: 0 
player value: 3 
card supply: [28. 23. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  7.] 
adversary cards in hand: [29. 29. 29.  1.  3.] 
adversary cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.  1.  1. 29. 29.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15  1] -> size -> 27 
adversary victory points: 4
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
Chosen buy action: 22.0 : ['Library' '22' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 0. 6.] 
cards in discard: [] 
cards in deck: 16 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3] -> size -> 20 
action values: 0 
buys: 1 
player value: 6 
card supply: [28. 23. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10. 10.  7.] 
adversary cards in hand: [29. 29. 29.  1.  3.] 
adversary cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.  1.  1. 29. 29.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15  1] -> size -> 27 
adversary victory points: 4
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 0. 6.] 
cards in discard: [22.] 
cards in deck: 16 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22] -> size -> 21 
action values: 0 
buys: 0 
player value: 1 
card supply: [28. 23. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [29. 29. 29.  1.  3.] 
adversary cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.  1.  1. 29. 29.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15  1] -> size -> 27 
adversary victory points: 4
player victory points: 3 





         -------------------- Turn: 18 -------------------- 
Player: 0 
cards in hand: [29. 29. 29.  1.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29.] 
expected returns: [[25.078215]
 [38.071823]
 [38.071823]
 [38.071823]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29. 29.  1.  3.] 
cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.  1.  1. 29. 29.  0.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15  1] -> size -> 27 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 23. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [0. 8. 3. 0. 3.] 
adversary cards in discard: [22. 15.  1.  0.  6.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 7.3106689453125



action possibilites: [-1. 29.] 
expected returns: [[ 99.112724]
 [113.43345 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  1.  3.  0.] 
cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.  1.  1. 29. 29.  0. 29.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15  1] -> size -> 27 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 23. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [0. 8. 3. 0. 3.] 
adversary cards in discard: [22. 15.  1.  0.  6.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 33.32172393798828



action possibilites: [-1. 29.] 
expected returns: [[36.91095]
 [50.24375]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 29.] 
cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.  1.  1. 29. 29.  0. 29.  1.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15  1] -> size -> 27 
action values: 1 
buys: 0 
player value: 2 
card supply: [28. 23. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [0. 8. 3. 0. 3.] 
adversary cards in discard: [22. 15.  1.  0.  6.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 108.20052337646484



action possibilites: [-1.] 
expected returns: [[-5.4331675]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0.] 
cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.  1.  1. 29. 29.  0. 29.  1.  3.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15  1] -> size -> 27 
action values: 1 
buys: 0 
player value: 3 
card supply: [28. 23. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [0. 8. 3. 0. 3.] 
adversary cards in discard: [22. 15.  1.  0.  6.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 45.34423065185547





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ -9.370231 ]
 [  1.5683885]
 [ -4.351505 ]
 [-29.490725 ]
 [-64.736824 ]
 [ -3.5681877]
 [  0.9839301]
 [ -6.674021 ]
 [  3.7514381]
 [ -2.9106367]
 [  2.2517653]
 [ -1.4747164]
 [-16.795815 ]
 [  3.5193825]
 [ -7.431169 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0.] 
cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.  1.  1. 29. 29.  0. 29.  1.  3.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15  1] -> size -> 27 
action values: 0 
buys: 1 
player value: 5 
card supply: [28. 23. 30. 27. 30.  8.  9.  9.  9.  9.  8.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [0. 8. 3. 0. 3.] 
adversary cards in discard: [22. 15.  1.  0.  6.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -5.433167457580566



buy possibilites: [-1] 
expected returns: [[-23.375856]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0.] 
cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.  1.  1. 29. 29.  0. 29.  1.  3. 25.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15  1 25] -> size -> 28 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 23. 30. 27. 30.  8.  9.  9.  9.  9.  7.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [0. 8. 3. 0. 3.] 
adversary cards in discard: [22. 15.  1.  0.  6.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22] -> size -> 21 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  30   0   0  60   0   0   0   0   0   0   0 250   0] 
sum of rewards: 335 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 3.7514467239379883






Player: 1 
cards in hand: [0. 8. 3. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 8. 3. 0. 3.] 
cards in discard: [22. 15.  1.  0.  6.] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22] -> size -> 21 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 23. 30. 27. 30.  8.  9.  9.  9.  9.  7.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [29.  0. 25.  1. 29.] 
adversary cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.  1.  1. 29. 29.  0. 29.  1.  3. 25.
 29. 29. 29.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15  1 25] -> size -> 28 
adversary victory points: 4
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 8. 3. 0. 3.] 
cards in discard: [22. 15.  1.  0.  6.] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22] -> size -> 21 
action values: 0 
buys: 1 
player value: 2 
card supply: [28. 23. 30. 27. 30.  8.  9.  9.  9.  9.  7.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [29.  0. 25.  1. 29.] 
adversary cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.  1.  1. 29. 29.  0. 29.  1.  3. 25.
 29. 29. 29.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15  1 25] -> size -> 28 
adversary victory points: 4
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 8. 3. 0. 3.] 
cards in discard: [22. 15.  1.  0.  6.  0.] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0] -> size -> 22 
action values: 0 
buys: 0 
player value: 2 
card supply: [27. 23. 30. 27. 30.  8.  9.  9.  9.  9.  7.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [29.  0. 25.  1. 29.] 
adversary cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.  1.  1. 29. 29.  0. 29.  1.  3. 25.
 29. 29. 29.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15  1 25] -> size -> 28 
adversary victory points: 4
player victory points: 3 





         -------------------- Turn: 19 -------------------- 
Player: 0 
cards in hand: [29.  0. 25.  1. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 25. 29.] 
expected returns: [[-4.380435 ]
 [ 7.2231045]
 [ 4.52221  ]
 [ 7.2231045]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0. 25.  1. 29.] 
cards in discard: [15.  3.  3.  0.  0.  1.  3.  0. 29.  1.  1. 29. 29.  0. 29.  1.  3. 25.
 29. 29. 29.  0.  0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15  1 25] -> size -> 28 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 30.  8.  9.  9.  9.  9.  7.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [0. 1. 3. 0. 0.] 
adversary cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0] -> size -> 22 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: -23.375856399536133



action possibilites: [-1. 29.] 
expected returns: [[-11.247567]
 [  2.603045]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  1. 29.  3.] 
cards in discard: [25.] 
cards in deck: 22 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15  1 25] -> size -> 28 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 23. 30. 27. 30.  8.  9.  9.  9.  9.  7.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [0. 1. 3. 0. 0.] 
adversary cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0] -> size -> 22 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 2.988950729370117



action possibilites: [-1. 15.] 
expected returns: [[-1.6900028]
 [ 9.320856 ]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3. 15.] 
cards in discard: [25.  1.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1
  1 15  1 25] -> size -> 28 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 23. 30. 27. 30.  8.  9.  9.  9.  9.  7.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [0. 1. 3. 0. 0.] 
adversary cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0] -> size -> 22 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -2.4551467895507812



action possibilites: [-1] 
expected returns: [[-15.754999]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3.] 
cards in discard: [25.  1.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25] -> size -> 27 
action values: 0 
buys: 0 
player value: 5 
card supply: [27. 23. 30. 27. 30.  8.  9.  9.  9.  9.  7.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [0. 1. 3. 0. 0.] 
adversary cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0] -> size -> 22 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 9.320856094360352





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[-18.331902 ]
 [ -7.227245 ]
 [-13.367967 ]
 [-38.223595 ]
 [-67.86422  ]
 [-12.50719  ]
 [ -8.185942 ]
 [-15.911547 ]
 [ -5.3011403]
 [-11.939057 ]
 [ -6.6767626]
 [-10.504564 ]
 [-25.751963 ]
 [ -5.528411 ]
 [-16.223534 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3.] 
cards in discard: [25.  1.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25] -> size -> 27 
action values: 0 
buys: 1 
player value: 5 
card supply: [27. 23. 30. 27. 30.  8.  9.  9.  9.  9.  7.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [0. 1. 3. 0. 0.] 
adversary cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0] -> size -> 22 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[-5  0  0 30  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: take_action - action -1
Learning step: 0
desired expected reward: -15.754999160766602



buy possibilites: [-1] 
expected returns: [[-7.0060005]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3.] 
cards in discard: [25.  1. 25.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25] -> size -> 28 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 30.  8.  9.  9.  9.  9.  6.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [0. 1. 3. 0. 0.] 
adversary cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0] -> size -> 22 
adversary victory points: 3
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  30   0   0  60   0   0   0   0   0   0   0 250   0] 
sum of rewards: 335 

action type: buy - action 25.0
Learning step: 0
desired expected reward: -5.301144599914551






Player: 1 
cards in hand: [0. 1. 3. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 1. 3. 0. 0.] 
cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0] -> size -> 22 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 30.  8.  9.  9.  9.  9.  6.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [ 0.  1. 29. 25. 29.] 
adversary cards in discard: [25.  1. 25. 29. 29. 15.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25] -> size -> 28 
adversary victory points: 4
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
Chosen buy action: 4.0 : ['Duchy' '4' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 3. 0. 0.] 
cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0] -> size -> 22 
action values: 0 
buys: 1 
player value: 5 
card supply: [27. 23. 30. 27. 30.  8.  9.  9.  9.  9.  6.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [ 0.  1. 29. 25. 29.] 
adversary cards in discard: [25.  1. 25. 29. 29. 15.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25] -> size -> 28 
adversary victory points: 4
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 3. 0. 0.] 
cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.  4.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4] -> size -> 23 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 29.  8.  9.  9.  9.  9.  6.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [ 0.  1. 29. 25. 29.] 
adversary cards in discard: [25.  1. 25. 29. 29. 15.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25] -> size -> 28 
adversary victory points: 4
player victory points: 6 





         -------------------- Turn: 20 -------------------- 
Player: 0 
cards in hand: [ 0.  1. 29. 25. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 25. 29.] 
expected returns: [[2.777258 ]
 [3.9965134]
 [2.073142 ]
 [3.9965134]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  1. 29. 25. 29.] 
cards in discard: [25.  1. 25. 29. 29. 15.  3.] 
cards in deck: 16 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25] -> size -> 28 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 29.  8.  9.  9.  9.  9.  6.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [14. 11. 15. 25. 16.] 
adversary cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.  4.  0.  1.  3.  0.  0.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4] -> size -> 23 
adversary victory points: 6
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -60   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -65 

action type: buy - action -1
Learning step: 0
desired expected reward: -7.006000518798828



action possibilites: [-1. 25. 29. 29.] 
expected returns: [[ 2.438655]
 [12.686184]
 [15.664061]
 [15.664061]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 25. 29. 29.] 
cards in discard: [25.  1. 25. 29. 29. 15.  3.  1.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25] -> size -> 28 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 23. 30. 27. 29.  8.  9.  9.  9.  9.  6.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [14. 11. 15. 25. 16.] 
adversary cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.  4.  0.  1.  3.  0.  0.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4] -> size -> 23 
adversary victory points: 6
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -60   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 3.1676201820373535



action possibilites: [-1. 25.] 
expected returns: [[ 7.305376]
 [19.09125 ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 25.  3.] 
cards in discard: [25.  1. 25. 29. 29. 15.  3.  1. 29.] 
cards in deck: 14 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25] -> size -> 28 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 23. 30. 27. 29.  8.  9.  9.  9.  9.  6.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [14. 11. 15. 25. 16.] 
adversary cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.  4.  0.  1.  3.  0.  0.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4] -> size -> 23 
adversary victory points: 6
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -25 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 10.834966659545898



action possibilites: [-1] 
expected returns: [[4.660329]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 1. 1.] 
cards in discard: [25.  1. 25. 29. 29. 15.  3.  1. 29.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25] -> size -> 28 
action values: 0 
buys: 0 
player value: 2 
card supply: [27. 23. 30. 27. 29.  8.  8.  9.  9.  9.  6.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [14. 11. 15. 25. 16.] 
adversary cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.  4.  0.  1.  3.  0.  0.  6.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6] -> size -> 24 
adversary victory points: 6
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -60   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: -5 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 19.091251373291016





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[  0.4694085]
 [ 10.967405 ]
 [-25.545536 ]
 [  5.2260637]
 [-19.721182 ]
 [-49.23056  ]
 [  6.194768 ]
 [ 10.337487 ]
 [  3.0049143]
 [ 12.895979 ]
 [  6.674941 ]
 [ 11.557669 ]
 [  8.116729 ]
 [ -7.0434914]
 [ 12.685196 ]
 [  2.68539  ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 1. 1.] 
cards in discard: [25.  1. 25. 29. 29. 15.  3.  1. 29.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25] -> size -> 28 
action values: 0 
buys: 1 
player value: 7 
card supply: [27. 23. 30. 27. 29.  8.  8.  9.  9.  9.  6.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [14. 11. 15. 25. 16.] 
adversary cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.  4.  0.  1.  3.  0.  0.  6.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6] -> size -> 24 
adversary victory points: 6
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -60   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: -5 

action type: take_action - action -1
Learning step: 0
desired expected reward: 4.6603288650512695



buy possibilites: [-1] 
expected returns: [[7.6212416]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 1. 1.] 
cards in discard: [25.  1. 25. 29. 29. 15.  3.  1. 29. 25.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25] -> size -> 29 
action values: 0 
buys: 0 
player value: 2 
card supply: [27. 23. 30. 27. 29.  8.  8.  9.  9.  9.  5.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [14. 11. 15. 25. 16.] 
adversary cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.  4.  0.  1.  3.  0.  0.  6.] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6] -> size -> 24 
adversary victory points: 6
player victory points: 4 

Reward from previous game state: 
[ -5.    0.    0.  -60.    0.    0.   60.    0.    0.    0.    0.    0.
   0.    0.   62.5   0. ] 
sum of rewards: 57.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 12.89598274230957






Player: 1 
cards in hand: [14. 11. 15. 25. 16.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 14. 11. 15. 25. 16.] 
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [14. 11. 15. 25. 16.] 
cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.  4.  0.  1.  3.  0.  0.  6.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6] -> size -> 24 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 29.  8.  8.  9.  9.  9.  5.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [ 3.  1.  0. 29.  0.] 
adversary cards in discard: [25.  1. 25. 29. 29. 15.  3.  1. 29. 25. 29. 29. 25.  0.  3.  1.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25] -> size -> 29 
adversary victory points: 4
player victory points: 5 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [14. 11. 25. 16.] 
cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.  4.  0.  1.  3.  0.  0.  6.] 
cards in deck: 1 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6] -> size -> 24 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 29.  8.  8.  9.  9.  9.  5.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [ 3.  1.  0. 29.  0.] 
adversary cards in discard: [25.  1. 25. 29. 29. 15.  3.  1. 29. 25. 29. 29. 25.  0.  3.  1.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25] -> size -> 29 
adversary victory points: 4
player victory points: 5 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [14. 11. 25. 16.] 
cards in discard: [22. 15.  1.  0.  6.  0.  0.  8.  3.  0.  3.  4.  0.  1.  3.  0.  0.  6.] 
cards in deck: 1 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6] -> size -> 24 
action values: 0 
buys: 1 
player value: 0 
card supply: [27. 23. 30. 27. 29.  8.  8.  9.  9.  9.  5.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [ 3.  1.  0. 29.  0.] 
adversary cards in discard: [25.  1. 25. 29. 29. 15.  3.  1. 29. 25. 29. 29. 25.  0.  3.  1.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25] -> size -> 29 
adversary victory points: 4
player victory points: 5 





         -------------------- Turn: 21 -------------------- 
Player: 0 
cards in hand: [ 3.  1.  0. 29.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[59.78192 ]
 [70.546425]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  1.  0. 29.  0.] 
cards in discard: [25.  1. 25. 29. 29. 15.  3.  1. 29. 25. 29. 29. 25.  0.  3.  1.  1.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25] -> size -> 29 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 29.  8.  8.  9.  9.  9.  5.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [ 0. 22.  3. 15.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6] -> size -> 24 
adversary victory points: 5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -30   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -35 

action type: buy - action -1
Learning step: 0
desired expected reward: 7.621241569519043



action possibilites: [-1.] 
expected returns: [[83.68681]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 0.] 
cards in discard: [25.  1. 25. 29. 29. 15.  3.  1. 29. 25. 29. 29. 25.  0.  3.  1.  1.  1.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25] -> size -> 29 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 23. 30. 27. 29.  8.  8.  9.  9.  9.  5.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [ 0. 22.  3. 15.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6] -> size -> 24 
adversary victory points: 5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 66.61864471435547





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[78.854164]
 [88.66569 ]
 [83.304794]
 [26.84722 ]
 [84.735756]
 [88.114456]
 [80.960396]
 [84.81011 ]
 [86.302986]
 [90.055115]
 [81.83493 ]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0.] 
cards in discard: [25.  1. 25. 29. 29. 15.  3.  1. 29. 25. 29. 29. 25.  0.  3.  1.  1.  1.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25] -> size -> 29 
action values: 0 
buys: 1 
player value: 4 
card supply: [27. 23. 30. 27. 29.  8.  8.  9.  9.  9.  5.  0.  9. 10. 10.  9.  7.] 
adversary cards in hand: [ 0. 22.  3. 15.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6] -> size -> 24 
adversary victory points: 5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 83.68681335449219



buy possibilites: [-1] 
expected returns: [[48.902054]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0.] 
cards in discard: [25.  1. 25. 29. 29. 15.  3.  1. 29. 25. 29. 29. 25.  0.  3.  1.  1.  1.
 15.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15] -> size -> 30 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 29.  8.  8.  9.  9.  9.  5.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 0. 22.  3. 15.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6] -> size -> 24 
adversary victory points: 5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0 128   0] 
sum of rewards: 113 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 90.05513000488281






Player: 1 
cards in hand: [ 0. 22.  3. 15.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 22. 15.] 
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 22.  3. 15.  3.] 
cards in discard: [] 
cards in deck: 19 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6] -> size -> 24 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 27. 29.  8.  8.  9.  9.  9.  5.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3.  0. 29. 29. 29.] 
adversary cards in discard: [25.  1. 25. 29. 29. 15.  3.  1. 29. 25. 29. 29. 25.  0.  3.  1.  1.  1.
 15. 29.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15] -> size -> 30 
adversary victory points: 4
player victory points: 5 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [22.  3.  3.] 
cards in discard: [] 
cards in deck: 19 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6] -> size -> 23 
action values: 0 
buys: 0 
player value: 3 
card supply: [27. 23. 30. 27. 29.  8.  8.  9.  9.  9.  5.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3.  0. 29. 29. 29.] 
adversary cards in discard: [25.  1. 25. 29. 29. 15.  3.  1. 29. 25. 29. 29. 25.  0.  3.  1.  1.  1.
 15. 29.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15] -> size -> 30 
adversary victory points: 4
player victory points: 5 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [22.  3.  3.] 
cards in discard: [] 
cards in deck: 19 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6] -> size -> 23 
action values: 0 
buys: 1 
player value: 3 
card supply: [27. 23. 30. 27. 29.  8.  8.  9.  9.  9.  5.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3.  0. 29. 29. 29.] 
adversary cards in discard: [25.  1. 25. 29. 29. 15.  3.  1. 29. 25. 29. 29. 25.  0.  3.  1.  1.  1.
 15. 29.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15] -> size -> 30 
adversary victory points: 4
player victory points: 5 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [22.  3.  3.] 
cards in discard: [3.] 
cards in deck: 19 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3] -> size -> 24 
action values: 0 
buys: 0 
player value: 1 
card supply: [27. 23. 30. 26. 29.  8.  8.  9.  9.  9.  5.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3.  0. 29. 29. 29.] 
adversary cards in discard: [25.  1. 25. 29. 29. 15.  3.  1. 29. 25. 29. 29. 25.  0.  3.  1.  1.  1.
 15. 29.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15] -> size -> 30 
adversary victory points: 4
player victory points: 6 





         -------------------- Turn: 22 -------------------- 
Player: 0 
cards in hand: [ 3.  0. 29. 29. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29.] 
expected returns: [[ 6.5059967]
 [17.620571 ]
 [17.620571 ]
 [17.620571 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 29. 29. 29.] 
cards in discard: [25.  1. 25. 29. 29. 15.  3.  1. 29. 25. 29. 29. 25.  0.  3.  1.  1.  1.
 15. 29.  3.  0.  0.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15] -> size -> 30 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 26. 29.  8.  8.  9.  9.  9.  5.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [1. 1. 0. 0. 8.] 
adversary cards in discard: [ 3. 15. 22.  3.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3] -> size -> 24 
adversary victory points: 6
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -60   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -65 

action type: buy - action -1
Learning step: 0
desired expected reward: 48.90205383300781



action possibilites: [-1. 29.] 
expected returns: [[ 1.8863268]
 [13.578987 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 29.  0.] 
cards in discard: [25.  1. 25. 29. 29. 15.  3.  1. 29. 25. 29. 29. 25.  0.  3.  1.  1.  1.
 15. 29.  3.  0.  0.  0. 29.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15] -> size -> 30 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 23. 30. 26. 29.  8.  8.  9.  9.  9.  5.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [1. 1. 0. 0. 8.] 
adversary cards in discard: [ 3. 15. 22.  3.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3] -> size -> 24 
adversary victory points: 6
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -60   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 13.567499160766602



action possibilites: [-1.] 
expected returns: [[1.987196]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0.] 
cards in discard: [3.] 
cards in deck: 24 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15] -> size -> 30 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 23. 30. 26. 29.  8.  8.  9.  9.  9.  5.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [1. 1. 0. 0. 8.] 
adversary cards in discard: [ 3. 15. 22.  3.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3] -> size -> 24 
adversary victory points: 6
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -25 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 9.314122200012207





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[  2.0443883]
 [ 13.078583 ]
 [  7.132041 ]
 [-17.842077 ]
 [-48.72325  ]
 [  7.888809 ]
 [ 12.399454 ]
 [  4.570507 ]
 [ 15.180349 ]
 [  8.558899 ]
 [ 13.722725 ]
 [  9.981516 ]
 [ -5.247963 ]
 [ 14.951015 ]
 [  4.0175943]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0.] 
cards in discard: [3.] 
cards in deck: 24 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15] -> size -> 30 
action values: 0 
buys: 1 
player value: 5 
card supply: [27. 23. 30. 26. 29.  8.  8.  9.  9.  9.  5.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [1. 1. 0. 0. 8.] 
adversary cards in discard: [ 3. 15. 22.  3.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3] -> size -> 24 
adversary victory points: 6
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -25 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 1.9871959686279297



buy possibilites: [-1] 
expected returns: [[0.8619237]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0.] 
cards in discard: [ 3. 25.] 
cards in deck: 24 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25] -> size -> 31 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 26. 29.  8.  8.  9.  9.  9.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [1. 1. 0. 0. 8.] 
adversary cards in discard: [ 3. 15. 22.  3.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3] -> size -> 24 
adversary victory points: 6
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0 250   0] 
sum of rewards: 225 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 15.18035888671875






Player: 1 
cards in hand: [1. 1. 0. 0. 8.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 1. 0. 0. 8.] 
cards in discard: [ 3. 15. 22.  3.  3.] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3] -> size -> 24 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 23. 30. 26. 29.  8.  8.  9.  9.  9.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [29.  1.  0. 25. 15.] 
adversary cards in discard: [ 3. 25. 29. 29.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25] -> size -> 31 
adversary victory points: 4
player victory points: 6 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
Chosen buy action: 2.0 : ['Gold' '2' '6']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 1. 0. 0. 8.] 
cards in discard: [ 3. 15. 22.  3.  3.] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3] -> size -> 24 
action values: 0 
buys: 1 
player value: 6 
card supply: [27. 23. 30. 26. 29.  8.  8.  9.  9.  9.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [29.  1.  0. 25. 15.] 
adversary cards in discard: [ 3. 25. 29. 29.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25] -> size -> 31 
adversary victory points: 4
player victory points: 6 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 1. 0. 0. 8.] 
cards in discard: [ 3. 15. 22.  3.  3.  2.] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2] -> size -> 25 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 23. 29. 26. 29.  8.  8.  9.  9.  9.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [29.  1.  0. 25. 15.] 
adversary cards in discard: [ 3. 25. 29. 29.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25] -> size -> 31 
adversary victory points: 4
player victory points: 6 





         -------------------- Turn: 23 -------------------- 
Player: 0 
cards in hand: [29.  1.  0. 25. 15.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 25. 15.] 
expected returns: [[ 8.293993]
 [23.246782]
 [20.001722]
 [19.760397]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  1.  0. 25. 15.] 
cards in discard: [ 3. 25. 29. 29.  0.  0.  0.] 
cards in deck: 19 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25] -> size -> 31 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 23. 29. 26. 29.  8.  8.  9.  9.  9.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3. 11.  6.  0.  0.] 
adversary cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2] -> size -> 25 
adversary victory points: 6
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -60   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -65 

action type: buy - action -1
Learning step: 0
desired expected reward: 0.8619236946105957



action possibilites: [-1. 25. 15. 25.] 
expected returns: [[-5.1175003]
 [ 6.6165514]
 [ 6.371413 ]
 [ 6.6165514]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 25. 15. 25.] 
cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.] 
cards in deck: 18 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25] -> size -> 31 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 23. 29. 26. 29.  8.  8.  9.  9.  9.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3. 11.  6.  0.  0.] 
adversary cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2] -> size -> 25 
adversary victory points: 6
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -60   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 17.769302368164062



action possibilites: [-1] 
expected returns: [[-27.062689]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 15. 25.  1. 29.] 
cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.] 
cards in deck: 16 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25] -> size -> 31 
action values: 0 
buys: 0 
player value: 1 
card supply: [27. 23. 29. 26. 29.  8.  7.  9.  9.  9.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3. 11.  6.  0.  0.] 
adversary cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.  6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6] -> size -> 26 
adversary victory points: 6
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -25 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 6.616556167602539





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[ -34.102047]
 [ -24.371756]
 [ -31.618944]
 [-114.03173 ]
 [ -27.062191]
 [ -27.21481 ]
 [ -32.45961 ]
 [ -29.163109]
 [ -26.666397]
 [ -25.738989]
 [ -28.667181]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 15. 25.  1. 29.] 
cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.] 
cards in deck: 16 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25] -> size -> 31 
action values: 0 
buys: 1 
player value: 4 
card supply: [27. 23. 29. 26. 29.  8.  7.  9.  9.  9.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3. 11.  6.  0.  0.] 
adversary cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.  6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6] -> size -> 26 
adversary victory points: 6
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -25 

action type: take_action - action -1
Learning step: 0
desired expected reward: -27.06268882751465



buy possibilites: [-1] 
expected returns: [[1.5638103]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 15. 25.  1. 29.] 
cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.  1.] 
cards in deck: 16 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1] -> size -> 32 
action values: 0 
buys: 0 
player value: 1 
card supply: [27. 22. 29. 26. 29.  8.  7.  9.  9.  9.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3. 11.  6.  0.  0.] 
adversary cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.  6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6] -> size -> 26 
adversary victory points: 6
player victory points: 4 

Reward from previous game state: 
[ -5.    0.    0.  -60.    0.    0.   40.    0.    0.    0.    0.    0.
   0.    0.   13.5   0. ] 
sum of rewards: -11.5 

action type: buy - action 1.0
Learning step: 0
desired expected reward: -24.371732711791992






Player: 1 
cards in hand: [ 3. 11.  6.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 11.  6.  0.  0.] 
cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.  6.] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6] -> size -> 26 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 22. 29. 26. 29.  8.  7.  9.  9.  9.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [29. 29. 29. 29.  3.] 
adversary cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.  1. 29. 25.  0. 15. 25.  1. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1] -> size -> 32 
adversary victory points: 4
player victory points: 5 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 6. 0. 0.] 
cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.  6.  0.] 
cards in deck: 9 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0] -> size -> 27 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 22. 29. 26. 29.  8.  7.  9.  9.  9.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [29. 29. 29. 29.  3.] 
adversary cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.  1. 29. 25.  0. 15. 25.  1. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1] -> size -> 32 
adversary victory points: 4
player victory points: 5 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 6. 0. 0.] 
cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.  6.  0.] 
cards in deck: 9 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0] -> size -> 27 
action values: 0 
buys: 1 
player value: 2 
card supply: [26. 22. 29. 26. 29.  8.  7.  9.  9.  9.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [29. 29. 29. 29.  3.] 
adversary cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.  1. 29. 25.  0. 15. 25.  1. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1] -> size -> 32 
adversary victory points: 4
player victory points: 5 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 6. 0. 0.] 
cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.  6.  0.  8.] 
cards in deck: 9 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8] -> size -> 28 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 22. 29. 26. 29.  8.  7.  9.  9.  8.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [29. 29. 29. 29.  3.] 
adversary cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.  1. 29. 25.  0. 15. 25.  1. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1] -> size -> 32 
adversary victory points: 4
player victory points: 5 





         -------------------- Turn: 24 -------------------- 
Player: 0 
cards in hand: [29. 29. 29. 29.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29. 29.] 
expected returns: [[19.365257]
 [29.09263 ]
 [29.09263 ]
 [29.09263 ]
 [29.09263 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29. 29. 29.  3.] 
cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.  1. 29. 25.  0. 15. 25.  1. 29.] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1] -> size -> 32 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 22. 29. 26. 29.  8.  7.  9.  9.  8.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [15. 25.  3.  0. 14.] 
adversary cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.  6.  0.  8. 11.  3.  6.  0.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8] -> size -> 28 
adversary victory points: 5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -30   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -35 

action type: buy - action -1
Learning step: 0
desired expected reward: 1.5638103485107422



action possibilites: [-1. 29. 29. 29.] 
expected returns: [[ 3.8630242]
 [16.14528  ]
 [16.14528  ]
 [16.14528  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29. 29.  3.] 
cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.  1. 29. 25.  0. 15. 25.  1. 29.  1.] 
cards in deck: 10 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1] -> size -> 32 
action values: 1 
buys: 0 
player value: 1 
card supply: [26. 22. 29. 26. 29.  8.  7.  9.  9.  8.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [15. 25.  3.  0. 14.] 
adversary cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.  6.  0.  8. 11.  3.  6.  0.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8] -> size -> 28 
adversary victory points: 5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 25.54135513305664



action possibilites: [-1. 29. 29.] 
expected returns: [[23.771502]
 [35.0709  ]
 [35.0709  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29.  3.] 
cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.  1. 29. 25.  0. 15. 25.  1. 29.  1.  1.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1] -> size -> 32 
action values: 1 
buys: 0 
player value: 2 
card supply: [26. 22. 29. 26. 29.  8.  7.  9.  9.  8.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [15. 25.  3.  0. 14.] 
adversary cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.  6.  0.  8. 11.  3.  6.  0.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8] -> size -> 28 
adversary victory points: 5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -30   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 5 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 11.657611846923828



action possibilites: [-1. 29.] 
expected returns: [[ 5.783139]
 [17.764938]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  3.] 
cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.  1. 29. 25.  0. 15. 25.  1. 29.  1.  1.
  3.] 
cards in deck: 8 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1] -> size -> 32 
action values: 1 
buys: 0 
player value: 3 
card supply: [26. 22. 29. 26. 29.  8.  7.  9.  9.  8.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [15. 25.  3.  0. 14.] 
adversary cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.  6.  0.  8. 11.  3.  6.  0.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8] -> size -> 28 
adversary victory points: 5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -30   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 25 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 30.942277908325195



action possibilites: [-1.] 
expected returns: [[-17.499624]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0.] 
cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.  1. 29. 25.  0. 15. 25.  1. 29.  1.  1.
  3.  3.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1] -> size -> 32 
action values: 1 
buys: 0 
player value: 4 
card supply: [26. 22. 29. 26. 29.  8.  7.  9.  9.  8.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [15. 25.  3.  0. 14.] 
adversary cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.  6.  0.  8. 11.  3.  6.  0.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8] -> size -> 28 
adversary victory points: 5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -30   0   0  80   0   0   0   0   0   0   0   0   0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 13.390626907348633





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[-20.470318]
 [-10.160227]
 [-15.522813]
 [-39.03283 ]
 [-67.082115]
 [-14.652237]
 [-10.728773]
 [-18.880587]
 [ -8.32576 ]
 [-14.173721]
 [ -9.595137]
 [-12.831786]
 [-27.034994]
 [ -8.5247  ]
 [-18.018955]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.  1. 29. 25.  0. 15. 25.  1. 29.  1.  1.
  3.  3.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1] -> size -> 32 
action values: 0 
buys: 1 
player value: 5 
card supply: [26. 22. 29. 26. 29.  8.  7.  9.  9.  8.  4.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [15. 25.  3.  0. 14.] 
adversary cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.  6.  0.  8. 11.  3.  6.  0.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8] -> size -> 28 
adversary victory points: 5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -30   0   0  80   0   0   0   0   0   0   0   0   0] 
sum of rewards: 45 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -17.499624252319336



buy possibilites: [-1] 
expected returns: [[4.4338017]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.  1. 29. 25.  0. 15. 25.  1. 29.  1.  1.
  3.  3. 25.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25] -> size -> 33 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 22. 29. 26. 29.  8.  7.  9.  9.  8.  3.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [15. 25.  3.  0. 14.] 
adversary cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.  6.  0.  8. 11.  3.  6.  0.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8] -> size -> 28 
adversary victory points: 5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -30   0   0  80   0   0   0   0   0   0   0 250   0] 
sum of rewards: 295 

action type: buy - action 25.0
Learning step: 0
desired expected reward: -8.325764656066895






Player: 1 
cards in hand: [15. 25.  3.  0. 14.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 25. 14.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 25.  3.  0. 14.] 
cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.  6.  0.  8. 11.  3.  6.  0.
  0.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8] -> size -> 28 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 22. 29. 26. 29.  8.  7.  9.  9.  8.  3.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 1.  0. 25. 29. 15.] 
adversary cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.  1. 29. 25.  0. 15. 25.  1. 29.  1.  1.
  3.  3. 25. 29. 29. 29. 29.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25] -> size -> 33 
adversary victory points: 4
player victory points: 5 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15. 25.  3.  0. 14.] 
cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.  6.  0.  8. 11.  3.  6.  0.
  0.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8] -> size -> 28 
action values: 0 
buys: 1 
player value: 1 
card supply: [26. 22. 29. 26. 29.  8.  7.  9.  9.  8.  3.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 1.  0. 25. 29. 15.] 
adversary cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.  1. 29. 25.  0. 15. 25.  1. 29.  1.  1.
  3.  3. 25. 29. 29. 29. 29.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25] -> size -> 33 
adversary victory points: 4
player victory points: 5 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15. 25.  3.  0. 14.] 
cards in discard: [ 3. 15. 22.  3.  3.  2.  1.  1.  0.  0.  8.  6.  0.  8. 11.  3.  6.  0.
  0.  0.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0] -> size -> 29 
action values: 0 
buys: 0 
player value: 1 
card supply: [25. 22. 29. 26. 29.  8.  7.  9.  9.  8.  3.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 1.  0. 25. 29. 15.] 
adversary cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.  1. 29. 25.  0. 15. 25.  1. 29.  1.  1.
  3.  3. 25. 29. 29. 29. 29.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25] -> size -> 33 
adversary victory points: 4
player victory points: 5 





         -------------------- Turn: 25 -------------------- 
Player: 0 
cards in hand: [ 1.  0. 25. 29. 15.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 29. 15.] 
expected returns: [[-7.4522576]
 [ 2.9122906]
 [ 5.948103 ]
 [ 2.697301 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  0. 25. 29. 15.] 
cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.  1. 29. 25.  0. 15. 25.  1. 29.  1.  1.
  3.  3. 25. 29. 29. 29. 29.  0.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25] -> size -> 33 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 22. 29. 26. 29.  8.  7.  9.  9.  8.  3.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 0.  0.  6.  4. 16.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0] -> size -> 29 
adversary victory points: 5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -30   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -35 

action type: buy - action -1
Learning step: 0
desired expected reward: 4.433801651000977



action possibilites: [-1. 25. 15.] 
expected returns: [[11.86298 ]
 [22.765371]
 [22.53962 ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 25. 15.  3.] 
cards in discard: [ 3. 25. 29. 29.  0.  0.  0.  1.  1. 29. 25.  0. 15. 25.  1. 29.  1.  1.
  3.  3. 25. 29. 29. 29. 29.  0.  1.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25] -> size -> 33 
action values: 1 
buys: 0 
player value: 1 
card supply: [25. 22. 29. 26. 29.  8.  7.  9.  9.  8.  3.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 0.  0.  6.  4. 16.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0] -> size -> 29 
adversary victory points: 5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 1.0538620948791504



action possibilites: [-1] 
expected returns: [[28.768202]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 15.  3. 25. 25.] 
cards in discard: [] 
cards in deck: 26 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25] -> size -> 33 
action values: 0 
buys: 0 
player value: 1 
card supply: [25. 22. 29. 26. 29.  8.  6.  9.  9.  8.  3.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 0.  0.  6.  4. 16.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6] -> size -> 30 
adversary victory points: 5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -30   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 5 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 22.7653751373291





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
expected returns: [[ 27.251583]
 [ 32.436584]
 [-21.644545]
 [ 29.682772]
 [ 29.05683 ]]
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 15.  3. 25. 25.] 
cards in discard: [] 
cards in deck: 26 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25] -> size -> 33 
action values: 0 
buys: 1 
player value: 2 
card supply: [25. 22. 29. 26. 29.  8.  6.  9.  9.  8.  3.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 0.  0.  6.  4. 16.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6] -> size -> 30 
adversary victory points: 5
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 -30   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 5 

action type: take_action - action -1
Learning step: 0
desired expected reward: 28.76820182800293



buy possibilites: [-1] 
expected returns: [[26.615328]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 15.  3. 25. 25.] 
cards in discard: [3.] 
cards in deck: 26 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25  3] -> size -> 34 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 22. 29. 25. 29.  8.  6.  9.  9.  8.  3.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 0.  0.  6.  4. 16.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6] -> size -> 30 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0 16  0] 
sum of rewards: 51 

action type: buy - action 3.0
Learning step: 0
desired expected reward: 32.43659210205078






Player: 1 
cards in hand: [ 0.  0.  6.  4. 16.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  6.  4. 16.] 
cards in discard: [6.] 
cards in deck: 24 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6] -> size -> 30 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 22. 29. 25. 29.  8.  6.  9.  9.  8.  3.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 1. 25. 29.  3. 29.] 
adversary cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25  3] -> size -> 34 
adversary victory points: 5
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0.  6.  4. 16.] 
cards in discard: [6.] 
cards in deck: 24 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6] -> size -> 30 
action values: 0 
buys: 1 
player value: 2 
card supply: [25. 22. 29. 25. 29.  8.  6.  9.  9.  8.  3.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 1. 25. 29.  3. 29.] 
adversary cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25  3] -> size -> 34 
adversary victory points: 5
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0.  6.  4. 16.] 
cards in discard: [6. 3.] 
cards in deck: 24 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3] -> size -> 31 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 22. 29. 24. 29.  8.  6.  9.  9.  8.  3.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 1. 25. 29.  3. 29.] 
adversary cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25  3] -> size -> 34 
adversary victory points: 5
player victory points: 5 





         -------------------- Turn: 26 -------------------- 
Player: 0 
cards in hand: [ 1. 25. 29.  3. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 29. 29.] 
expected returns: [[ 9.041735]
 [17.268288]
 [19.707777]
 [19.707777]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 25. 29.  3. 29.] 
cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25.] 
cards in deck: 21 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25  3] -> size -> 34 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 22. 29. 24. 29.  8.  6.  9.  9.  8.  3.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [15.  0. 11.  3.  3.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3] -> size -> 31 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 26.615327835083008



action possibilites: [-1. 29.] 
expected returns: [[-16.967669]
 [ -3.289688]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  3. 29.  0.] 
cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.] 
cards in deck: 20 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25  3] -> size -> 34 
action values: 1 
buys: 0 
player value: 1 
card supply: [25. 22. 29. 24. 29.  8.  6.  9.  9.  8.  3.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [15.  0. 11.  3.  3.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3] -> size -> 31 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 15.814371109008789



action possibilites: [-1.] 
expected returns: [[-3.6492298]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 1.] 
cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25  3] -> size -> 34 
action values: 1 
buys: 0 
player value: 2 
card supply: [25. 22. 29. 24. 29.  8.  6.  9.  9.  8.  3.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [15.  0. 11.  3.  3.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3] -> size -> 31 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -8.295697212219238





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ -8.252178  ]
 [  2.8699384 ]
 [ -3.1220334 ]
 [-31.114086  ]
 [-71.26257   ]
 [ -2.3723006 ]
 [  2.1837635 ]
 [ -5.708275  ]
 [  4.9928102 ]
 [ -1.688232  ]
 [  3.520523  ]
 [ -0.25852156]
 [-15.575168  ]
 [  4.76116   ]
 [ -6.282503  ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 1.] 
cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25  3] -> size -> 34 
action values: 0 
buys: 1 
player value: 5 
card supply: [25. 22. 29. 24. 29.  8.  6.  9.  9.  8.  3.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [15.  0. 11.  3.  3.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3] -> size -> 31 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -3.6492297649383545



buy possibilites: [-1] 
expected returns: [[-21.234465]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 1.] 
cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25  3 25] -> size -> 35 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 22. 29. 24. 29.  8.  6.  9.  9.  8.  2.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [15.  0. 11.  3.  3.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3] -> size -> 31 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[ -5   0   0   0   0   0  40   0   0   0   0   0   0   0 250   0] 
sum of rewards: 285 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 4.992815971374512






Player: 1 
cards in hand: [15.  0. 11.  3.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 11.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0. 11.  3.  3.] 
cards in discard: [ 6.  3.  0.  0.  6.  4. 16.] 
cards in deck: 19 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3] -> size -> 31 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 22. 29. 24. 29.  8.  6.  9.  9.  8.  2.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3.  1. 29. 29. 15.] 
adversary cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25  3 25] -> size -> 35 
adversary victory points: 5
player victory points: 5 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  0. 11.  3.  3.] 
cards in discard: [ 6.  3.  0.  0.  6.  4. 16.] 
cards in deck: 19 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3] -> size -> 31 
action values: 0 
buys: 1 
player value: 1 
card supply: [25. 22. 29. 24. 29.  8.  6.  9.  9.  8.  2.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3.  1. 29. 29. 15.] 
adversary cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25  3 25] -> size -> 35 
adversary victory points: 5
player victory points: 5 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  0. 11.  3.  3.] 
cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0.] 
cards in deck: 19 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0] -> size -> 32 
action values: 0 
buys: 0 
player value: 1 
card supply: [24. 22. 29. 24. 29.  8.  6.  9.  9.  8.  2.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3.  1. 29. 29. 15.] 
adversary cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25  3 25] -> size -> 35 
adversary victory points: 5
player victory points: 5 





         -------------------- Turn: 27 -------------------- 
Player: 0 
cards in hand: [ 3.  1. 29. 29. 15.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 15.] 
expected returns: [[76.774925]
 [87.3868  ]
 [87.3868  ]
 [84.84488 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  1. 29. 29. 15.] 
cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25  3 25] -> size -> 35 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 22. 29. 24. 29.  8.  6.  9.  9.  8.  2.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3.  0.  3. 25.  0.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0] -> size -> 32 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: -21.234464645385742



action possibilites: [-1. 29. 15.] 
expected returns: [[-28.778093]
 [-17.080132]
 [-19.971159]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 29. 15.  1.] 
cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.  3.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25  3 25] -> size -> 35 
action values: 1 
buys: 0 
player value: 1 
card supply: [24. 22. 29. 24. 29.  8.  6.  9.  9.  8.  2.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3.  0.  3. 25.  0.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0] -> size -> 32 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 83.45545196533203



action possibilites: [-1. 15.] 
expected returns: [[-35.52825 ]
 [-27.171412]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  1.  0.] 
cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.  3.  1.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1
 15  1 25 25 25 15 25  1 25  3 25] -> size -> 35 
action values: 1 
buys: 0 
player value: 2 
card supply: [24. 22. 29. 24. 29.  8.  6.  9.  9.  8.  2.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3.  0.  3. 25.  0.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0] -> size -> 32 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -21.350961685180664



action possibilites: [-1] 
expected returns: [[-13.990044]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1.] 
cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.  3.  1.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25] -> size -> 34 
action values: 0 
buys: 0 
player value: 5 
card supply: [24. 22. 29. 24. 29.  8.  6.  9.  9.  8.  2.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3.  0.  3. 25.  0.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0] -> size -> 32 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: -27.171419143676758





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[-16.769693 ]
 [ -6.112482 ]
 [-42.56008  ]
 [-11.958858 ]
 [-36.828377 ]
 [-65.65531  ]
 [-10.885743 ]
 [ -6.941199 ]
 [-14.60058  ]
 [ -4.3411245]
 [-10.508007 ]
 [ -5.5999565]
 [ -9.057362 ]
 [-24.26746  ]
 [ -4.5474405]
 [-14.276959 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1.] 
cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.  3.  1.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25] -> size -> 34 
action values: 0 
buys: 1 
player value: 7 
card supply: [24. 22. 29. 24. 29.  8.  6.  9.  9.  8.  2.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3.  0.  3. 25.  0.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0] -> size -> 32 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: take_action - action -1
Learning step: 0
desired expected reward: -13.990043640136719



buy possibilites: [-1] 
expected returns: [[-4.589569]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1.] 
cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.  3.  1.
 25.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25] -> size -> 35 
action values: 0 
buys: 0 
player value: 2 
card supply: [24. 22. 29. 24. 29.  8.  6.  9.  9.  8.  1.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 3.  0.  3. 25.  0.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0] -> size -> 32 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[-5.   0.   0.   0.   0.   0.  60.   0.   0.   0.   0.   0.   0.   0.
 62.5  0. ] 
sum of rewards: 117.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: -4.341127872467041






Player: 1 
cards in hand: [ 3.  0.  3. 25.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0.  3. 25.  0.] 
cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0] -> size -> 32 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 22. 29. 24. 29.  8.  6.  9.  9.  8.  1.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [29.  1.  1. 25.  0.] 
adversary cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.  3.  1.
 25. 29. 29. 15.  1.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25] -> size -> 35 
adversary victory points: 5
player victory points: 5 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0.  3. 25.  0.] 
cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0] -> size -> 32 
action values: 0 
buys: 1 
player value: 2 
card supply: [24. 22. 29. 24. 29.  8.  6.  9.  9.  8.  1.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [29.  1.  1. 25.  0.] 
adversary cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.  3.  1.
 25. 29. 29. 15.  1.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25] -> size -> 35 
adversary victory points: 5
player victory points: 5 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0.  3. 25.  0.] 
cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0] -> size -> 33 
action values: 0 
buys: 0 
player value: 2 
card supply: [23. 22. 29. 24. 29.  8.  6.  9.  9.  8.  1.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [29.  1.  1. 25.  0.] 
adversary cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.  3.  1.
 25. 29. 29. 15.  1.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25] -> size -> 35 
adversary victory points: 5
player victory points: 5 





         -------------------- Turn: 28 -------------------- 
Player: 0 
cards in hand: [29.  1.  1. 25.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 25.] 
expected returns: [[-14.776218 ]
 [ -3.573843 ]
 [ -6.2588596]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  1.  1. 25.  0.] 
cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.  3.  1.
 25. 29. 29. 15.  1.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25] -> size -> 35 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 22. 29. 24. 29.  8.  6.  9.  9.  8.  1.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 0.  0. 15.  0.  8.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0] -> size -> 33 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: -4.589569091796875



action possibilites: [-1. 25. 29.] 
expected returns: [[-16.523903 ]
 [ -7.5875688]
 [ -4.829396 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 25.  0. 29.] 
cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.  3.  1.
 25. 29. 29. 15.  1.  1.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25] -> size -> 35 
action values: 1 
buys: 0 
player value: 1 
card supply: [23. 22. 29. 24. 29.  8.  6.  9.  9.  8.  1.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 0.  0. 15.  0.  8.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0] -> size -> 33 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -7.659182071685791



action possibilites: [-1. 25. 25.] 
expected returns: [[-19.664913]
 [-10.728569]
 [-10.728569]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  0. 25.] 
cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.  3.  1.
 25. 29. 29. 15.  1.  1.  1.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25] -> size -> 35 
action values: 1 
buys: 0 
player value: 2 
card supply: [23. 22. 29. 24. 29.  8.  6.  9.  9.  8.  1.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 0.  0. 15.  0.  8.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0] -> size -> 33 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -9.09560775756836



action possibilites: [-1] 
expected returns: [[-8.110445]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 25. 29.  0.] 
cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.  3.  1.
 25. 29. 29. 15.  1.  1.  1.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25] -> size -> 35 
action values: 0 
buys: 0 
player value: 2 
card supply: [23. 22. 29. 24. 29.  8.  5.  9.  9.  8.  1.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 0.  0. 15.  0.  8.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.  6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6] -> size -> 34 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: -10.72857666015625





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[-11.020299  ]
 [ -1.2155559 ]
 [ -6.4636474 ]
 [-54.196976  ]
 [ -5.674918  ]
 [ -2.087977  ]
 [ -9.701277  ]
 [ -5.241667  ]
 [ -4.013156  ]
 [  0.17097116]
 [ -8.761665  ]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 25. 29.  0.] 
cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.  3.  1.
 25. 29. 29. 15.  1.  1.  1.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25] -> size -> 35 
action values: 0 
buys: 1 
player value: 4 
card supply: [23. 22. 29. 24. 29.  8.  5.  9.  9.  8.  1.  0.  9. 10. 10.  9.  6.] 
adversary cards in hand: [ 0.  0. 15.  0.  8.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.  6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6] -> size -> 34 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: take_action - action -1
Learning step: 0
desired expected reward: -8.110445022583008



buy possibilites: [-1] 
expected returns: [[-21.619724]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 25. 29.  0.] 
cards in discard: [ 3. 29. 25.  0. 15.  3. 25. 25. 25.  1. 25. 29. 29.  3.  0.  1.  3.  1.
 25. 29. 29. 15.  1.  1.  1. 15.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15] -> size -> 36 
action values: 0 
buys: 0 
player value: 0 
card supply: [23. 22. 29. 24. 29.  8.  5.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [ 0.  0. 15.  0.  8.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.  6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6] -> size -> 34 
adversary victory points: 5
player victory points: 5 

Reward from previous game state: 
[ -5   0   0   0   0   0  60   0   0   0   0 -10   0   0 128   0] 
sum of rewards: 173 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 0.17097187042236328






Player: 1 
cards in hand: [ 0.  0. 15.  0.  8.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15.  8.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0. 15.  0.  8.] 
cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.  6.] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6] -> size -> 34 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 22. 29. 24. 29.  8.  5.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [25. 25.  3. 29.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15] -> size -> 36 
adversary victory points: 5
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0. 15.  0.  8.] 
cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.  6.] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6] -> size -> 34 
action values: 0 
buys: 1 
player value: 3 
card supply: [23. 22. 29. 24. 29.  8.  5.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [25. 25.  3. 29.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15] -> size -> 36 
adversary victory points: 5
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0. 15.  0.  8.] 
cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.  6.  0.] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0] -> size -> 35 
action values: 0 
buys: 0 
player value: 3 
card supply: [22. 22. 29. 24. 29.  8.  5.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [25. 25.  3. 29.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15] -> size -> 36 
adversary victory points: 5
player victory points: 4 





         -------------------- Turn: 29 -------------------- 
Player: 0 
cards in hand: [25. 25.  3. 29.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 25. 29.] 
expected returns: [[ 4.035534]
 [13.651945]
 [13.651945]
 [16.460073]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 25.  3. 29.  0.] 
cards in discard: [] 
cards in deck: 31 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15] -> size -> 36 
action values: 1 
buys: 0 
player value: 0 
card supply: [22. 22. 29. 24. 29.  8.  5.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [22.  6.  1. 14.  8.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.  6.  0.  0.  0. 15.  0.  8.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0] -> size -> 35 
adversary victory points: 4
player victory points: 5 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: -21.61972427368164



action possibilites: [-1. 25. 25.] 
expected returns: [[ 8.808258]
 [18.814777]
 [18.814777]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 25.  3.  0.] 
cards in discard: [1.] 
cards in deck: 30 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15] -> size -> 36 
action values: 1 
buys: 0 
player value: 1 
card supply: [22. 22. 29. 24. 29.  8.  5.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [22.  6.  1. 14.  8.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.  6.  0.  0.  0. 15.  0.  8.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0] -> size -> 35 
adversary victory points: 4
player victory points: 5 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 11.923723220825195



action possibilites: [-1] 
expected returns: [[7.934881]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  3.  0.  0. 25.] 
cards in discard: [1.] 
cards in deck: 28 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15] -> size -> 36 
action values: 0 
buys: 0 
player value: 1 
card supply: [22. 22. 29. 24. 29.  8.  4.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [22.  6.  1. 14.  8.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.  6.  0.  0.  0. 15.  0.  8.  6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6] -> size -> 36 
adversary victory points: 4
player victory points: 5 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 18.814762115478516





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[  5.920829 ]
 [ 16.504126 ]
 [ 10.7699795]
 [-42.642998 ]
 [ 15.68218  ]
 [  7.8584785]
 [ 13.540276 ]
 [  8.312302 ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [25.  3.  0.  0. 25.] 
cards in discard: [1.] 
cards in deck: 28 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15] -> size -> 36 
action values: 0 
buys: 1 
player value: 3 
card supply: [22. 22. 29. 24. 29.  8.  4.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [22.  6.  1. 14.  8.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.  6.  0.  0.  0. 15.  0.  8.  6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6] -> size -> 36 
adversary victory points: 4
player victory points: 5 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action -1
Learning step: 0
desired expected reward: 7.934881210327148



buy possibilites: [-1] 
expected returns: [[28.26143]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [25.  3.  0.  0. 25.] 
cards in discard: [1. 1.] 
cards in deck: 28 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15  1] -> size -> 37 
action values: 0 
buys: 0 
player value: 0 
card supply: [22. 21. 29. 24. 29.  8.  4.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [22.  6.  1. 14.  8.] 
adversary cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.  6.  0.  0.  0. 15.  0.  8.  6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6] -> size -> 36 
adversary victory points: 4
player victory points: 5 

Reward from previous game state: 
[ -5   0   0  30   0   0  40   0   0   0   0 -20   0   0  54   0] 
sum of rewards: 99 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 16.504125595092773






Player: 1 
cards in hand: [22.  6.  1. 14.  8.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 22. 14.  8.] 
Chosen action: 14 : ['Militia' '14' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [22.  6.  1. 14.  8.] 
cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.  6.  0.  0.  0. 15.  0.  8.  6.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6] -> size -> 36 
action values: 1 
buys: 0 
player value: 0 
card supply: [22. 21. 29. 24. 29.  8.  4.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [ 1. 25.  1. 25. 25.] 
adversary cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15  1] -> size -> 37 
adversary victory points: 5
player victory points: 3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [22.  6.  1.  8.] 
cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.  6.  0.  0.  0. 15.  0.  8.  6.] 
cards in deck: 4 
card top of deck: [] 
played cards: [14.] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6] -> size -> 36 
action values: 0 
buys: 0 
player value: 2 
card supply: [22. 21. 29. 24. 29.  8.  4.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [ 1. 25. 25.] 
adversary cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15  1] -> size -> 37 
adversary victory points: 5
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [22.  6.  1.  8.] 
cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.  6.  0.  0.  0. 15.  0.  8.  6.] 
cards in deck: 4 
card top of deck: [] 
played cards: [14.] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6] -> size -> 36 
action values: 0 
buys: 1 
player value: 4 
card supply: [22. 21. 29. 24. 29.  8.  4.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [ 1. 25. 25.] 
adversary cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15  1] -> size -> 37 
adversary victory points: 5
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [22.  6.  1.  8.] 
cards in discard: [ 6.  3.  0.  0.  6.  4. 16.  0. 15.  0. 11.  3.  3.  0.  3.  0.  3. 25.
  0.  6.  0.  0.  0. 15.  0.  8.  6.  3.] 
cards in deck: 4 
card top of deck: [] 
played cards: [14.] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3] -> size -> 37 
action values: 0 
buys: 0 
player value: 2 
card supply: [22. 21. 29. 23. 29.  8.  4.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [ 1. 25. 25.] 
adversary cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15  1] -> size -> 37 
adversary victory points: 5
player victory points: 4 





         -------------------- Turn: 30 -------------------- 
Player: 0 
cards in hand: [ 1. 25. 25.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 25.] 
expected returns: [[-20.860546]
 [-11.507094]
 [-11.507094]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 25. 25.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.] 
cards in deck: 23 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15  1] -> size -> 37 
action values: 1 
buys: 0 
player value: 0 
card supply: [22. 21. 29. 23. 29.  8.  4.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [0. 3. 6. 2. 1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3] -> size -> 37 
adversary victory points: 4
player victory points: 5 

Reward from previous game state: 
[  -5    0    0   30    0    0    0    0    0    0    0  -20    0    0
 1650    0] 
sum of rewards: 1655 

action type: discard_down_to_3_cards - action 1
Learning step: 0
desired expected reward: -38.68280792236328



action possibilites: [-1] 
expected returns: [[10.129064]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 25.  0. 29.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.] 
cards in deck: 21 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15  1] -> size -> 37 
action values: 0 
buys: 0 
player value: 0 
card supply: [22. 21. 29. 23. 29.  8.  3.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [0. 3. 6. 2. 1.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6] -> size -> 38 
adversary victory points: 4
player victory points: 5 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: -11.507096290588379





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[  5.2565203]
 [ 14.829094 ]
 [  9.581743 ]
 [-43.01207  ]
 [ 14.050058 ]
 [  6.7535057]
 [ 12.335591 ]
 [  8.192532 ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1. 25.  0. 29.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.] 
cards in deck: 21 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15  1] -> size -> 37 
action values: 0 
buys: 1 
player value: 3 
card supply: [22. 21. 29. 23. 29.  8.  3.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [0. 3. 6. 2. 1.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6] -> size -> 38 
adversary victory points: 4
player victory points: 5 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action -1
Learning step: 0
desired expected reward: 10.129063606262207



buy possibilites: [-1] 
expected returns: [[11.018938]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1. 25.  0. 29.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1.] 
cards in deck: 21 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15  1  1] -> size -> 38 
action values: 0 
buys: 0 
player value: 0 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [0. 3. 6. 2. 1.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6] -> size -> 38 
adversary victory points: 4
player victory points: 5 

Reward from previous game state: 
[ -5   0   0  30   0   0  20   0   0   0   0 -30   0   0  54   0] 
sum of rewards: 69 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 14.829103469848633






Player: 1 
cards in hand: [0. 3. 6. 2. 1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 6. 2. 1.] 
cards in discard: [6.] 
cards in deck: 32 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6] -> size -> 38 
action values: 1 
buys: 0 
player value: 0 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [ 3. 29.  0. 29. 15.] 
adversary cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15  1  1] -> size -> 38 
adversary victory points: 5
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
Chosen buy action: 11.0 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 6. 2. 1.] 
cards in discard: [6.] 
cards in deck: 32 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6] -> size -> 38 
action values: 0 
buys: 1 
player value: 6 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  9.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [ 3. 29.  0. 29. 15.] 
adversary cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15  1  1] -> size -> 38 
adversary victory points: 5
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 6. 2. 1.] 
cards in discard: [ 6. 11.] 
cards in deck: 32 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11] -> size -> 39 
action values: 0 
buys: 0 
player value: 3 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [ 3. 29.  0. 29. 15.] 
adversary cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15  1  1] -> size -> 38 
adversary victory points: 5
player victory points: 3 





         -------------------- Turn: 31 -------------------- 
Player: 0 
cards in hand: [ 3. 29.  0. 29. 15.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 15.] 
expected returns: [[ 3.4960194]
 [14.143362 ]
 [14.143362 ]
 [11.499884 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29.  0. 29. 15.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29.] 
cards in deck: 16 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15  1  1] -> size -> 38 
action values: 1 
buys: 0 
player value: 0 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [15.  1.  0.  6.  3.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11] -> size -> 39 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: 11.018938064575195



action possibilites: [-1. 15.] 
expected returns: [[-23.148954]
 [-14.79222 ]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 15.  3.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15
  1 25 25 25 15 25  1 25  3 25 25 15  1  1] -> size -> 38 
action values: 1 
buys: 0 
player value: 1 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [15.  1.  0.  6.  3.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11] -> size -> 39 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 10.257943153381348



action possibilites: [-1] 
expected returns: [[-3.985661]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 3.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1] -> size -> 37 
action values: 0 
buys: 0 
player value: 4 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [15.  1.  0.  6.  3.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11] -> size -> 39 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[-5  0  0 60  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 95 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: -14.792236328125





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[ -6.62382   ]
 [  4.2220144 ]
 [ -1.5976197 ]
 [-65.592766  ]
 [ -0.62659526]
 [  3.4652724 ]
 [ -4.6889524 ]
 [ -0.16617751]
 [  1.2628427 ]
 [  5.8966484 ]
 [ -4.1269965 ]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1] -> size -> 37 
action values: 0 
buys: 1 
player value: 4 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  8.  1.  0.  9. 10. 10.  9.  5.] 
adversary cards in hand: [15.  1.  0.  6.  3.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11] -> size -> 39 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[-5  0  0 60  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 95 

action type: take_action - action -1
Learning step: 0
desired expected reward: -3.985661029815674



buy possibilites: [-1] 
expected returns: [[0.97720766]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15] -> size -> 38 
action values: 0 
buys: 0 
player value: 0 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  8.  1.  0.  9. 10. 10.  9.  4.] 
adversary cards in hand: [15.  1.  0.  6.  3.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11] -> size -> 39 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[ -5   0   0  60   0   0  40   0   0   0   0 -30   0   0 128   0] 
sum of rewards: 193 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 5.896644592285156






Player: 1 
cards in hand: [15.  1.  0.  6.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  1.  0.  6.  3.] 
cards in discard: [ 6. 11.  0.  3.  6.  2.  1.] 
cards in deck: 27 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11] -> size -> 39 
action values: 1 
buys: 0 
player value: 0 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  8.  1.  0.  9. 10. 10.  9.  4.] 
adversary cards in hand: [29. 29. 15.  1. 29.] 
adversary cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15. 29. 15.  3.  3.] 
adversary owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15] -> size -> 38 
adversary victory points: 5
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  1.  0.  6.  3.] 
cards in discard: [ 6. 11.  0.  3.  6.  2.  1.] 
cards in deck: 27 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11] -> size -> 39 
action values: 0 
buys: 1 
player value: 3 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  8.  1.  0.  9. 10. 10.  9.  4.] 
adversary cards in hand: [29. 29. 15.  1. 29.] 
adversary cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15. 29. 15.  3.  3.] 
adversary owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15] -> size -> 38 
adversary victory points: 5
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  1.  0.  6.  3.] 
cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8.] 
cards in deck: 27 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8] -> size -> 40 
action values: 0 
buys: 0 
player value: 1 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  7.  1.  0.  9. 10. 10.  9.  4.] 
adversary cards in hand: [29. 29. 15.  1. 29.] 
adversary cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15. 29. 15.  3.  3.] 
adversary owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15] -> size -> 38 
adversary victory points: 5
player victory points: 3 





         -------------------- Turn: 32 -------------------- 
Player: 0 
cards in hand: [29. 29. 15.  1. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 15. 29.] 
expected returns: [[-10.151913  ]
 [  2.2293305 ]
 [  2.2293305 ]
 [ -0.89148307]
 [  2.2293305 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29. 15.  1. 29.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15. 29. 15.  3.  3.] 
cards in deck: 10 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15] -> size -> 38 
action values: 1 
buys: 0 
player value: 0 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  7.  1.  0.  9. 10. 10.  9.  4.] 
adversary cards in hand: [ 8.  0. 22.  0.  0.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8] -> size -> 40 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: 0.9772076606750488



action possibilites: [-1. 29. 15. 29. 29.] 
expected returns: [[ 3.7865796]
 [14.949057 ]
 [12.090943 ]
 [14.949057 ]
 [14.949057 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 15. 29. 29.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15. 29. 15.  3.  3.  1.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15] -> size -> 38 
action values: 1 
buys: 0 
player value: 1 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  7.  1.  0.  9. 10. 10.  9.  4.] 
adversary cards in hand: [ 8.  0. 22.  0.  0.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8] -> size -> 40 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -2.255439281463623



action possibilites: [-1. 15. 29. 29.] 
expected returns: [[ 2.7036257]
 [12.057972 ]
 [15.224173 ]
 [15.224173 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 29. 29.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15. 29. 15.  3.  3.  1.  3.] 
cards in deck: 8 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15] -> size -> 38 
action values: 1 
buys: 0 
player value: 2 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  7.  1.  0.  9. 10. 10.  9.  4.] 
adversary cards in hand: [ 8.  0. 22.  0.  0.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8] -> size -> 40 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[-5  0  0 60  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 95 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 10.878371238708496



action possibilites: [-1. 15. 29.] 
expected returns: [[-3.0072446]
 [ 6.7743587]
 [ 9.982887 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 29.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15. 29. 15.  3.  3.  1.  3. 25.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15] -> size -> 38 
action values: 1 
buys: 0 
player value: 3 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  7.  1.  0.  9. 10. 10.  9.  4.] 
adversary cards in hand: [ 8.  0. 22.  0.  0.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8] -> size -> 40 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[-5  0  0 60  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 115 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 10.607808113098145



action possibilites: [-1.] 
expected returns: [[8.007575]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15. 29. 15.  3.  3.  1.  3. 25. 15.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15] -> size -> 38 
action values: 1 
buys: 0 
player value: 4 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  7.  1.  0.  9. 10. 10.  9.  4.] 
adversary cards in hand: [ 8.  0. 22.  0.  0.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8] -> size -> 40 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[-5  0  0 60  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 135 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 5.242026329040527





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[  5.762083  ]
 [ 15.915152  ]
 [-16.531271  ]
 [ 10.4177    ]
 [-10.782394  ]
 [-37.812263  ]
 [ 11.087458  ]
 [ 14.926748  ]
 [  7.288269  ]
 [ 17.587667  ]
 [ 11.636126  ]
 [ 16.374693  ]
 [ 12.865734  ]
 [ -0.27116466]
 [ 17.382391  ]
 [  7.7821913 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15. 29. 15.  3.  3.  1.  3. 25. 15.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15] -> size -> 38 
action values: 0 
buys: 1 
player value: 6 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  7.  1.  0.  9. 10. 10.  9.  4.] 
adversary cards in hand: [ 8.  0. 22.  0.  0.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8] -> size -> 40 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[-5  0  0 60  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 135 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 8.007575035095215



buy possibilites: [-1] 
expected returns: [[-22.926893]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15. 29. 15.  3.  3.  1.  3. 25. 15. 25.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25] -> size -> 39 
action values: 0 
buys: 0 
player value: 1 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  7.  0.  0.  9. 10. 10.  9.  4.] 
adversary cards in hand: [ 8.  0. 22.  0.  0.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8] -> size -> 40 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[ -5.    0.    0.   60.    0.    0.   80.    0.    0.    0.    0.  -40.
   0.    0.   62.5   0. ] 
sum of rewards: 157.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 17.587665557861328






Player: 1 
cards in hand: [ 8.  0. 22.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8. 22.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 8.  0. 22.  0.  0.] 
cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.] 
cards in deck: 22 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8] -> size -> 40 
action values: 1 
buys: 0 
player value: 0 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  7.  0.  0.  9. 10. 10.  9.  4.] 
adversary cards in hand: [ 1. 25. 29.  0. 15.] 
adversary cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15. 29. 15.  3.  3.  1.  3. 25. 15. 25. 29. 29. 29. 29.  1.] 
adversary owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25] -> size -> 39 
adversary victory points: 5
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 8.  0. 22.  0.  0.] 
cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.] 
cards in deck: 22 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8] -> size -> 40 
action values: 0 
buys: 1 
player value: 3 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  7.  0.  0.  9. 10. 10.  9.  4.] 
adversary cards in hand: [ 1. 25. 29.  0. 15.] 
adversary cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15. 29. 15.  3.  3.  1.  3. 25. 15. 25. 29. 29. 29. 29.  1.] 
adversary owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25] -> size -> 39 
adversary victory points: 5
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 8.  0. 22.  0.  0.] 
cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.] 
cards in deck: 22 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8] -> size -> 41 
action values: 0 
buys: 0 
player value: 1 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  6.  0.  0.  9. 10. 10.  9.  4.] 
adversary cards in hand: [ 1. 25. 29.  0. 15.] 
adversary cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15. 29. 15.  3.  3.  1.  3. 25. 15. 25. 29. 29. 29. 29.  1.] 
adversary owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25] -> size -> 39 
adversary victory points: 5
player victory points: 3 





         -------------------- Turn: 33 -------------------- 
Player: 0 
cards in hand: [ 1. 25. 29.  0. 15.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 29. 15.] 
expected returns: [[-14.926898 ]
 [ -5.638686 ]
 [ -2.5970302]
 [ -5.832632 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 25. 29.  0. 15.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15. 29. 15.  3.  3.  1.  3. 25. 15. 25. 29. 29. 29. 29.  1.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25] -> size -> 39 
action values: 1 
buys: 0 
player value: 0 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  6.  0.  0.  9. 10. 10.  9.  4.] 
adversary cards in hand: [ 6.  3. 15. 14.  3.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8] -> size -> 41 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: -22.92689323425293



action possibilites: [-1.] 
expected returns: [[-5.805219]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 0. 3.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15. 29. 15.  3.  3.  1.  3. 25. 15. 25. 29. 29. 29. 29.  1. 25. 15.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25] -> size -> 39 
action values: 1 
buys: 0 
player value: 1 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  6.  0.  0.  9. 10. 10.  9.  4.] 
adversary cards in hand: [ 6.  3. 15. 14.  3.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8] -> size -> 41 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: discard_n_cards - action 5
Learning step: 0
desired expected reward: -4.3671698570251465





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[ -7.618514  ]
 [  3.0804935 ]
 [ -2.6910603 ]
 [-53.06208   ]
 [ -2.1266649 ]
 [  2.1376328 ]
 [ -5.7132983 ]
 [ -1.4224869 ]
 [ -0.14453769]
 [  4.7453556 ]
 [ -5.8052173 ]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 0. 3.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15. 29. 15.  3.  3.  1.  3. 25. 15. 25. 29. 29. 29. 29.  1. 25. 15.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25] -> size -> 39 
action values: 0 
buys: 1 
player value: 4 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  6.  0.  0.  9. 10. 10.  9.  4.] 
adversary cards in hand: [ 6.  3. 15. 14.  3.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8] -> size -> 41 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -5.8052191734313965



buy possibilites: [-1] 
expected returns: [[-3.1184013]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 0. 3.] 
cards in discard: [ 1.  1. 29. 25. 25.  3.  0.  0. 25. 25.  1.  1. 25.  1. 25.  0. 29. 29.
 15. 29. 15.  3.  3.  1.  3. 25. 15. 25. 29. 29. 29. 29.  1. 25. 15. 15.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15] -> size -> 40 
action values: 0 
buys: 0 
player value: 0 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  6.  0.  0.  9. 10. 10.  9.  3.] 
adversary cards in hand: [ 6.  3. 15. 14.  3.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8] -> size -> 41 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[ -5   0   0  60   0   0  20   0   0   0   0 -50   0   0 128   0] 
sum of rewards: 153 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 4.745365142822266






Player: 1 
cards in hand: [ 6.  3. 15. 14.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 14.] 
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 6.  3. 15. 14.  3.] 
cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0.] 
cards in deck: 17 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8] -> size -> 41 
action values: 1 
buys: 0 
player value: 0 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  6.  0.  0.  9. 10. 10.  9.  3.] 
adversary cards in hand: [25.  3.  0. 25.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15] -> size -> 40 
adversary victory points: 5
player victory points: 3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 6.  3. 14.  3.] 
cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0.] 
cards in deck: 17 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8] -> size -> 41 
action values: 0 
buys: 0 
player value: 0 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  6.  0.  0.  9. 10. 10.  9.  3.] 
adversary cards in hand: [25.  3.  0. 25.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15] -> size -> 40 
adversary victory points: 5
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 6.  3. 14.  3.] 
cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0.] 
cards in deck: 17 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8] -> size -> 41 
action values: 0 
buys: 1 
player value: 0 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  6.  0.  0.  9. 10. 10.  9.  3.] 
adversary cards in hand: [25.  3.  0. 25.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15] -> size -> 40 
adversary victory points: 5
player victory points: 3 





         -------------------- Turn: 34 -------------------- 
Player: 0 
cards in hand: [25.  3.  0. 25.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 25.] 
expected returns: [[-4.8542814]
 [ 4.5321045]
 [ 4.5321045]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  3.  0. 25.  0.] 
cards in discard: [] 
cards in deck: 35 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15] -> size -> 40 
action values: 1 
buys: 0 
player value: 0 
card supply: [22. 20. 29. 23. 29.  8.  3.  9.  8.  6.  0.  0.  9. 10. 10.  9.  3.] 
adversary cards in hand: [3. 6. 0. 0. 3.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8] -> size -> 41 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: -3.118401288986206



action possibilites: [-1] 
expected returns: [[47.866722]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 25.  0. 15.  0.] 
cards in discard: [] 
cards in deck: 33 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15] -> size -> 40 
action values: 0 
buys: 0 
player value: 0 
card supply: [22. 20. 29. 23. 29.  8.  2.  9.  8.  6.  0.  0.  9. 10. 10.  9.  3.] 
adversary cards in hand: [3. 6. 0. 0. 3.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6] -> size -> 42 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 4.53212308883667





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[45.61399  ]
 [56.7564   ]
 [50.72567  ]
 [-2.3294742]
 [55.88746  ]
 [47.77796  ]
 [53.548553 ]
 [47.77954  ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0. 25.  0. 15.  0.] 
cards in discard: [] 
cards in deck: 33 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15] -> size -> 40 
action values: 0 
buys: 1 
player value: 3 
card supply: [22. 20. 29. 23. 29.  8.  2.  9.  8.  6.  0.  0.  9. 10. 10.  9.  3.] 
adversary cards in hand: [3. 6. 0. 0. 3.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6] -> size -> 42 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: take_action - action -1
Learning step: 0
desired expected reward: 47.866722106933594



buy possibilites: [-1] 
expected returns: [[19.07282]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0. 25.  0. 15.  0.] 
cards in discard: [1.] 
cards in deck: 33 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1] -> size -> 41 
action values: 0 
buys: 0 
player value: 0 
card supply: [22. 19. 29. 23. 29.  8.  2.  9.  8.  6.  0.  0.  9. 10. 10.  9.  3.] 
adversary cards in hand: [3. 6. 0. 0. 3.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6] -> size -> 42 
adversary victory points: 3
player victory points: 5 

Reward from previous game state: 
[ -5   0   0  60   0   0  20   0   0   0   0 -60   0   0  54   0] 
sum of rewards: 69 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 56.75641632080078






Player: 1 
cards in hand: [3. 6. 0. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 6. 0. 0. 3.] 
cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.] 
cards in deck: 12 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6] -> size -> 42 
action values: 1 
buys: 0 
player value: 0 
card supply: [22. 19. 29. 23. 29.  8.  2.  9.  8.  6.  0.  0.  9. 10. 10.  9.  3.] 
adversary cards in hand: [25. 15.  3.  1. 29.] 
adversary cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0.] 
adversary owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1] -> size -> 41 
adversary victory points: 5
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 6. 0. 0. 3.] 
cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.] 
cards in deck: 12 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6] -> size -> 42 
action values: 0 
buys: 1 
player value: 2 
card supply: [22. 19. 29. 23. 29.  8.  2.  9.  8.  6.  0.  0.  9. 10. 10.  9.  3.] 
adversary cards in hand: [25. 15.  3.  1. 29.] 
adversary cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0.] 
adversary owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1] -> size -> 41 
adversary victory points: 5
player victory points: 2 


 --------- NOTHING HAPPENED --------- 



         -------------------- Turn: 35 -------------------- 
Player: 0 
cards in hand: [25. 15.  3.  1. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 15. 29.] 
expected returns: [[-21.375486]
 [-11.503377]
 [-11.710326]
 [ -8.464851]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 15.  3.  1. 29.] 
cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0.] 
cards in deck: 28 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1] -> size -> 41 
action values: 1 
buys: 0 
player value: 0 
card supply: [22. 19. 29. 23. 29.  8.  2.  9.  8.  6.  0.  0.  9. 10. 10.  9.  3.] 
adversary cards in hand: [ 6. 11. 25.  8.  0.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.  3.  6.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6] -> size -> 42 
adversary victory points: 2
player victory points: 5 

Reward from previous game state: 
[-5  0  0 90  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: buy - action -1
Learning step: 0
desired expected reward: 19.07282066345215



action possibilites: [-1. 25. 15.] 
expected returns: [[-25.16395 ]
 [-15.061533]
 [-15.272036]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  1. 15.] 
cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3.] 
cards in deck: 27 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1] -> size -> 41 
action values: 1 
buys: 0 
player value: 1 
card supply: [22. 19. 29. 23. 29.  8.  2.  9.  8.  6.  0.  0.  9. 10. 10.  9.  3.] 
adversary cards in hand: [ 6. 11. 25.  8.  0.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.  3.  6.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6] -> size -> 42 
adversary victory points: 2
player victory points: 5 

Reward from previous game state: 
[-5  0  0 90  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 105 

action type: discard_n_cards - action 5
Learning step: 0
desired expected reward: -10.16671085357666



action possibilites: [-1] 
expected returns: [[3.770207]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 15.  3.  1.] 
cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3.] 
cards in deck: 25 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1] -> size -> 41 
action values: 0 
buys: 0 
player value: 1 
card supply: [22. 19. 29. 23. 29.  8.  1.  9.  8.  6.  0.  0.  9. 10. 10.  9.  3.] 
adversary cards in hand: [ 6. 11. 25.  8.  0.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.  3.  6.  0.  0.  3.  6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6] -> size -> 43 
adversary victory points: 2
player victory points: 5 

Reward from previous game state: 
[-5  0  0 90  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 125 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: -15.06153678894043





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 14. 23. 10. 22. 15. -1.] 
expected returns: [[  3.150478 ]
 [ 13.741728 ]
 [  8.035337 ]
 [-14.072193 ]
 [-42.12674  ]
 [  8.632811 ]
 [ 12.8186   ]
 [  4.976489 ]
 [  9.299928 ]
 [ 14.27614  ]
 [ 10.572697 ]
 [ -3.1281362]
 [ 15.389011 ]
 [  5.033098 ]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1. 15.  3.  1.] 
cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3.] 
cards in deck: 25 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1] -> size -> 41 
action values: 0 
buys: 1 
player value: 5 
card supply: [22. 19. 29. 23. 29.  8.  1.  9.  8.  6.  0.  0.  9. 10. 10.  9.  3.] 
adversary cards in hand: [ 6. 11. 25.  8.  0.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.  3.  6.  0.  0.  3.  6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6] -> size -> 43 
adversary victory points: 2
player victory points: 5 

Reward from previous game state: 
[-5  0  0 90  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 125 

action type: take_action - action -1
Learning step: 0
desired expected reward: 3.770206928253174



buy possibilites: [-1] 
expected returns: [[0.4417708]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1. 15.  3.  1.] 
cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3. 15.] 
cards in deck: 25 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1 15] -> size -> 42 
action values: 0 
buys: 0 
player value: 1 
card supply: [22. 19. 29. 23. 29.  8.  1.  9.  8.  6.  0.  0.  9. 10. 10.  9.  2.] 
adversary cards in hand: [ 6. 11. 25.  8.  0.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.  3.  6.  0.  0.  3.  6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6] -> size -> 43 
adversary victory points: 2
player victory points: 5 

Reward from previous game state: 
[ -5.   0.   0.  90.   0.   0.  40.   0.   0.   0.   0. -70.   0.   0.
  32.   0.] 
sum of rewards: 87.0 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 15.38901138305664






Player: 1 
cards in hand: [ 6. 11. 25.  8.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11. 25.  8.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 6. 11. 25.  8.  0.] 
cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.  3.  6.  0.  0.  3.  6.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6] -> size -> 43 
action values: 1 
buys: 0 
player value: 0 
card supply: [22. 19. 29. 23. 29.  8.  1.  9.  8.  6.  0.  0.  9. 10. 10.  9.  2.] 
adversary cards in hand: [ 1. 25. 29. 29.  1.] 
adversary cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3. 15. 29. 25.  1. 15.  3.  1.] 
adversary owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1 15] -> size -> 42 
adversary victory points: 5
player victory points: 1 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 6. 11. 25.  8.  0.] 
cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.  3.  6.  0.  0.  3.  6.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6] -> size -> 43 
action values: 0 
buys: 1 
player value: 1 
card supply: [22. 19. 29. 23. 29.  8.  1.  9.  8.  6.  0.  0.  9. 10. 10.  9.  2.] 
adversary cards in hand: [ 1. 25. 29. 29.  1.] 
adversary cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3. 15. 29. 25.  1. 15.  3.  1.] 
adversary owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1 15] -> size -> 42 
adversary victory points: 5
player victory points: 1 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 6. 11. 25.  8.  0.] 
cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.  3.  6.  0.  0.  3.  6.  0.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6  0] -> size -> 44 
action values: 0 
buys: 0 
player value: 1 
card supply: [21. 19. 29. 23. 29.  8.  1.  9.  8.  6.  0.  0.  9. 10. 10.  9.  2.] 
adversary cards in hand: [ 1. 25. 29. 29.  1.] 
adversary cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3. 15. 29. 25.  1. 15.  3.  1.] 
adversary owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1 15] -> size -> 42 
adversary victory points: 5
player victory points: 1 





         -------------------- Turn: 36 -------------------- 
Player: 0 
cards in hand: [ 1. 25. 29. 29.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 29. 29.] 
expected returns: [[-38.557632]
 [-29.032297]
 [-26.255613]
 [-26.255613]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 25. 29. 29.  1.] 
cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3. 15. 29. 25.  1. 15.  3.  1.] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1 15] -> size -> 42 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 19. 29. 23. 29.  8.  1.  9.  8.  6.  0.  0.  9. 10. 10.  9.  2.] 
adversary cards in hand: [ 0.  0.  4. 16.  0.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.  3.  6.  0.  0.  3.  6.  0.  6. 11. 25.  8.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6  0] -> size -> 44 
adversary victory points: 1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 120   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 115 

action type: buy - action -1
Learning step: 0
desired expected reward: 0.4417707920074463



action possibilites: [-1. 29.] 
expected returns: [[-34.541252]
 [-22.91358 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  1.  1.] 
cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3. 15. 29. 25.  1. 15.  3.  1.  1.
 25.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1 15] -> size -> 42 
action values: 1 
buys: 0 
player value: 1 
card supply: [21. 19. 29. 23. 29.  8.  1.  9.  8.  6.  0.  0.  9. 10. 10.  9.  2.] 
adversary cards in hand: [ 0.  0.  4. 16.  0.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.  3.  6.  0.  0.  3.  6.  0.  6. 11. 25.  8.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6  0] -> size -> 44 
adversary victory points: 1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 120   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 135 

action type: discard_n_cards - action 5
Learning step: 0
desired expected reward: -27.97066307067871



action possibilites: [-1.] 
expected returns: [[-29.727142]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1.] 
cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3. 15. 29. 25.  1. 15.  3.  1.  1.
 25.  1.  1.] 
cards in deck: 18 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1 15] -> size -> 42 
action values: 1 
buys: 0 
player value: 2 
card supply: [21. 19. 29. 23. 29.  8.  1.  9.  8.  6.  0.  0.  9. 10. 10.  9.  2.] 
adversary cards in hand: [ 0.  0.  4. 16.  0.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.  3.  6.  0.  0.  3.  6.  0.  6. 11. 25.  8.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6  0] -> size -> 44 
adversary victory points: 1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 120   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 155 

action type: discard_n_cards - action 0
Learning step: 0
desired expected reward: -36.991477966308594





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[-31.101988]
 [-19.666862]
 [-25.856318]
 [-76.455826]
 [-25.261873]
 [-20.525675]
 [-28.620857]
 [-24.462656]
 [-23.065565]
 [-17.74572 ]
 [-29.330307]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1.] 
cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3. 15. 29. 25.  1. 15.  3.  1.  1.
 25.  1.  1.] 
cards in deck: 18 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1 15] -> size -> 42 
action values: 0 
buys: 1 
player value: 4 
card supply: [21. 19. 29. 23. 29.  8.  1.  9.  8.  6.  0.  0.  9. 10. 10.  9.  2.] 
adversary cards in hand: [ 0.  0.  4. 16.  0.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.  3.  6.  0.  0.  3.  6.  0.  6. 11. 25.  8.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6  0] -> size -> 44 
adversary victory points: 1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 120   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 155 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -29.727142333984375



buy possibilites: [-1] 
expected returns: [[-36.005707]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1.] 
cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3. 15. 29. 25.  1. 15.  3.  1.  1.
 25.  1.  1. 15.] 
cards in deck: 18 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1 15 15] -> size -> 43 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 19. 29. 23. 29.  8.  1.  9.  8.  6.  0.  0.  9. 10. 10.  9.  1.] 
adversary cards in hand: [ 0.  0.  4. 16.  0.] 
adversary cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.  3.  6.  0.  0.  3.  6.  0.  6. 11. 25.  8.
  0.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6  0] -> size -> 44 
adversary victory points: 1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 120   0   0  40   0   0   0   0 -80   0   0 128   0] 
sum of rewards: 203 

action type: buy - action 15.0
Learning step: 0
desired expected reward: -17.7457218170166






Player: 1 
cards in hand: [ 0.  0.  4. 16.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  4. 16.  0.] 
cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.  3.  6.  0.  0.  3.  6.  0.  6. 11. 25.  8.
  0.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6  0] -> size -> 44 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 19. 29. 23. 29.  8.  1.  9.  8.  6.  0.  0.  9. 10. 10.  9.  1.] 
adversary cards in hand: [15. 29. 25. 29. 25.] 
adversary cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3. 15. 29. 25.  1. 15.  3.  1.  1.
 25.  1.  1. 15. 29. 29.  1.] 
adversary owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1 15 15] -> size -> 43 
adversary victory points: 5
player victory points: 1 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 11.0 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0.  4. 16.  0.] 
cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.  3.  6.  0.  0.  3.  6.  0.  6. 11. 25.  8.
  0.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6  0] -> size -> 44 
action values: 0 
buys: 1 
player value: 3 
card supply: [21. 19. 29. 23. 29.  8.  1.  9.  8.  6.  0.  0.  9. 10. 10.  9.  1.] 
adversary cards in hand: [15. 29. 25. 29. 25.] 
adversary cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3. 15. 29. 25.  1. 15.  3.  1.  1.
 25.  1.  1. 15. 29. 29.  1.] 
adversary owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1 15 15] -> size -> 43 
adversary victory points: 5
player victory points: 1 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0.  4. 16.  0.] 
cards in discard: [ 6. 11.  0.  3.  6.  2.  1.  8. 15.  1.  0.  6.  3.  8.  8.  0. 22.  0.
  0. 15.  6.  3. 14.  3.  6.  3.  6.  0.  0.  3.  6.  0.  6. 11. 25.  8.
  0. 11.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6  0 11] -> size -> 45 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 19. 29. 23. 29.  8.  1.  9.  7.  6.  0.  0.  9. 10. 10.  9.  1.] 
adversary cards in hand: [15. 29. 25. 29. 25.] 
adversary cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3. 15. 29. 25.  1. 15.  3.  1.  1.
 25.  1.  1. 15. 29. 29.  1.] 
adversary owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1 15 15] -> size -> 43 
adversary victory points: 5
player victory points: 1 





         -------------------- Turn: 37 -------------------- 
Player: 0 
cards in hand: [15. 29. 25. 29. 25.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 29. 25. 29. 25.] 
expected returns: [[-31.470602]
 [-22.538717]
 [-19.563644]
 [-22.3482  ]
 [-19.563644]
 [-22.3482  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 29. 25. 29. 25.] 
cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3. 15. 29. 25.  1. 15.  3.  1.  1.
 25.  1.  1. 15. 29. 29.  1.] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1 15 15] -> size -> 43 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 19. 29. 23. 29.  8.  1.  9.  7.  6.  0.  0.  9. 10. 10.  9.  1.] 
adversary cards in hand: [3. 4. 0. 6. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6  0 11] -> size -> 45 
adversary victory points: 1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 120   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 115 

action type: buy - action -1
Learning step: 0
desired expected reward: -36.005706787109375



action possibilites: [-1. 29. 25. 15.] 
expected returns: [[-32.78875 ]
 [-21.070124]
 [-23.852411]
 [-24.03985 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 25. 15.] 
cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3. 15. 29. 25.  1. 15.  3.  1.  1.
 25.  1.  1. 15. 29. 29.  1. 15. 25.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1 15 15] -> size -> 43 
action values: 1 
buys: 0 
player value: 1 
card supply: [21. 19. 29. 23. 29.  8.  1.  9.  7.  6.  0.  0.  9. 10. 10.  9.  1.] 
adversary cards in hand: [3. 4. 0. 6. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6  0 11] -> size -> 45 
adversary victory points: 1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 120   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 135 

action type: discard_n_cards - action 5
Learning step: 0
desired expected reward: -21.156579971313477



action possibilites: [-1. 25.] 
expected returns: [[-6.962596 ]
 [ 2.1806087]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.] 
cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3. 15. 29. 25.  1. 15.  3.  1.  1.
 25.  1.  1. 15. 29. 29.  1. 15. 25. 15.  1.] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1 15 15] -> size -> 43 
action values: 1 
buys: 0 
player value: 2 
card supply: [21. 19. 29. 23. 29.  8.  1.  9.  7.  6.  0.  0.  9. 10. 10.  9.  1.] 
adversary cards in hand: [3. 4. 0. 6. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6  0 11] -> size -> 45 
adversary victory points: 1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 120   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 155 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -25.360462188720703



Player 0 won the game! 



Player 0 bought cards:
Copper: 0 
Silver: 9 
Gold: 0 
Estate: 2 
Duchy: 0 
Province: 0 
Curse: 0 

Remodel: 0 
Workshop: 0 
Chapel: 0 
Witch: 9 
Poacher: 9 
Militia: 0 
Market: 0 
Village: 0 
Library: 0 
Moneylender: 7 


 --- GAME STATE ---
Action that is taken: None 
cards in hand: [ 3. 25.] 
cards in discard: [ 1. 25.  3.  0. 25.  0. 15.  0. 15.  3. 15. 29. 25.  1. 15.  3.  1.  1.
 25.  1.  1. 15. 29. 29.  1. 15. 25. 15.  1.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  3  3  3  3 29 29 29 29 29 29  1 29 29  1 29 25  1  1 15  1
 25 25 25 15 25  1 25  3 25 25 15  1  1 15 25 15  1 15 15] -> size -> 43 
action values: 0 
buys: 0 
player value: 2 
card supply: [21. 19. 29. 23. 29.  8.  0.  9.  7.  6.  0.  0.  9. 10. 10.  9.  1.] 
adversary cards in hand: [3. 4. 0. 6. 3.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 0  0  0  3  3  0 14  3  1  0 16  1 15  8  6 11 25 15  3 22  0  4  6  3
  2  6  0  8  0  6  3  0  0  6  0  6  3  6 11  8  8  6  6  0 11  6] -> size -> 46 
adversary victory points: 1
player victory points: 5 

Reward from previous game state: 
[     -5 3000000       0     120       0       0      60       0       0
       0       0       0       0       0       0       0] 
sum of rewards: 3000175 

action type: take_action - action 25.0
Learning step: 120006.90625
desired expected reward: 120009.0859375



