 --------- CARDS IN THIS GAME --------- 
Game cards: [['Copper' '0' '0']
 ['Silver' '1' '3']
 ['Gold' '2' '6']
 ['Estate' '3' '2']
 ['Duchy' '4' '5']
 ['Province' '5' '8']
 ['Curse' '6' '0']
 ['Remodel' '16' '4']
 ['Workshop' '11' '3']
 ['Chapel' '8' '2']
 ['Witch' '25' '5']
 ['Poacher' '29' '4']
 ['Militia' '14' '4']
 ['Market' '23' '5']
 ['Village' '10' '3']
 ['Library' '22' '5']
 ['Moneylender' '15' '4']]






         -------------------- Turn: 1 -------------------- 
Player: 1 
cards in hand: [3. 3. 0. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 0 
buys: 1 
player value: 3 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 0. 0. 0.] 
cards in discard: [0.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3 0] -> size -> 11 
action values: 0 
buys: 0 
player value: 3 
card supply: [29. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 3 





Player: 0 
cards in hand: [3. 0. 0. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[5.488722]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 0.] 
adversary cards in discard: [0. 3. 3. 0. 0. 0.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 0] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[      -5 -3000000        0      -90        0        0        0        0
        0        0        0     -150        0        0       27        0] 
sum of rewards: -3000218 

action type: buy - action 10.0
Learning step: -120006.9296875
desired expected reward: -120051.625





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
expected returns: [[  2.9326622]
 [ 12.85703  ]
 [  6.9634085]
 [-29.319473 ]
 [  7.92941  ]
 [ 11.731025 ]
 [  6.049945 ]
 [ 19.325018 ]
 [  5.209901 ]
 [  6.975362 ]
 [ 14.113448 ]
 [  4.0974255]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 0 
buys: 1 
player value: 4 
card supply: [29. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 0.] 
adversary cards in discard: [0. 3. 3. 0. 0. 0.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 0] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 5.539068222045898



buy possibilites: [-1] 
expected returns: [[8.422059]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 0.] 
cards in discard: [29.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 0.] 
adversary cards in discard: [0. 3. 3. 0. 0. 0.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 0] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0   0   0   0   0   0   0   0   0   0   0   0 128   0] 
sum of rewards: 123 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 19.32501983642578






         -------------------- Turn: 2 -------------------- 
Player: 1 
cards in hand: [0. 0. 3. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3. 0. 0.] 
cards in discard: [0. 3. 3. 0. 0. 0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3 0] -> size -> 11 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 3.] 
adversary cards in discard: [29.  3.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: 16.0 : ['Remodel' '16' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 0.] 
cards in discard: [0. 3. 3. 0. 0. 0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3 0] -> size -> 11 
action values: 0 
buys: 1 
player value: 4 
card supply: [29. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 3.] 
adversary cards in discard: [29.  3.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 0.] 
cards in discard: [ 0.  3.  3.  0.  0.  0. 16.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  0 16] -> size -> 12 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 30. 30. 30. 30.  8. 10.  9. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 3.] 
adversary cards in discard: [29.  3.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
adversary victory points: 3
player victory points: 3 





Player: 0 
cards in hand: [3. 0. 0. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[-3.2912235]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 0. 3.] 
cards in discard: [29.  3.  0.  0.  0.  0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 30. 30. 30. 30.  8. 10.  9. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  0 16] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 8.422059059143066





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[ -4.4457397 ]
 [  5.8371964 ]
 [ -0.27639103]
 [-37.349403  ]
 [  4.5751057 ]
 [ -1.2899956 ]
 [ -0.33157134]
 [ -3.303141  ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 3.] 
cards in discard: [29.  3.  0.  0.  0.  0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29] -> size -> 11 
action values: 0 
buys: 1 
player value: 3 
card supply: [29. 30. 30. 30. 30.  8. 10.  9. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  0 16] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -5.025006294250488



buy possibilites: [-1] 
expected returns: [[-4.038511]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 3.] 
cards in discard: [29.  3.  0.  0.  0.  0.  1.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1] -> size -> 12 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 30. 30.  8. 10.  9. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  0 16] -> size -> 12 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0 54  0] 
sum of rewards: 49 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 5.837203025817871






         -------------------- Turn: 3 -------------------- 
Player: 1 
cards in hand: [3. 0. 0. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  0 16] -> size -> 12 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 30. 30.  8. 10.  9. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1] -> size -> 12 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: 11.0 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  0 16] -> size -> 12 
action values: 0 
buys: 1 
player value: 4 
card supply: [29. 29. 30. 30. 30.  8. 10.  9. 10. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1] -> size -> 12 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 0.] 
cards in discard: [11.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  0 16 11] -> size -> 13 
action values: 0 
buys: 0 
player value: 1 
card supply: [29. 29. 30. 30. 30.  8. 10.  9.  9. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1] -> size -> 12 
adversary victory points: 3
player victory points: 3 





Player: 0 
cards in hand: [3. 0. 0. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[11.651895]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1] -> size -> 12 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 30. 30.  8. 10.  9.  9. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [16.  0.  3.  0.  0.] 
adversary cards in discard: [11.  3.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  0 16 11] -> size -> 13 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: -4.038510799407959





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[  9.596409]
 [ 19.864956]
 [ 13.690129]
 [-25.588713]
 [ 18.94402 ]
 [ 13.056903]
 [ 13.995154]
 [ 11.110074]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1] -> size -> 12 
action values: 0 
buys: 1 
player value: 3 
card supply: [29. 29. 30. 30. 30.  8. 10.  9.  9. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [16.  0.  3.  0.  0.] 
adversary cards in discard: [11.  3.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  0 16 11] -> size -> 13 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 10.42491626739502



buy possibilites: [-1] 
expected returns: [[4.4874287]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 3.] 
cards in discard: [1.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1] -> size -> 13 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 30. 30.  8. 10.  9.  9. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [16.  0.  3.  0.  0.] 
adversary cards in discard: [11.  3.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  0 16 11] -> size -> 13 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0 54  0] 
sum of rewards: 49 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 19.864953994750977






         -------------------- Turn: 4 -------------------- 
Player: 1 
cards in hand: [16.  0.  3.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16.] 
Chosen action: 16 : ['Remodel' '16' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [16.  0.  3.  0.  0.] 
cards in discard: [11.  3.  0.  0.  0.  0.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  0 16 11] -> size -> 13 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 30. 30.  8. 10.  9.  9. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 29.  0.  0.  1.] 
adversary cards in discard: [1. 3. 0. 0. 0. 3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1] -> size -> 13 
adversary victory points: 3
player victory points: 3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0.] 
cards in discard: [11.  3.  0.  0.  0.  0.  6.] 
cards in deck: 2 
card top of deck: [] 
played cards: [16.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6] -> size -> 13 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 30. 30.  8.  9.  9.  9. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 29.  0.  0.  1.] 
adversary cards in discard: [1. 3. 0. 0. 0. 3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1] -> size -> 13 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0.] 
cards in discard: [11.  3.  0.  0.  0.  0.  6.] 
cards in deck: 2 
card top of deck: [] 
played cards: [16.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6] -> size -> 13 
action values: 0 
buys: 1 
player value: 2 
card supply: [29. 28. 30. 30. 30.  8.  9.  9.  9. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 29.  0.  0.  1.] 
adversary cards in discard: [1. 3. 0. 0. 0. 3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1] -> size -> 13 
adversary victory points: 3
player victory points: 2 





Player: 0 
cards in hand: [ 0. 29.  0.  0.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[-6.124543]
 [ 9.187757]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29.  0.  0.  1.] 
cards in discard: [1. 3. 0. 0. 0. 3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1] -> size -> 13 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 30. 30.  8.  9.  9.  9. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 6. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6] -> size -> 13 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 4.487428665161133



action possibilites: [-1.] 
expected returns: [[22.160376]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 1. 3.] 
cards in discard: [1. 3. 0. 0. 0. 3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1] -> size -> 13 
action values: 1 
buys: 0 
player value: 1 
card supply: [29. 28. 30. 30. 30.  8.  9.  9.  9. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 6. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6] -> size -> 13 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 6.40217399597168





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[24.58568  ]
 [34.65645  ]
 [ 5.8074694]
 [28.684153 ]
 [12.268691 ]
 [-8.887004 ]
 [29.768745 ]
 [33.63564  ]
 [27.79398  ]
 [38.7361   ]
 [41.21496  ]
 [26.93134  ]
 [35.954483 ]
 [28.793327 ]
 [14.534152 ]
 [35.94387  ]
 [25.892324 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 1. 3.] 
cards in discard: [1. 3. 0. 0. 0. 3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1] -> size -> 13 
action values: 0 
buys: 1 
player value: 6 
card supply: [29. 28. 30. 30. 30.  8.  9.  9.  9. 10. 10.  9. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 6. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6] -> size -> 13 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 22.160375595092773



buy possibilites: [-1] 
expected returns: [[-3.503972]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 1. 3.] 
cards in discard: [ 1.  3.  0.  0.  0.  3. 29.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29] -> size -> 14 
action values: 0 
buys: 0 
player value: 2 
card supply: [29. 28. 30. 30. 30.  8.  9.  9.  9. 10. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 6. 0. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6] -> size -> 13 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5.  0.  0. 30.  0.  0. 20.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 77.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 41.2149543762207






         -------------------- Turn: 5 -------------------- 
Player: 1 
cards in hand: [0. 6. 0. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 6. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6] -> size -> 13 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 30. 30.  8.  9.  9.  9. 10. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29] -> size -> 14 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 6. 0. 0. 3.] 
cards in discard: [] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6] -> size -> 13 
action values: 0 
buys: 1 
player value: 3 
card supply: [29. 28. 30. 30. 30.  8.  9.  9.  9. 10. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29] -> size -> 14 
adversary victory points: 3
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 6. 0. 0. 3.] 
cards in discard: [3.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3] -> size -> 14 
action values: 0 
buys: 0 
player value: 1 
card supply: [29. 28. 30. 29. 30.  8.  9.  9.  9. 10. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29] -> size -> 14 
adversary victory points: 3
player victory points: 3 





Player: 0 
cards in hand: [0. 3. 0. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[7.8119955]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29] -> size -> 14 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 29. 30.  8.  9.  9.  9. 10. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3. 16.  3.  0. 11.] 
adversary cards in discard: [3. 0. 6. 0. 0. 3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3] -> size -> 14 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: -3.503972053527832





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
expected returns: [[  7.846534]
 [ 18.154142]
 [ 12.012677]
 [-26.112724]
 [ 13.162005]
 [ 17.12765 ]
 [ 11.281099]
 [ 24.747095]
 [ 10.225537]
 [ 12.197651]
 [ 19.477549]
 [  9.190124]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29] -> size -> 14 
action values: 0 
buys: 1 
player value: 4 
card supply: [29. 28. 30. 29. 30.  8.  9.  9.  9. 10. 10.  8. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3. 16.  3.  0. 11.] 
adversary cards in discard: [3. 0. 6. 0. 0. 3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3] -> size -> 14 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 5.950710296630859



buy possibilites: [-1] 
expected returns: [[11.48743]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [29.] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29] -> size -> 15 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 29. 30.  8.  9.  9.  9. 10. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3. 16.  3.  0. 11.] 
adversary cards in discard: [3. 0. 6. 0. 0. 3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3] -> size -> 14 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0   0   0   0   0   0   0   0   0   0   0   0 128   0] 
sum of rewards: 123 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 24.74710464477539






         -------------------- Turn: 6 -------------------- 
Player: 1 
cards in hand: [ 3. 16.  3.  0. 11.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16. 11.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 16.  3.  0. 11.] 
cards in discard: [3. 0. 6. 0. 0. 3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3] -> size -> 14 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 29. 30.  8.  9.  9.  9. 10. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3.  0.  3. 29.  0.] 
adversary cards in discard: [29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29] -> size -> 15 
adversary victory points: 3
player victory points: 3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 16.  3.  0.] 
cards in discard: [ 3.  0.  6.  0.  0.  3. 11.] 
cards in deck: 3 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11] -> size -> 15 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 29. 30.  8.  9.  9.  8. 10. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3.  0.  3. 29.  0.] 
adversary cards in discard: [29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29] -> size -> 15 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 16.  3.  0.] 
cards in discard: [ 3.  0.  6.  0.  0.  3. 11.] 
cards in deck: 3 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11] -> size -> 15 
action values: 0 
buys: 1 
player value: 1 
card supply: [29. 28. 30. 29. 30.  8.  9.  9.  8. 10. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3.  0.  3. 29.  0.] 
adversary cards in discard: [29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29] -> size -> 15 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 16.  3.  0.] 
cards in discard: [ 3.  0.  6.  0.  0.  3. 11.  0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0] -> size -> 16 
action values: 0 
buys: 0 
player value: 1 
card supply: [28. 28. 30. 29. 30.  8.  9.  9.  8. 10. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3.  0.  3. 29.  0.] 
adversary cards in discard: [29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29] -> size -> 15 
adversary victory points: 3
player victory points: 3 





Player: 0 
cards in hand: [ 3.  0.  3. 29.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[13.481111]
 [30.388721]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0.  3. 29.  0.] 
cards in discard: [29.  0.  3.  0.  0.  0.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29] -> size -> 15 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 28. 30. 29. 30.  8.  9.  9.  8. 10. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0] -> size -> 16 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 11.48742961883545



action possibilites: [-1.] 
expected returns: [[15.544877]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 3. 0. 0.] 
cards in discard: [29.  0.  3.  0.  0.  0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29] -> size -> 15 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 28. 30. 29. 30.  8.  9.  9.  8. 10. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0] -> size -> 16 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 27.921932220458984





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
expected returns: [[ 14.23862 ]
 [ 24.35809 ]
 [ 18.317421]
 [-18.620642]
 [ 19.422052]
 [ 23.260986]
 [ 17.440147]
 [ 30.845726]
 [ 16.576178]
 [ 18.442984]
 [ 25.630379]
 [ 15.559146]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 3. 0. 0.] 
cards in discard: [29.  0.  3.  0.  0.  0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29] -> size -> 15 
action values: 0 
buys: 1 
player value: 4 
card supply: [28. 28. 30. 29. 30.  8.  9.  9.  8. 10. 10.  7. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0] -> size -> 16 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 15.544877052307129



buy possibilites: [-1] 
expected returns: [[9.268746]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 3. 0. 0.] 
cards in discard: [29.  0.  3.  0.  0.  0. 29.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29] -> size -> 16 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 28. 30. 29. 30.  8.  9.  9.  8. 10. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0] -> size -> 16 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0   0   0   0  20   0   0   0   0   0   0   0 128   0] 
sum of rewards: 143 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 30.84572982788086






         -------------------- Turn: 7 -------------------- 
Player: 1 
cards in hand: [3. 0. 0. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0] -> size -> 16 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 28. 30. 29. 30.  8.  9.  9.  8. 10. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3.  0. 29.  1.  1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29] -> size -> 16 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0] -> size -> 16 
action values: 0 
buys: 1 
player value: 4 
card supply: [28. 28. 30. 29. 30.  8.  9.  9.  8. 10. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3.  0. 29.  1.  1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29] -> size -> 16 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 0.] 
cards in discard: [1.] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1] -> size -> 17 
action values: 0 
buys: 0 
player value: 1 
card supply: [28. 27. 30. 29. 30.  8.  9.  9.  8. 10. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3.  0. 29.  1.  1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29] -> size -> 16 
adversary victory points: 3
player victory points: 3 





Player: 0 
cards in hand: [ 3.  0. 29.  1.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[15.413497]
 [30.778759]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 29.  1.  1.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29] -> size -> 16 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 27. 30. 29. 30.  8.  9.  9.  8. 10. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 3. 0. 6.] 
adversary cards in discard: [1. 3. 0. 0. 0. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1] -> size -> 17 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 9.268746376037598



action possibilites: [-1.] 
expected returns: [[21.449083]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 1. 1. 3.] 
cards in discard: [] 
cards in deck: 10 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29] -> size -> 16 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 27. 30. 29. 30.  8.  9.  9.  8. 10. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 3. 0. 6.] 
adversary cards in discard: [1. 3. 0. 0. 0. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1] -> size -> 17 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 27.46426010131836





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 25.142166]
 [ 35.696236]
 [  4.021858]
 [ 29.32808 ]
 [ 10.957604]
 [-12.601248]
 [ 30.727901]
 [ 34.68649 ]
 [ 28.663515]
 [ 39.871662]
 [ 42.26286 ]
 [ 27.59508 ]
 [ 37.0233  ]
 [ 29.723732]
 [ 13.647786]
 [ 37.01695 ]
 [ 26.763657]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 1. 1. 3.] 
cards in discard: [] 
cards in deck: 10 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29] -> size -> 16 
action values: 0 
buys: 1 
player value: 6 
card supply: [28. 27. 30. 29. 30.  8.  9.  9.  8. 10. 10.  6. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 3. 0. 6.] 
adversary cards in discard: [1. 3. 0. 0. 0. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1] -> size -> 17 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 21.44908332824707



buy possibilites: [-1] 
expected returns: [[3.856231]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 1. 1. 3.] 
cards in discard: [29.] 
cards in deck: 10 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29] -> size -> 17 
action values: 0 
buys: 0 
player value: 2 
card supply: [28. 27. 30. 29. 30.  8.  9.  9.  8. 10. 10.  5. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 3. 3. 0. 6.] 
adversary cards in discard: [1. 3. 0. 0. 0. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1] -> size -> 17 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5.  0.  0.  0.  0.  0. 20.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 47.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 42.26286315917969






         -------------------- Turn: 8 -------------------- 
Player: 1 
cards in hand: [0. 3. 3. 0. 6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 3. 0. 6.] 
cards in discard: [1. 3. 0. 0. 0. 0.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1] -> size -> 17 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 27. 30. 29. 30.  8.  9.  9.  8. 10. 10.  5. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  0. 29. 29.  0.] 
adversary cards in discard: [29. 29.  3.  0.  1.  1.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29] -> size -> 17 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 3. 0. 6.] 
cards in discard: [1. 3. 0. 0. 0. 0.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1] -> size -> 17 
action values: 0 
buys: 1 
player value: 2 
card supply: [28. 27. 30. 29. 30.  8.  9.  9.  8. 10. 10.  5. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0.  0. 29. 29.  0.] 
adversary cards in discard: [29. 29.  3.  0.  1.  1.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29] -> size -> 17 
adversary victory points: 3
player victory points: 3 


 --------- NOTHING HAPPENED --------- 



Player: 0 
cards in hand: [ 0.  0. 29. 29.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[-9.161814 ]
 [ 4.8589773]
 [ 4.8589773]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0. 29. 29.  0.] 
cards in discard: [29. 29.  3.  0.  1.  1.  3.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29] -> size -> 17 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 27. 30. 29. 30.  8.  9.  9.  8. 10. 10.  5. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11. 16. 11.  3.  0.] 
adversary cards in discard: [1. 3. 0. 0. 0. 0. 0. 3. 3. 0. 6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1] -> size -> 17 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 3.8562309741973877



action possibilites: [-1. 29.] 
expected returns: [[-4.609646]
 [ 9.535073]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0. 29.  0.  3.] 
cards in discard: [29. 29.  3.  0.  1.  1.  3.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29] -> size -> 17 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 27. 30. 29. 30.  8.  9.  9.  8. 10. 10.  5. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11. 16. 11.  3.  0.] 
adversary cards in discard: [1. 3. 0. 0. 0. 0. 0. 3. 3. 0. 6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1] -> size -> 17 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 3.963116407394409



action possibilites: [-1.] 
expected returns: [[22.577465]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 3. 0.] 
cards in discard: [29. 29.  3.  0.  1.  1.  3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29] -> size -> 17 
action values: 1 
buys: 0 
player value: 2 
card supply: [28. 27. 30. 29. 30.  8.  9.  9.  8. 10. 10.  5. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11. 16. 11.  3.  0.] 
adversary cards in discard: [1. 3. 0. 0. 0. 0. 0. 3. 3. 0. 6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1] -> size -> 17 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 9.535075187683105





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 21.512587 ]
 [ 31.293    ]
 [  2.3309557]
 [ 25.449429 ]
 [  8.684305 ]
 [-13.262114 ]
 [ 26.689682 ]
 [ 30.370079 ]
 [ 24.58322  ]
 [ 35.17413  ]
 [ 37.37947  ]
 [ 23.829493 ]
 [ 32.52704  ]
 [ 25.715576 ]
 [ 11.128958 ]
 [ 32.52326  ]
 [ 23.01082  ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3. 0.] 
cards in discard: [29. 29.  3.  0.  1.  1.  3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29] -> size -> 17 
action values: 0 
buys: 1 
player value: 6 
card supply: [28. 27. 30. 29. 30.  8.  9.  9.  8. 10. 10.  5. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11. 16. 11.  3.  0.] 
adversary cards in discard: [1. 3. 0. 0. 0. 0. 0. 3. 3. 0. 6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1] -> size -> 17 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 22.577465057373047



buy possibilites: [-1] 
expected returns: [[30.924038]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3. 0.] 
cards in discard: [29. 29.  3.  0.  1.  1.  3. 29.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29] -> size -> 18 
action values: 0 
buys: 0 
player value: 2 
card supply: [28. 27. 30. 29. 30.  8.  9.  9.  8. 10. 10.  4. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11. 16. 11.  3.  0.] 
adversary cards in discard: [1. 3. 0. 0. 0. 0. 0. 3. 3. 0. 6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1] -> size -> 17 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5.  0.  0.  0.  0.  0. 40.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 67.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 37.37947463989258






         -------------------- Turn: 9 -------------------- 
Player: 1 
cards in hand: [11. 16. 11.  3.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11. 16. 11.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [11. 16. 11.  3.  0.] 
cards in discard: [1. 3. 0. 0. 0. 0. 0. 3. 3. 0. 6.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1] -> size -> 17 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 27. 30. 29. 30.  8.  9.  9.  8. 10. 10.  4. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 1.  3. 29.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29] -> size -> 18 
adversary victory points: 3
player victory points: 3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [16. 11.  3.  0.] 
cards in discard: [1. 3. 0. 0. 0. 0. 0. 3. 3. 0. 6. 6.] 
cards in deck: 1 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6] -> size -> 18 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  4. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 1.  3. 29.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29] -> size -> 18 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [16. 11.  3.  0.] 
cards in discard: [1. 3. 0. 0. 0. 0. 0. 3. 3. 0. 6. 6.] 
cards in deck: 1 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6] -> size -> 18 
action values: 0 
buys: 1 
player value: 1 
card supply: [28. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  4. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 1.  3. 29.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29] -> size -> 18 
adversary victory points: 3
player victory points: 2 





Player: 0 
cards in hand: [ 1.  3. 29.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[ 3.0567315]
 [16.654167 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  3. 29.  0.  0.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29] -> size -> 18 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  4. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 11.  0.  3.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6] -> size -> 18 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 30.92403793334961



action possibilites: [-1. 29.] 
expected returns: [[22.302954]
 [36.827442]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  3.  0.  0. 29.] 
cards in discard: [] 
cards in deck: 12 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29] -> size -> 18 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  4. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 11.  0.  3.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6] -> size -> 18 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 13.974701881408691



action possibilites: [-1.] 
expected returns: [[42.845364]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 3. 0. 0. 1.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29] -> size -> 18 
action values: 1 
buys: 0 
player value: 2 
card supply: [28. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  4. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 11.  0.  3.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6] -> size -> 18 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 36.82743835449219





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  5.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[46.28735 ]
 [56.524647]
 [25.589447]
 [50.3486  ]
 [32.32303 ]
 [58.807407]
 [ 8.800022]
 [51.736443]
 [55.58181 ]
 [49.729755]
 [60.57481 ]
 [62.862118]
 [48.67478 ]
 [57.811886]
 [50.764084]
 [35.003345]
 [57.810566]
 [47.893066]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 3. 0. 0. 1.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29] -> size -> 18 
action values: 0 
buys: 1 
player value: 8 
card supply: [28. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  4. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 11.  0.  3.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6] -> size -> 18 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 42.84536361694336



buy possibilites: [-1] 
expected returns: [[16.875565]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 3. 0. 0. 1.] 
cards in discard: [29.] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29] -> size -> 19 
action values: 0 
buys: 0 
player value: 4 
card supply: [28. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 11.  0.  3.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6] -> size -> 18 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5.  0.  0. 30.  0.  0. 40.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 97.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 62.862117767333984






         -------------------- Turn: 10 -------------------- 
Player: 1 
cards in hand: [ 0. 11.  0.  3.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 11.  0.  3.  0.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6] -> size -> 18 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 29. 29.  0.  3.] 
adversary cards in discard: [29. 29. 29.  1.  3.  0.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29] -> size -> 19 
adversary victory points: 3
player victory points: 2 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3. 0.] 
cards in discard: [0.] 
cards in deck: 13 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0] -> size -> 19 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 29. 29.  0.  3.] 
adversary cards in discard: [29. 29. 29.  1.  3.  0.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29] -> size -> 19 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0.] 
cards in discard: [0.] 
cards in deck: 13 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0] -> size -> 19 
action values: 0 
buys: 1 
player value: 3 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 29. 29.  0.  3.] 
adversary cards in discard: [29. 29. 29.  1.  3.  0.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29] -> size -> 19 
adversary victory points: 3
player victory points: 2 





Player: 0 
cards in hand: [ 0. 29. 29.  0.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[37.90137 ]
 [51.696323]
 [51.696323]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29. 29.  0.  3.] 
cards in discard: [29. 29. 29.  1.  3.  0.  0.  1.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29] -> size -> 19 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 6.] 
adversary cards in discard: [ 0. 11.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0] -> size -> 19 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 16.875564575195312



action possibilites: [-1. 29. 29.] 
expected returns: [[25.041262]
 [39.394985]
 [39.394985]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29.  0.  3. 29.] 
cards in discard: [29. 29. 29.  1.  3.  0.  0.  1.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29] -> size -> 19 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 6.] 
adversary cards in discard: [ 0. 11.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0] -> size -> 19 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 50.02444839477539



action possibilites: [-1. 29.] 
expected returns: [[54.30068]
 [68.65271]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  3. 29.  0.] 
cards in discard: [29. 29. 29.  1.  3.  0.  0.  1.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29] -> size -> 19 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 6.] 
adversary cards in discard: [ 0. 11.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0] -> size -> 19 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 39.39499282836914



action possibilites: [-1.] 
expected returns: [[53.08094]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3. 0. 3.] 
cards in discard: [29. 29. 29.  1.  3.  0.  0.  1.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29] -> size -> 19 
action values: 1 
buys: 0 
player value: 3 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 6.] 
adversary cards in discard: [ 0. 11.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0] -> size -> 19 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 68.6527099609375





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[52.73422 ]
 [62.43394 ]
 [33.438297]
 [56.55681 ]
 [39.668842]
 [18.258436]
 [57.99791 ]
 [61.5078  ]
 [55.86562 ]
 [66.22187 ]
 [68.398994]
 [55.02132 ]
 [63.638783]
 [57.026894]
 [41.996964]
 [63.63177 ]
 [54.4871  ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 3.] 
cards in discard: [29. 29. 29.  1.  3.  0.  0.  1.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29] -> size -> 19 
action values: 0 
buys: 1 
player value: 6 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  3. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 6.] 
adversary cards in discard: [ 0. 11.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0] -> size -> 19 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 53.08094024658203



buy possibilites: [-1] 
expected returns: [[111.135475]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 3.] 
cards in discard: [29. 29. 29.  1.  3.  0.  0.  1. 29.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29] -> size -> 20 
action values: 0 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  2. 10. 10. 10. 10. 10.] 
adversary cards in hand: [3. 0. 0. 0. 6.] 
adversary cards in discard: [ 0. 11.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0] -> size -> 19 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5.  0.  0. 30.  0.  0. 60.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 117.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 68.39900207519531






         -------------------- Turn: 11 -------------------- 
Player: 1 
cards in hand: [3. 0. 0. 0. 6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 0. 6.] 
cards in discard: [ 0. 11.  0.  0.  3.  0.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0] -> size -> 19 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  2. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3. 29. 29.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29] -> size -> 20 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 6.] 
cards in discard: [ 0. 11.  0.  0.  3.  0.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0] -> size -> 19 
action values: 0 
buys: 1 
player value: 3 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  2. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3. 29. 29.  0.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29] -> size -> 20 
adversary victory points: 3
player victory points: 2 


 --------- NOTHING HAPPENED --------- 



Player: 0 
cards in hand: [ 3. 29. 29.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[14.707646]
 [29.80532 ]
 [29.80532 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29. 29.  0.  0.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29] -> size -> 20 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  2. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3. 11.  0. 16.  6.] 
adversary cards in discard: [ 0. 11.  0.  0.  3.  0.  3.  0.  0.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0] -> size -> 19 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 111.1354751586914



action possibilites: [-1. 29.] 
expected returns: [[15.995664]
 [31.573761]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29.  0.  0.  0.] 
cards in discard: [] 
cards in deck: 14 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29] -> size -> 20 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  2. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3. 11.  0. 16.  6.] 
adversary cards in discard: [ 0. 11.  0.  0.  3.  0.  3.  0.  0.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0] -> size -> 19 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 26.76272201538086



action possibilites: [-1.] 
expected returns: [[28.441511]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29] -> size -> 20 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  2. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3. 11.  0. 16.  6.] 
adversary cards in discard: [ 0. 11.  0.  0.  3.  0.  3.  0.  0.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0] -> size -> 19 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 31.573749542236328





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[28.491144 ]
 [38.037155 ]
 [ 9.672387 ]
 [32.30298  ]
 [15.914252 ]
 [-5.6358085]
 [33.550613 ]
 [37.095932 ]
 [31.45586  ]
 [41.86974  ]
 [44.11849  ]
 [30.745842 ]
 [39.227505 ]
 [32.592148 ]
 [18.283445 ]
 [39.21709  ]
 [30.024593 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29] -> size -> 20 
action values: 0 
buys: 1 
player value: 6 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  2. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3. 11.  0. 16.  6.] 
adversary cards in discard: [ 0. 11.  0.  0.  3.  0.  3.  0.  0.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0] -> size -> 19 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 28.441511154174805



buy possibilites: [-1] 
expected returns: [[14.510667]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 0.] 
cards in discard: [29.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29] -> size -> 21 
action values: 0 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 3. 11.  0. 16.  6.] 
adversary cards in discard: [ 0. 11.  0.  0.  3.  0.  3.  0.  0.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0] -> size -> 19 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5.  0.  0. 30.  0.  0. 40.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 97.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 44.118473052978516






         -------------------- Turn: 12 -------------------- 
Player: 1 
cards in hand: [ 3. 11.  0. 16.  6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11. 16.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 11.  0. 16.  6.] 
cards in discard: [ 0. 11.  0.  0.  3.  0.  3.  0.  0.  0.  6.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0] -> size -> 19 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  8. 10. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 1.  0.  0. 29. 29.] 
adversary cards in discard: [29. 29. 29.  3.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29] -> size -> 21 
adversary victory points: 3
player victory points: 2 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 16.  6.] 
cards in discard: [ 0. 11.  0.  0.  3.  0.  3.  0.  0.  0.  6. 11.] 
cards in deck: 3 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11] -> size -> 20 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  7. 10. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 1.  0.  0. 29. 29.] 
adversary cards in discard: [29. 29. 29.  3.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29] -> size -> 21 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0. 16.  6.] 
cards in discard: [ 0. 11.  0.  0.  3.  0.  3.  0.  0.  0.  6. 11.] 
cards in deck: 3 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11] -> size -> 20 
action values: 0 
buys: 1 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  7. 10. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 1.  0.  0. 29. 29.] 
adversary cards in discard: [29. 29. 29.  3.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29] -> size -> 21 
adversary victory points: 3
player victory points: 2 





Player: 0 
cards in hand: [ 1.  0.  0. 29. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[76.50447]
 [90.84913]
 [90.84913]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  0.  0. 29. 29.] 
cards in discard: [29. 29. 29.  3.  0.  0.  0.  0.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29] -> size -> 21 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  7. 10. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  0.  0.  1.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11] -> size -> 20 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 14.510666847229004



action possibilites: [-1. 29. 29.] 
expected returns: [[83.171844]
 [97.50906 ]
 [97.50906 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  0.  0. 29. 29.] 
cards in discard: [29. 29. 29.  3.  0.  0.  0.  0.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29] -> size -> 21 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  7. 10. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  0.  0.  1.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11] -> size -> 20 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 88.23847961425781



action possibilites: [-1. 29.] 
expected returns: [[104.58804 ]
 [119.360214]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  0.  0. 29.  3.] 
cards in discard: [29. 29. 29.  3.  0.  0.  0.  0.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29] -> size -> 21 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  7. 10. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  0.  0.  1.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11] -> size -> 20 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 97.50907897949219



action possibilites: [-1.] 
expected returns: [[102.08151]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 0. 0. 3. 3.] 
cards in discard: [29. 29. 29.  3.  0.  0.  0.  0.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29] -> size -> 21 
action values: 1 
buys: 0 
player value: 3 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  7. 10. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  0.  0.  1.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11] -> size -> 20 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 119.36021423339844





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[106.19797 ]
 [116.07503 ]
 [ 84.85692 ]
 [110.06138 ]
 [ 91.45431 ]
 [ 67.52976 ]
 [111.62347 ]
 [115.30692 ]
 [109.69737 ]
 [119.94016 ]
 [121.999374]
 [108.518394]
 [117.30136 ]
 [110.70322 ]
 [ 94.41551 ]
 [117.31739 ]
 [107.96729 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 0. 0. 3. 3.] 
cards in discard: [29. 29. 29.  3.  0.  0.  0.  0.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29] -> size -> 21 
action values: 0 
buys: 1 
player value: 7 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  7. 10. 10.  1. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  0.  0.  1.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11] -> size -> 20 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 102.08151245117188



buy possibilites: [-1] 
expected returns: [[157.34529]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 0. 0. 3. 3.] 
cards in discard: [29. 29. 29.  3.  0.  0.  0.  0. 29.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29] -> size -> 22 
action values: 0 
buys: 0 
player value: 3 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  7. 10. 10.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [11.  0.  0.  1.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11] -> size -> 20 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5.  0.  0. 30.  0.  0. 60.  0.  0.  0.  0.  0.  0.  0. 32.  0.] 
sum of rewards: 117.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 121.9993896484375






         -------------------- Turn: 13 -------------------- 
Player: 1 
cards in hand: [11.  0.  0.  1.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [11.  0.  0.  1.  3.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11] -> size -> 20 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  7. 10. 10.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 1. 29.  0. 29. 29.] 
adversary cards in discard: [29. 29. 29.  3.  0.  0.  0.  0. 29. 29. 29. 29.  1.  0.  0.  3.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29] -> size -> 22 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: 16.0 : ['Remodel' '16' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [11.  0.  0.  1.  3.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11] -> size -> 20 
action values: 0 
buys: 1 
player value: 4 
card supply: [27. 27. 30. 29. 30.  8.  8.  9.  7. 10. 10.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 1. 29.  0. 29. 29.] 
adversary cards in discard: [29. 29. 29.  3.  0.  0.  0.  0. 29. 29. 29. 29.  1.  0.  0.  3.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29] -> size -> 22 
adversary victory points: 3
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [11.  0.  0.  1.  3.] 
cards in discard: [16.] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16] -> size -> 21 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  8.  8.  7. 10. 10.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 1. 29.  0. 29. 29.] 
adversary cards in discard: [29. 29. 29.  3.  0.  0.  0.  0. 29. 29. 29. 29.  1.  0.  0.  3.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29] -> size -> 22 
adversary victory points: 3
player victory points: 2 





Player: 0 
cards in hand: [ 1. 29.  0. 29. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29.] 
expected returns: [[106.449036]
 [117.44298 ]
 [117.44298 ]
 [117.44298 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 29.  0. 29. 29.] 
cards in discard: [29. 29. 29.  3.  0.  0.  0.  0. 29. 29. 29. 29.  1.  0.  0.  3.  3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  8.  8.  7. 10. 10.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [16. 11.  0.  0. 11.] 
adversary cards in discard: [16. 11.  0.  0.  1.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16] -> size -> 21 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 157.3452911376953



action possibilites: [-1. 29. 29.] 
expected returns: [[ 1.4761021]
 [14.318959 ]
 [14.318959 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29. 29.  3.] 
cards in discard: [1.] 
cards in deck: 16 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 27. 30. 29. 30.  8.  8.  8.  7. 10. 10.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [16. 11.  0.  0. 11.] 
adversary cards in discard: [16. 11.  0.  0.  1.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16] -> size -> 21 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 112.78392791748047



action possibilites: [-1.] 
expected returns: [[13.620332]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 1.] 
cards in discard: [ 1. 29.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 27. 30. 29. 30.  8.  8.  8.  7. 10. 10.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [16. 11.  0.  0. 11.] 
adversary cards in discard: [16. 11.  0.  0.  1.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16] -> size -> 21 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 8.100993156433105





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 14.364165 ]
 [ 23.625963 ]
 [ 17.983423 ]
 [  2.0666616]
 [-18.121418 ]
 [ 19.389433 ]
 [ 22.738752 ]
 [ 17.192421 ]
 [ 27.161097 ]
 [ 16.561026 ]
 [ 24.750797 ]
 [ 18.435421 ]
 [  4.243455 ]
 [ 24.741241 ]
 [ 16.034546 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 1.] 
cards in discard: [ 1. 29.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29] -> size -> 22 
action values: 0 
buys: 1 
player value: 5 
card supply: [27. 27. 30. 29. 30.  8.  8.  8.  7. 10. 10.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [16. 11.  0.  0. 11.] 
adversary cards in discard: [16. 11.  0.  0.  1.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16] -> size -> 21 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 13.620331764221191



buy possibilites: [-1] 
expected returns: [[24.911728]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 1.] 
cards in discard: [ 1. 29. 25.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25] -> size -> 23 
action values: 0 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  8.  8.  7. 10.  9.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [16. 11.  0.  0. 11.] 
adversary cards in discard: [16. 11.  0.  0.  1.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16] -> size -> 21 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[ -5   0   0  30   0   0  40   0   0   0   0   0   0   0 250   0] 
sum of rewards: 315 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 27.16110610961914






         -------------------- Turn: 14 -------------------- 
Player: 1 
cards in hand: [16. 11.  0.  0. 11.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16. 11. 11.] 
Chosen action: 16 : ['Remodel' '16' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [16. 11.  0.  0. 11.] 
cards in discard: [16. 11.  0.  0.  1.  3.] 
cards in deck: 10 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16] -> size -> 21 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 27. 30. 29. 30.  8.  8.  8.  7. 10.  9.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 29.  0.  3. 29.] 
adversary cards in discard: [ 1. 29. 25. 29. 29.  0.  3.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25] -> size -> 23 
adversary victory points: 3
player victory points: 2 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [11.  0. 11.] 
cards in discard: [16. 11.  0.  0.  1.  3.  0.] 
cards in deck: 10 
card top of deck: [] 
played cards: [16.] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0] -> size -> 21 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 29. 30.  8.  8.  8.  7. 10.  9.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 29.  0.  3. 29.] 
adversary cards in discard: [ 1. 29. 25. 29. 29.  0.  3.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25] -> size -> 23 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [11.  0. 11.] 
cards in discard: [16. 11.  0.  0.  1.  3.  0.] 
cards in deck: 10 
card top of deck: [] 
played cards: [16.] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0] -> size -> 21 
action values: 0 
buys: 1 
player value: 1 
card supply: [26. 27. 30. 29. 30.  8.  8.  8.  7. 10.  9.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [ 0. 29.  0.  3. 29.] 
adversary cards in discard: [ 1. 29. 25. 29. 29.  0.  3.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25] -> size -> 23 
adversary victory points: 3
player victory points: 2 





Player: 0 
cards in hand: [ 0. 29.  0.  3. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[36.150448]
 [48.38693 ]
 [48.38693 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29.  0.  3. 29.] 
cards in discard: [ 1. 29. 25. 29. 29.  0.  3.  1.] 
cards in deck: 10 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25] -> size -> 23 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 29. 30.  8.  8.  8.  7. 10.  9.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 6. 6. 3. 0.] 
adversary cards in discard: [16. 11.  0.  0.  1.  3.  0. 16. 11.  0. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0] -> size -> 21 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 24.911727905273438



action possibilites: [-1.] 
expected returns: [[59.934093]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3. 0.] 
cards in discard: [ 1. 29. 25. 29. 29.  0.  3.  1. 29.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25] -> size -> 23 
action values: 1 
buys: 0 
player value: 1 
card supply: [26. 27. 30. 29. 30.  8.  8.  8.  7. 10.  9.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 6. 6. 3. 0.] 
adversary cards in discard: [16. 11.  0.  0.  1.  3.  0. 16. 11.  0. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0] -> size -> 21 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 41.256282806396484





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[61.632572]
 [71.20821 ]
 [65.28668 ]
 [24.841951]
 [66.867   ]
 [70.284546]
 [64.82133 ]
 [63.85964 ]
 [65.935616]
 [72.34433 ]
 [63.434376]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0.] 
cards in discard: [ 1. 29. 25. 29. 29.  0.  3.  1. 29.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25] -> size -> 23 
action values: 0 
buys: 1 
player value: 4 
card supply: [26. 27. 30. 29. 30.  8.  8.  8.  7. 10.  9.  0. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 6. 6. 3. 0.] 
adversary cards in discard: [16. 11.  0.  0.  1.  3.  0. 16. 11.  0. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0] -> size -> 21 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 59.9340934753418



buy possibilites: [-1] 
expected returns: [[110.71521]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0.] 
cards in discard: [ 1. 29. 25. 29. 29.  0.  3.  1. 29. 15.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15] -> size -> 24 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 29. 30.  8.  8.  8.  7. 10.  9.  0. 10. 10. 10. 10.  9.] 
adversary cards in hand: [0. 6. 6. 3. 0.] 
adversary cards in discard: [16. 11.  0.  0.  1.  3.  0. 16. 11.  0. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0] -> size -> 21 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[ -5   0   0  30   0   0  20   0   0   0   0   0   0   0 128   0] 
sum of rewards: 173 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 72.34432220458984






         -------------------- Turn: 15 -------------------- 
Player: 1 
cards in hand: [0. 6. 6. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 6. 6. 3. 0.] 
cards in discard: [16. 11.  0.  0.  1.  3.  0. 16. 11.  0. 11.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0] -> size -> 21 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 29. 30.  8.  8.  8.  7. 10.  9.  0. 10. 10. 10. 10.  9.] 
adversary cards in hand: [ 0.  0.  0. 29.  3.] 
adversary cards in discard: [ 1. 29. 25. 29. 29.  0.  3.  1. 29. 15. 29.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15] -> size -> 24 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 6. 6. 3. 0.] 
cards in discard: [16. 11.  0.  0.  1.  3.  0. 16. 11.  0. 11.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0] -> size -> 21 
action values: 0 
buys: 1 
player value: 2 
card supply: [26. 27. 30. 29. 30.  8.  8.  8.  7. 10.  9.  0. 10. 10. 10. 10.  9.] 
adversary cards in hand: [ 0.  0.  0. 29.  3.] 
adversary cards in discard: [ 1. 29. 25. 29. 29.  0.  3.  1. 29. 15. 29.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15] -> size -> 24 
adversary victory points: 3
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 6. 6. 3. 0.] 
cards in discard: [16. 11.  0.  0.  1.  3.  0. 16. 11.  0. 11.  3.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3] -> size -> 22 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 28. 30.  8.  8.  8.  7. 10.  9.  0. 10. 10. 10. 10.  9.] 
adversary cards in hand: [ 0.  0.  0. 29.  3.] 
adversary cards in discard: [ 1. 29. 25. 29. 29.  0.  3.  1. 29. 15. 29.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15] -> size -> 24 
adversary victory points: 3
player victory points: 3 





Player: 0 
cards in hand: [ 0.  0.  0. 29.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[ 99.780365]
 [110.2637  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  0. 29.  3.] 
cards in discard: [ 1. 29. 25. 29. 29.  0.  3.  1. 29. 15. 29.  0.  0.  3.  0.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15] -> size -> 24 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 28. 30.  8.  8.  8.  7. 10.  9.  0. 10. 10. 10. 10.  9.] 
adversary cards in hand: [0. 3. 3. 0. 0.] 
adversary cards in discard: [16. 11.  0.  0.  1.  3.  0. 16. 11.  0. 11.  3.  0.  6.  6.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3] -> size -> 22 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 110.7152099609375



action possibilites: [-1.] 
expected returns: [[72.32941]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 3.] 
cards in discard: [ 1. 29. 25. 29. 29.  0.  3.  1. 29. 15. 29.  0.  0.  3.  0. 29.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15] -> size -> 24 
action values: 1 
buys: 0 
player value: 1 
card supply: [26. 27. 30. 28. 30.  8.  8.  8.  7. 10.  9.  0. 10. 10. 10. 10.  9.] 
adversary cards in hand: [0. 3. 3. 0. 0.] 
adversary cards in discard: [16. 11.  0.  0.  1.  3.  0. 16. 11.  0. 11.  3.  0.  6.  6.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3] -> size -> 22 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 105.8099365234375





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[70.27388 ]
 [78.85985 ]
 [73.59721 ]
 [36.944347]
 [75.065445]
 [78.02057 ]
 [72.602066]
 [72.379395]
 [74.09872 ]
 [79.839066]
 [72.08927 ]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3.] 
cards in discard: [ 1. 29. 25. 29. 29.  0.  3.  1. 29. 15. 29.  0.  0.  3.  0. 29.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15] -> size -> 24 
action values: 0 
buys: 1 
player value: 4 
card supply: [26. 27. 30. 28. 30.  8.  8.  8.  7. 10.  9.  0. 10. 10. 10. 10.  9.] 
adversary cards in hand: [0. 3. 3. 0. 0.] 
adversary cards in discard: [16. 11.  0.  0.  1.  3.  0. 16. 11.  0. 11.  3.  0.  6.  6.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3] -> size -> 22 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 72.32940673828125



buy possibilites: [-1] 
expected returns: [[55.166504]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3.] 
cards in discard: [ 1. 29. 25. 29. 29.  0.  3.  1. 29. 15. 29.  0.  0.  3.  0. 29. 15.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15
 15] -> size -> 25 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 28. 30.  8.  8.  8.  7. 10.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [0. 3. 3. 0. 0.] 
adversary cards in discard: [16. 11.  0.  0.  1.  3.  0. 16. 11.  0. 11.  3.  0.  6.  6.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3] -> size -> 22 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0   0   0   0  20   0   0   0   0   0   0   0 128   0] 
sum of rewards: 143 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 79.83906555175781






         -------------------- Turn: 16 -------------------- 
Player: 1 
cards in hand: [0. 3. 3. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 3. 0. 0.] 
cards in discard: [16. 11.  0.  0.  1.  3.  0. 16. 11.  0. 11.  3.  0.  6.  6.  3.  0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3] -> size -> 22 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 28. 30.  8.  8.  8.  7. 10.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 3.  0. 29. 29. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15
 15] -> size -> 25 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 11.0 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 3. 0. 0.] 
cards in discard: [16. 11.  0.  0.  1.  3.  0. 16. 11.  0. 11.  3.  0.  6.  6.  3.  0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3] -> size -> 22 
action values: 0 
buys: 1 
player value: 3 
card supply: [26. 27. 30. 28. 30.  8.  8.  8.  7. 10.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 3.  0. 29. 29. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15
 15] -> size -> 25 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 3. 0. 0.] 
cards in discard: [16. 11.  0.  0.  1.  3.  0. 16. 11.  0. 11.  3.  0.  6.  6.  3.  0. 11.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11] -> size -> 23 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 28. 30.  8.  8.  8.  6. 10.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 3.  0. 29. 29. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15
 15] -> size -> 25 
adversary victory points: 3
player victory points: 3 





Player: 0 
cards in hand: [ 3.  0. 29. 29. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29.] 
expected returns: [[25.641249]
 [38.64394 ]
 [38.64394 ]
 [38.64394 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 29. 29. 29.] 
cards in discard: [] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15
 15] -> size -> 25 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 28. 30.  8.  8.  8.  6. 10.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 1.  3.  0.  0. 11.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11] -> size -> 23 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 55.16650390625



action possibilites: [-1. 29. 29.] 
expected returns: [[14.582273]
 [28.277864]
 [28.277864]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 29. 29.] 
cards in discard: [29.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15
 15] -> size -> 25 
action values: 1 
buys: 0 
player value: 1 
card supply: [26. 27. 30. 28. 30.  8.  8.  8.  6. 10.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 1.  3.  0.  0. 11.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11] -> size -> 23 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 31.144927978515625



action possibilites: [-1. 15.] 
expected returns: [[31.787258]
 [40.88078 ]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 15.] 
cards in discard: [29. 29.] 
cards in deck: 18 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15
 15] -> size -> 25 
action values: 1 
buys: 0 
player value: 2 
card supply: [26. 27. 30. 28. 30.  8.  8.  8.  6. 10.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 1.  3.  0.  0. 11.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11] -> size -> 23 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 22.440471649169922



action possibilites: [-1] 
expected returns: [[-16.709635]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3.] 
cards in discard: [29. 29.] 
cards in deck: 18 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15] -> size -> 24 
action values: 0 
buys: 0 
player value: 5 
card supply: [26. 27. 30. 28. 30.  8.  8.  8.  6. 10.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 1.  3.  0.  0. 11.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11] -> size -> 23 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 40.88077163696289





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[-17.602013 ]
 [ -9.6946335]
 [-14.688404 ]
 [-28.78899  ]
 [-48.197426 ]
 [-13.259379 ]
 [-10.749    ]
 [-15.681881 ]
 [ -6.9836817]
 [-15.721818 ]
 [ -8.825543 ]
 [-14.186306 ]
 [-26.824936 ]
 [ -8.882307 ]
 [-15.851428 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3.] 
cards in discard: [29. 29.] 
cards in deck: 18 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15] -> size -> 24 
action values: 0 
buys: 1 
player value: 5 
card supply: [26. 27. 30. 28. 30.  8.  8.  8.  6. 10.  9.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 1.  3.  0.  0. 11.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11] -> size -> 23 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: take_action - action -1
Learning step: 0
desired expected reward: -16.70963478088379



buy possibilites: [-1] 
expected returns: [[-8.700477]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3.] 
cards in discard: [29. 29. 25.] 
cards in deck: 18 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25] -> size -> 25 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 28. 30.  8.  8.  8.  6. 10.  8.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [ 1.  3.  0.  0. 11.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11] -> size -> 23 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0   0   0   0  60   0   0   0   0   0   0   0 250   0] 
sum of rewards: 305 

action type: buy - action 25.0
Learning step: 0
desired expected reward: -6.983677864074707






         -------------------- Turn: 17 -------------------- 
Player: 1 
cards in hand: [ 1.  3.  0.  0. 11.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  3.  0.  0. 11.] 
cards in discard: [] 
cards in deck: 18 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11] -> size -> 23 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 28. 30.  8.  8.  8.  6. 10.  8.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [29.  0. 15. 25.  1.] 
adversary cards in discard: [29. 29. 25. 29. 29. 15.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25] -> size -> 25 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: 16.0 : ['Remodel' '16' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  3.  0.  0. 11.] 
cards in discard: [] 
cards in deck: 18 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11] -> size -> 23 
action values: 0 
buys: 1 
player value: 4 
card supply: [26. 27. 30. 28. 30.  8.  8.  8.  6. 10.  8.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [29.  0. 15. 25.  1.] 
adversary cards in discard: [29. 29. 25. 29. 29. 15.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25] -> size -> 25 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  3.  0.  0. 11.] 
cards in discard: [16.] 
cards in deck: 18 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16] -> size -> 24 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 28. 30.  8.  8.  7.  6. 10.  8.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [29.  0. 15. 25.  1.] 
adversary cards in discard: [29. 29. 25. 29. 29. 15.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25] -> size -> 25 
adversary victory points: 3
player victory points: 3 





Player: 0 
cards in hand: [29.  0. 15. 25.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 15. 25.] 
expected returns: [[51.921253]
 [64.556076]
 [60.197872]
 [62.469738]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0. 15. 25.  1.] 
cards in discard: [29. 29. 25. 29. 29. 15.  3.] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25] -> size -> 25 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 28. 30.  8.  8.  7.  6. 10.  8.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [3. 0. 0. 6. 6.] 
adversary cards in discard: [16.  1.  3.  0.  0. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16] -> size -> 24 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: -8.70047664642334



action possibilites: [-1. 15. 25.] 
expected returns: [[40.18294 ]
 [48.978085]
 [51.397713]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 15. 25.  3.] 
cards in discard: [29. 29. 25. 29. 29. 15.  3.  1.] 
cards in deck: 12 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25] -> size -> 25 
action values: 1 
buys: 0 
player value: 1 
card supply: [26. 27. 30. 28. 30.  8.  8.  7.  6. 10.  8.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [3. 0. 0. 6. 6.] 
adversary cards in discard: [16.  1.  3.  0.  0. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16] -> size -> 24 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 58.182411193847656



action possibilites: [-1] 
expected returns: [[31.81129]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 15.  3.  0.  0.] 
cards in discard: [29. 29. 25. 29. 29. 15.  3.  1.] 
cards in deck: 10 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25] -> size -> 25 
action values: 0 
buys: 0 
player value: 1 
card supply: [26. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [3. 0. 0. 6. 6.] 
adversary cards in discard: [16.  1.  3.  0.  0. 11.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6] -> size -> 25 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 51.397708892822266





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[27.06014  ]
 [36.050167 ]
 [30.568298 ]
 [-4.6819487]
 [31.827003 ]
 [35.00989  ]
 [29.6152   ]
 [29.177677 ]
 [30.87387  ]
 [37.094154 ]
 [28.59465  ]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 15.  3.  0.  0.] 
cards in discard: [29. 29. 25. 29. 29. 15.  3.  1.] 
cards in deck: 10 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25] -> size -> 25 
action values: 0 
buys: 1 
player value: 4 
card supply: [26. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10. 10. 10.  8.] 
adversary cards in hand: [3. 0. 0. 6. 6.] 
adversary cards in discard: [16.  1.  3.  0.  0. 11.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6] -> size -> 25 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action -1
Learning step: 0
desired expected reward: 31.811290740966797



buy possibilites: [-1] 
expected returns: [[41.911133]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 15.  3.  0.  0.] 
cards in discard: [29. 29. 25. 29. 29. 15.  3.  1. 15.] 
cards in deck: 10 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25 15] -> size -> 26 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10. 10. 10.  7.] 
adversary cards in hand: [3. 0. 0. 6. 6.] 
adversary cards in discard: [16.  1.  3.  0.  0. 11.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6] -> size -> 25 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0   0   0   0  40   0   0   0   0   0   0   0 128   0] 
sum of rewards: 163 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 37.094154357910156






         -------------------- Turn: 18 -------------------- 
Player: 1 
cards in hand: [3. 0. 0. 6. 6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 6. 6.] 
cards in discard: [16.  1.  3.  0.  0. 11.  6.] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6] -> size -> 25 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10. 10. 10.  7.] 
adversary cards in hand: [ 3. 29.  0. 29. 29.] 
adversary cards in discard: [29. 29. 25. 29. 29. 15.  3.  1. 15. 29. 25.  0. 15.  3.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25 15] -> size -> 26 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 6. 6.] 
cards in discard: [16.  1.  3.  0.  0. 11.  6.] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6] -> size -> 25 
action values: 0 
buys: 1 
player value: 2 
card supply: [26. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10. 10. 10.  7.] 
adversary cards in hand: [ 3. 29.  0. 29. 29.] 
adversary cards in discard: [29. 29. 25. 29. 29. 15.  3.  1. 15. 29. 25.  0. 15.  3.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25 15] -> size -> 26 
adversary victory points: 3
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 6. 6.] 
cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0] -> size -> 26 
action values: 0 
buys: 0 
player value: 2 
card supply: [25. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10. 10. 10.  7.] 
adversary cards in hand: [ 3. 29.  0. 29. 29.] 
adversary cards in discard: [29. 29. 25. 29. 29. 15.  3.  1. 15. 29. 25.  0. 15.  3.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25 15] -> size -> 26 
adversary victory points: 3
player victory points: 2 





Player: 0 
cards in hand: [ 3. 29.  0. 29. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29.] 
expected returns: [[67.854126]
 [79.19319 ]
 [79.19319 ]
 [79.19319 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29.  0. 29. 29.] 
cards in discard: [29. 29. 25. 29. 29. 15.  3.  1. 15. 29. 25.  0. 15.  3.  0.  0.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25 15] -> size -> 26 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10. 10. 10.  7.] 
adversary cards in hand: [ 0.  3.  0. 11. 16.] 
adversary cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0] -> size -> 26 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 41.9111328125



action possibilites: [-1. 29.] 
expected returns: [[26.703585]
 [38.47086 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 29.  0.] 
cards in discard: [29. 29. 25. 29. 29. 15.  3.  1. 15. 29. 25.  0. 15.  3.  0.  0. 29.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25 15] -> size -> 26 
action values: 1 
buys: 0 
player value: 1 
card supply: [25. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10. 10. 10.  7.] 
adversary cards in hand: [ 0.  3.  0. 11. 16.] 
adversary cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0] -> size -> 26 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 74.38502502441406



action possibilites: [-1.] 
expected returns: [[106.200325]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0.] 
cards in discard: [29. 29. 25. 29. 29. 15.  3.  1. 15. 29. 25.  0. 15.  3.  0.  0. 29. 29.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25 15] -> size -> 26 
action values: 1 
buys: 0 
player value: 2 
card supply: [25. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10. 10. 10.  7.] 
adversary cards in hand: [ 0.  3.  0. 11. 16.] 
adversary cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0] -> size -> 26 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 33.474159240722656





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[103.10816 ]
 [111.58319 ]
 [106.420784]
 [ 69.224724]
 [107.92363 ]
 [110.90083 ]
 [105.52518 ]
 [105.2157  ]
 [106.98318 ]
 [112.58253 ]
 [104.95736 ]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0.] 
cards in discard: [29. 29. 25. 29. 29. 15.  3.  1. 15. 29. 25.  0. 15.  3.  0.  0. 29. 29.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25 15] -> size -> 26 
action values: 0 
buys: 1 
player value: 4 
card supply: [25. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10. 10. 10.  7.] 
adversary cards in hand: [ 0.  3.  0. 11. 16.] 
adversary cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0] -> size -> 26 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 106.20032501220703



buy possibilites: [-1] 
expected returns: [[81.660614]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0.] 
cards in discard: [29. 29. 25. 29. 29. 15.  3.  1. 15. 29. 25.  0. 15.  3.  0.  0. 29. 29.
 15.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25 15 15] -> size -> 27 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10. 10. 10.  6.] 
adversary cards in hand: [ 0.  3.  0. 11. 16.] 
adversary cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0] -> size -> 26 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[ -5   0   0  30   0   0  40   0   0   0   0   0   0   0 128   0] 
sum of rewards: 193 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 112.58251953125






         -------------------- Turn: 19 -------------------- 
Player: 1 
cards in hand: [ 0.  3.  0. 11. 16.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11. 16.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3.  0. 11. 16.] 
cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0] -> size -> 26 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10. 10. 10.  6.] 
adversary cards in hand: [15.  0. 29.  1.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25 15 15] -> size -> 27 
adversary victory points: 3
player victory points: 2 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3.  0. 16.] 
cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6. 10.] 
cards in deck: 8 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10] -> size -> 27 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [15.  0. 29.  1.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25 15 15] -> size -> 27 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3.  0. 16.] 
cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6. 10.] 
cards in deck: 8 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10] -> size -> 27 
action values: 0 
buys: 1 
player value: 2 
card supply: [25. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [15.  0. 29.  1.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25 15 15] -> size -> 27 
adversary victory points: 3
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3.  0. 16.] 
cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6. 10.  0.] 
cards in deck: 8 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0] -> size -> 28 
action values: 0 
buys: 0 
player value: 2 
card supply: [24. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [15.  0. 29.  1.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25 15 15] -> size -> 27 
adversary victory points: 3
player victory points: 2 





Player: 0 
cards in hand: [15.  0. 29.  1.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 29.] 
expected returns: [[-15.903622 ]
 [ -9.308241 ]
 [ -5.9310036]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0. 29.  1.  0.] 
cards in discard: [] 
cards in deck: 22 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25 15 15] -> size -> 27 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [16.  0.  0.  3. 11.] 
adversary cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6. 10.  0. 11.  0.  3.
  0. 16.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0] -> size -> 28 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 81.66061401367188



action possibilites: [-1. 15. 29.] 
expected returns: [[-15.375913 ]
 [ -7.0765104]
 [ -2.6128113]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0.  0. 29.] 
cards in discard: [1.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25 15 15] -> size -> 27 
action values: 1 
buys: 0 
player value: 1 
card supply: [24. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [16.  0.  0.  3. 11.] 
adversary cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6. 10.  0. 11.  0.  3.
  0. 16.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0] -> size -> 28 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -9.87549877166748



action possibilites: [-1. 15.] 
expected returns: [[-7.593295 ]
 [ 0.8339369]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0.  0.] 
cards in discard: [ 1. 29.] 
cards in deck: 20 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15
 25 15 15] -> size -> 27 
action values: 1 
buys: 0 
player value: 2 
card supply: [24. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [16.  0.  0.  3. 11.] 
adversary cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6. 10.  0. 11.  0.  3.
  0. 16.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0] -> size -> 28 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -8.105279922485352



action possibilites: [-1] 
expected returns: [[-14.078223]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0.] 
cards in discard: [ 1. 29.] 
cards in deck: 20 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15] -> size -> 26 
action values: 0 
buys: 0 
player value: 5 
card supply: [24. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [16.  0.  0.  3. 11.] 
adversary cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6. 10.  0. 11.  0.  3.
  0. 16.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0] -> size -> 28 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 0.8339483737945557





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[-15.840309 ]
 [ -7.2742605]
 [-32.2678   ]
 [-12.699596 ]
 [-26.846733 ]
 [-45.77003  ]
 [-11.314147 ]
 [ -8.410478 ]
 [-13.692826 ]
 [ -4.151288 ]
 [-13.856815 ]
 [ -6.274428 ]
 [-12.282188 ]
 [-24.97535  ]
 [ -6.3294945]
 [-14.112715 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [ 1. 29.] 
cards in deck: 20 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15] -> size -> 26 
action values: 0 
buys: 1 
player value: 6 
card supply: [24. 27. 30. 28. 30.  8.  7.  7.  6. 10.  8.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [16.  0.  0.  3. 11.] 
adversary cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6. 10.  0. 11.  0.  3.
  0. 16.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0] -> size -> 28 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: take_action - action -1
Learning step: 0
desired expected reward: -14.07822322845459



buy possibilites: [-1] 
expected returns: [[-1.1020238]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [ 1. 29. 25.] 
cards in deck: 20 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25] -> size -> 27 
action values: 0 
buys: 0 
player value: 1 
card supply: [24. 27. 30. 28. 30.  8.  7.  7.  6. 10.  7.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [16.  0.  0.  3. 11.] 
adversary cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6. 10.  0. 11.  0.  3.
  0. 16.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0] -> size -> 28 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5.   0.   0.  30.   0.   0.  60.   0.   0.   0.   0.   0.   0.   0.
 62.5  0. ] 
sum of rewards: 147.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: -4.151297092437744






         -------------------- Turn: 20 -------------------- 
Player: 1 
cards in hand: [16.  0.  0.  3. 11.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16. 11.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [16.  0.  0.  3. 11.] 
cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6. 10.  0. 11.  0.  3.
  0. 16.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0] -> size -> 28 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 27. 30. 28. 30.  8.  7.  7.  6. 10.  7.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [29. 29.  0.  1.  0.] 
adversary cards in discard: [ 1. 29. 25. 29. 29. 15.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25] -> size -> 27 
adversary victory points: 3
player victory points: 2 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [16.  0.  0.  3.] 
cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6. 10.  0. 11.  0.  3.
  0. 16.  3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3] -> size -> 29 
action values: 0 
buys: 0 
player value: 0 
card supply: [24. 27. 30. 27. 30.  8.  7.  7.  6. 10.  7.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [29. 29.  0.  1.  0.] 
adversary cards in discard: [ 1. 29. 25. 29. 29. 15.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25] -> size -> 27 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [16.  0.  0.  3.] 
cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6. 10.  0. 11.  0.  3.
  0. 16.  3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3] -> size -> 29 
action values: 0 
buys: 1 
player value: 2 
card supply: [24. 27. 30. 27. 30.  8.  7.  7.  6. 10.  7.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [29. 29.  0.  1.  0.] 
adversary cards in discard: [ 1. 29. 25. 29. 29. 15.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25] -> size -> 27 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [16.  0.  0.  3.] 
cards in discard: [16.  1.  3.  0.  0. 11.  6.  0.  3.  0.  0.  6.  6. 10.  0. 11.  0.  3.
  0. 16.  3.  8.] 
cards in deck: 3 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8] -> size -> 30 
action values: 0 
buys: 0 
player value: 0 
card supply: [24. 27. 30. 27. 30.  8.  7.  7.  6.  9.  7.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [29. 29.  0.  1.  0.] 
adversary cards in discard: [ 1. 29. 25. 29. 29. 15.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25] -> size -> 27 
adversary victory points: 3
player victory points: 3 





Player: 0 
cards in hand: [29. 29.  0.  1.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[12.310885]
 [23.367657]
 [23.367657]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29.  0.  1.  0.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  0.] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25] -> size -> 27 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 27. 30. 27. 30.  8.  7.  7.  6.  9.  7.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [ 3.  3.  3.  0. 11.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8] -> size -> 30 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: -1.1020238399505615



action possibilites: [-1.] 
expected returns: [[38.014656]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 1. 0. 0.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  0. 29.] 
cards in deck: 14 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25] -> size -> 27 
action values: 1 
buys: 0 
player value: 1 
card supply: [24. 27. 30. 27. 30.  8.  7.  7.  6.  9.  7.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [ 3.  3.  3.  0. 11.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8] -> size -> 30 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 18.648799896240234





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[35.33714 ]
 [43.943645]
 [18.159878]
 [38.64698 ]
 [23.923946]
 [ 4.093486]
 [39.936676]
 [42.89003 ]
 [37.65115 ]
 [47.087605]
 [37.37179 ]
 [44.948578]
 [38.991413]
 [25.944872]
 [44.905415]
 [36.920055]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 0. 0.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  0. 29.] 
cards in deck: 14 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25] -> size -> 27 
action values: 0 
buys: 1 
player value: 6 
card supply: [24. 27. 30. 27. 30.  8.  7.  7.  6.  9.  7.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [ 3.  3.  3.  0. 11.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8] -> size -> 30 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 38.01465606689453



buy possibilites: [-1] 
expected returns: [[89.665535]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 1. 0. 0.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  0. 29. 25.] 
cards in deck: 14 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25] -> size -> 28 
action values: 0 
buys: 0 
player value: 1 
card supply: [24. 27. 30. 27. 30.  8.  7.  7.  6.  9.  6.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [ 3.  3.  3.  0. 11.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8] -> size -> 30 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5.   0.   0.   0.   0.   0.  20.   0.   0.   0.   0.   0.   0.   0.
 62.5  0. ] 
sum of rewards: 77.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 47.08759689331055






         -------------------- Turn: 21 -------------------- 
Player: 1 
cards in hand: [ 3.  3.  3.  0. 11.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  3.  3.  0. 11.] 
cards in discard: [] 
cards in deck: 25 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8] -> size -> 30 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 27. 30. 27. 30.  8.  7.  7.  6.  9.  6.  0. 10. 10.  9. 10.  6.] 
adversary cards in hand: [29. 29.  3. 29. 29.] 
adversary cards in discard: [ 1. 29. 25. 29. 29. 15.  0. 29. 25. 29.  0.  1.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25] -> size -> 28 
adversary victory points: 3
player victory points: 3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 3. 3. 0.] 
cards in discard: [14.] 
cards in deck: 25 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14] -> size -> 31 
action values: 0 
buys: 0 
player value: 0 
card supply: [24. 27. 30. 27. 30.  8.  7.  7.  6.  9.  6.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [29. 29.  3. 29. 29.] 
adversary cards in discard: [ 1. 29. 25. 29. 29. 15.  0. 29. 25. 29.  0.  1.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25] -> size -> 28 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 3. 0.] 
cards in discard: [14.] 
cards in deck: 25 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14] -> size -> 31 
action values: 0 
buys: 1 
player value: 1 
card supply: [24. 27. 30. 27. 30.  8.  7.  7.  6.  9.  6.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [29. 29.  3. 29. 29.] 
adversary cards in discard: [ 1. 29. 25. 29. 29. 15.  0. 29. 25. 29.  0.  1.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25] -> size -> 28 
adversary victory points: 3
player victory points: 3 





Player: 0 
cards in hand: [29. 29.  3. 29. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29. 29.] 
expected returns: [[76.45716]
 [86.34689]
 [86.34689]
 [86.34689]
 [86.34689]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29.  3. 29. 29.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  0. 29. 25. 29.  0.  1.  0.  0.] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25] -> size -> 28 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 27. 30. 27. 30.  8.  7.  7.  6.  9.  6.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 0.  0. 10.  0.  6.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14] -> size -> 31 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 89.66553497314453



action possibilites: [-1. 29. 29. 29. 15.] 
expected returns: [[45.954998]
 [58.4697  ]
 [58.4697  ]
 [58.4697  ]
 [54.244816]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29. 29. 15.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  0. 29. 25. 29.  0.  1.  0.  0.  3.] 
cards in deck: 8 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25] -> size -> 28 
action values: 1 
buys: 0 
player value: 1 
card supply: [24. 27. 30. 27. 30.  8.  7.  7.  6.  9.  6.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 0.  0. 10.  0.  6.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14] -> size -> 31 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 81.8411865234375



action possibilites: [-1. 29. 29. 15.] 
expected returns: [[109.51917 ]
 [121.56448 ]
 [121.56448 ]
 [117.475365]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29. 15.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  0. 29. 25. 29.  0.  1.  0.  0.  3. 15.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25] -> size -> 28 
action values: 1 
buys: 0 
player value: 2 
card supply: [24. 27. 30. 27. 30.  8.  7.  7.  6.  9.  6.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 0.  0. 10.  0.  6.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14] -> size -> 31 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 53.17264938354492



action possibilites: [-1. 29. 15.] 
expected returns: [[79.83447 ]
 [91.174644]
 [87.26344 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 15.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  0. 29. 25. 29.  0.  1.  0.  0.  3. 15.  3.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25] -> size -> 28 
action values: 1 
buys: 0 
player value: 3 
card supply: [24. 27. 30. 27. 30.  8.  7.  7.  6.  9.  6.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 0.  0. 10.  0.  6.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14] -> size -> 31 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 116.46011352539062



action possibilites: [-1. 25.] 
expected returns: [[80.24164]
 [89.92483]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  0. 29. 25. 29.  0.  1.  0.  0.  3. 15.  3. 15.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25] -> size -> 28 
action values: 1 
buys: 0 
player value: 4 
card supply: [24. 27. 30. 27. 30.  8.  7.  7.  6.  9.  6.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 0.  0. 10.  0.  6.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14] -> size -> 31 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 86.29266357421875



action possibilites: [-1] 
expected returns: [[44.5621]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  0. 29. 25. 29.  0.  1.  0.  0.  3. 15.  3. 15.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29. 29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25] -> size -> 28 
action values: 0 
buys: 0 
player value: 4 
card supply: [24. 27. 30. 27. 30.  8.  6.  7.  6.  9.  6.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 0.  0. 10.  0.  6.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6] -> size -> 32 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0   0   0   0 100   0   0   0   0   0   0   0   0   0] 
sum of rewards: 95 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 89.9248275756836





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[40.96347 ]
 [49.02081 ]
 [44.065357]
 [29.134384]
 [ 9.497553]
 [45.481277]
 [48.212284]
 [43.071194]
 [51.911068]
 [42.948296]
 [49.941883]
 [44.5546  ]
 [31.413532]
 [49.923183]
 [42.724354]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  0.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  0. 29. 25. 29.  0.  1.  0.  0.  3. 15.  3. 15.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29. 29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25] -> size -> 28 
action values: 0 
buys: 1 
player value: 5 
card supply: [24. 27. 30. 27. 30.  8.  6.  7.  6.  9.  6.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 0.  0. 10.  0.  6.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6] -> size -> 32 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0   0   0   0 100   0   0   0   0   0   0   0   0   0] 
sum of rewards: 95 

action type: take_action - action -1
Learning step: 0
desired expected reward: 44.56209945678711



buy possibilites: [-1] 
expected returns: [[0.17513251]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  0.] 
cards in discard: [ 1. 29. 25. 29. 29. 15.  0. 29. 25. 29.  0.  1.  0.  0.  3. 15.  3. 15.
 25.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29. 29. 29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25 25] -> size -> 29 
action values: 0 
buys: 0 
player value: 0 
card supply: [24. 27. 30. 27. 30.  8.  6.  7.  6.  9.  5.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 0.  0. 10.  0.  6.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6] -> size -> 32 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0   0   0   0 100   0   0   0   0   0   0   0 250   0] 
sum of rewards: 345 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 51.91103744506836






         -------------------- Turn: 22 -------------------- 
Player: 1 
cards in hand: [ 0.  0. 10.  0.  6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 10.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0. 10.  0.  6.] 
cards in discard: [14. 11.  3.  3.  3.  0.  6.] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6] -> size -> 32 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 27. 30. 27. 30.  8.  6.  7.  6.  9.  5.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [25.  3. 25. 29.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25 25] -> size -> 29 
adversary victory points: 3
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0. 10.  0.  6.] 
cards in discard: [14. 11.  3.  3.  3.  0.  6.] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6] -> size -> 32 
action values: 0 
buys: 1 
player value: 3 
card supply: [24. 27. 30. 27. 30.  8.  6.  7.  6.  9.  5.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [25.  3. 25. 29.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25 25] -> size -> 29 
adversary victory points: 3
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0. 10.  0.  6.] 
cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0] -> size -> 33 
action values: 0 
buys: 0 
player value: 3 
card supply: [23. 27. 30. 27. 30.  8.  6.  7.  6.  9.  5.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [25.  3. 25. 29.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25 25] -> size -> 29 
adversary victory points: 3
player victory points: 2 





Player: 0 
cards in hand: [25.  3. 25. 29.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 25. 29.] 
expected returns: [[-10.558769  ]
 [ -0.81085277]
 [ -0.81085277]
 [  1.3273742 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  3. 25. 29.  3.] 
cards in discard: [] 
cards in deck: 24 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25 25] -> size -> 29 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 27. 30. 27. 30.  8.  6.  7.  6.  9.  5.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 3. 16.  0. 16.  0.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0] -> size -> 33 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: 0.17513251304626465



action possibilites: [-1. 25. 25.] 
expected returns: [[ 8.974535]
 [18.92396 ]
 [18.92396 ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 25.  3.  0.] 
cards in discard: [3.] 
cards in deck: 23 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25 25] -> size -> 29 
action values: 1 
buys: 0 
player value: 1 
card supply: [23. 27. 30. 27. 30.  8.  6.  7.  6.  9.  5.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 3. 16.  0. 16.  0.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0] -> size -> 33 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -3.967188596725464



action possibilites: [-1] 
expected returns: [[16.516233]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  3.  0. 29. 29.] 
cards in discard: [3.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25 25] -> size -> 29 
action values: 0 
buys: 0 
player value: 1 
card supply: [23. 27. 30. 27. 30.  8.  5.  7.  6.  9.  5.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 3. 16.  0. 16.  0.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6] -> size -> 34 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 18.92396354675293





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
expected returns: [[ 15.452346]
 [ 18.73237 ]
 [-17.856398]
 [ 17.714443]
 [ 17.310392]]
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [25.  3.  0. 29. 29.] 
cards in discard: [3.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25 25] -> size -> 29 
action values: 0 
buys: 1 
player value: 2 
card supply: [23. 27. 30. 27. 30.  8.  5.  7.  6.  9.  5.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 3. 16.  0. 16.  0.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6] -> size -> 34 
adversary victory points: 2
player victory points: 3 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action -1
Learning step: 0
desired expected reward: 16.516233444213867



buy possibilites: [-1] 
expected returns: [[30.726276]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [25.  3.  0. 29. 29.] 
cards in discard: [3. 3.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25 25  3] -> size -> 30 
action values: 0 
buys: 0 
player value: 0 
card supply: [23. 27. 30. 26. 30.  8.  5.  7.  6.  9.  5.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 3. 16.  0. 16.  0.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6] -> size -> 34 
adversary victory points: 2
player victory points: 4 

Reward from previous game state: 
[-5  0  0 60  0  0 40  0  0  0  0  0  0  0 16  0] 
sum of rewards: 111 

action type: buy - action 3.0
Learning step: 0
desired expected reward: 18.73236083984375






         -------------------- Turn: 23 -------------------- 
Player: 1 
cards in hand: [ 3. 16.  0. 16.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16. 16.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 16.  0. 16.  0.] 
cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6] -> size -> 34 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 27. 30. 26. 30.  8.  5.  7.  6.  9.  5.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [29. 29. 15. 15. 29.] 
adversary cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25 25  3] -> size -> 30 
adversary victory points: 4
player victory points: 1 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 16.  0. 16.  0.] 
cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6] -> size -> 34 
action values: 0 
buys: 1 
player value: 2 
card supply: [23. 27. 30. 26. 30.  8.  5.  7.  6.  9.  5.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [29. 29. 15. 15. 29.] 
adversary cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25 25  3] -> size -> 30 
adversary victory points: 4
player victory points: 1 


 --------- NOTHING HAPPENED --------- 



Player: 0 
cards in hand: [29. 29. 15. 15. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 15. 15. 29.] 
expected returns: [[39.46196 ]
 [48.444805]
 [48.444805]
 [45.26486 ]
 [45.26486 ]
 [48.444805]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29. 15. 15. 29.] 
cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29.] 
cards in deck: 16 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25 25  3] -> size -> 30 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 27. 30. 26. 30.  8.  5.  7.  6.  9.  5.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 1.  0. 11.  6. 11.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6] -> size -> 34 
adversary victory points: 1
player victory points: 4 

Reward from previous game state: 
[-5  0  0 90  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: buy - action -1
Learning step: 0
desired expected reward: 30.726276397705078



action possibilites: [-1. 29. 15. 29. 29.] 
expected returns: [[27.397896]
 [37.295124]
 [33.844013]
 [37.295124]
 [37.295124]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 15. 29. 29.] 
cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25 25  3] -> size -> 30 
action values: 1 
buys: 0 
player value: 1 
card supply: [23. 27. 30. 26. 30.  8.  5.  7.  6.  9.  5.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 1.  0. 11.  6. 11.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6] -> size -> 34 
adversary victory points: 1
player victory points: 4 

Reward from previous game state: 
[-5  0  0 90  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 105 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 44.603824615478516



action possibilites: [-1. 15. 29.] 
expected returns: [[-0.28826547]
 [ 7.4679327 ]
 [11.554318  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 29.  0.] 
cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29.] 
cards in deck: 14 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25 25  3] -> size -> 30 
action values: 1 
buys: 0 
player value: 2 
card supply: [23. 27. 30. 26. 30.  8.  5.  7.  6.  9.  5.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 1.  0. 11.  6. 11.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6] -> size -> 34 
adversary victory points: 1
player victory points: 4 

Reward from previous game state: 
[-5  0  0 90  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 125 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 33.07699203491211



action possibilites: [-1. 15.] 
expected returns: [[ 4.762558]
 [12.519399]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0.] 
cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29. 29.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25
 15 15 25 25 25  3] -> size -> 30 
action values: 1 
buys: 0 
player value: 3 
card supply: [23. 27. 30. 26. 30.  8.  5.  7.  6.  9.  5.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 1.  0. 11.  6. 11.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6] -> size -> 34 
adversary victory points: 1
player victory points: 4 

Reward from previous game state: 
[-5  0  0 90  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 145 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 6.518320083618164



action possibilites: [-1] 
expected returns: [[42.105858]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [] 
cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29. 29.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29. 29. 15.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3] -> size -> 29 
action values: 0 
buys: 0 
player value: 6 
card supply: [23. 27. 30. 26. 30.  8.  5.  7.  6.  9.  5.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 1.  0. 11.  6. 11.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6] -> size -> 34 
adversary victory points: 1
player victory points: 4 

Reward from previous game state: 
[-5  0  0 90  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 165 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 12.51938533782959





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[40.165752]
 [48.151417]
 [23.29223 ]
 [43.1542  ]
 [28.652813]
 [ 9.426152]
 [44.626385]
 [47.187244]
 [42.062706]
 [50.88276 ]
 [42.114117]
 [49.025307]
 [43.670074]
 [30.738022]
 [48.98141 ]
 [42.00071 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29. 29.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29. 29. 15.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3] -> size -> 29 
action values: 0 
buys: 1 
player value: 6 
card supply: [23. 27. 30. 26. 30.  8.  5.  7.  6.  9.  5.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 1.  0. 11.  6. 11.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6] -> size -> 34 
adversary victory points: 1
player victory points: 4 

Reward from previous game state: 
[-5  0  0 90  0  0 80  0  0  0  0  0  0  0  0  0] 
sum of rewards: 165 

action type: take_action - action -1
Learning step: 0
desired expected reward: 42.105857849121094



buy possibilites: [-1] 
expected returns: [[49.064243]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29. 29. 25.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29. 29. 15.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25] -> size -> 30 
action values: 0 
buys: 0 
player value: 1 
card supply: [23. 27. 30. 26. 30.  8.  5.  7.  6.  9.  4.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 1.  0. 11.  6. 11.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6] -> size -> 34 
adversary victory points: 1
player victory points: 4 

Reward from previous game state: 
[-5.   0.   0.  90.   0.   0.  80.   0.   0.   0.   0.   0.   0.   0.
 62.5  0. ] 
sum of rewards: 227.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 50.882747650146484






         -------------------- Turn: 24 -------------------- 
Player: 1 
cards in hand: [ 1.  0. 11.  6. 11.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11. 11.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  0. 11.  6. 11.] 
cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.] 
cards in deck: 10 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6] -> size -> 34 
action values: 1 
buys: 0 
player value: 0 
card supply: [23. 27. 30. 26. 30.  8.  5.  7.  6.  9.  4.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [25. 25.  0.  0. 29.] 
adversary cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29. 29. 25. 29. 29. 29. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25] -> size -> 30 
adversary victory points: 4
player victory points: 1 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  0.  6. 11.] 
cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0.] 
cards in deck: 10 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0] -> size -> 35 
action values: 0 
buys: 0 
player value: 0 
card supply: [22. 27. 30. 26. 30.  8.  5.  7.  6.  9.  4.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [25. 25.  0.  0. 29.] 
adversary cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29. 29. 25. 29. 29. 29. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25] -> size -> 30 
adversary victory points: 4
player victory points: 1 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  0.  6. 11.] 
cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0.] 
cards in deck: 10 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0] -> size -> 35 
action values: 0 
buys: 1 
player value: 3 
card supply: [22. 27. 30. 26. 30.  8.  5.  7.  6.  9.  4.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [25. 25.  0.  0. 29.] 
adversary cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29. 29. 25. 29. 29. 29. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25] -> size -> 30 
adversary victory points: 4
player victory points: 1 





Player: 0 
cards in hand: [25. 25.  0.  0. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 25. 29.] 
expected returns: [[56.54238 ]
 [64.11302 ]
 [64.11302 ]
 [65.595695]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 25.  0.  0. 29.] 
cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29. 29. 25. 29. 29. 29. 15.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25] -> size -> 30 
action values: 1 
buys: 0 
player value: 0 
card supply: [22. 27. 30. 26. 30.  8.  5.  7.  6.  9.  4.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 0. 11.  8.  3.  6.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0. 11.  1.  0.  6. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0] -> size -> 35 
adversary victory points: 1
player victory points: 4 

Reward from previous game state: 
[-5  0  0 90  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 85 

action type: buy - action -1
Learning step: 0
desired expected reward: 49.06424331665039



action possibilites: [-1. 25.] 
expected returns: [[27.766884]
 [36.383297]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  0.  0.  0.] 
cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29. 29. 25. 29. 29. 29. 15. 25.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25] -> size -> 30 
action values: 1 
buys: 0 
player value: 1 
card supply: [22. 27. 30. 26. 30.  8.  5.  7.  6.  9.  4.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 0. 11.  8.  3.  6.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0. 11.  1.  0.  6. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0] -> size -> 35 
adversary victory points: 1
player victory points: 4 

Reward from previous game state: 
[-5  0  0 90  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 105 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 61.7481803894043



action possibilites: [-1] 
expected returns: [[31.141888]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  0. 25. 15.] 
cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29. 29. 25. 29. 29. 29. 15. 25.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25] -> size -> 30 
action values: 0 
buys: 0 
player value: 1 
card supply: [22. 27. 30. 26. 30.  8.  4.  7.  6.  9.  4.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 0. 11.  8.  3.  6.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0. 11.  1.  0.  6. 11.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6] -> size -> 36 
adversary victory points: 1
player victory points: 4 

Reward from previous game state: 
[-5  0  0 90  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 125 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 36.38327407836914





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[27.004013 ]
 [34.40064  ]
 [29.765858 ]
 [-2.4653494]
 [31.234066 ]
 [33.514317 ]
 [28.495121 ]
 [28.868366 ]
 [30.276447 ]
 [35.137505 ]
 [28.890636 ]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0.  0. 25. 15.] 
cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29. 29. 25. 29. 29. 29. 15. 25.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25] -> size -> 30 
action values: 0 
buys: 1 
player value: 4 
card supply: [22. 27. 30. 26. 30.  8.  4.  7.  6.  9.  4.  0.  9. 10.  9. 10.  6.] 
adversary cards in hand: [ 0. 11.  8.  3.  6.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0. 11.  1.  0.  6. 11.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6] -> size -> 36 
adversary victory points: 1
player victory points: 4 

Reward from previous game state: 
[-5  0  0 90  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 125 

action type: take_action - action -1
Learning step: 0
desired expected reward: 31.141887664794922



buy possibilites: [-1] 
expected returns: [[7.075366]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0.  0. 25. 15.] 
cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29. 29. 25. 29. 29. 29. 15. 25.
 15.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15] -> size -> 31 
action values: 0 
buys: 0 
player value: 0 
card supply: [22. 27. 30. 26. 30.  8.  4.  7.  6.  9.  4.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 0. 11.  8.  3.  6.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0. 11.  1.  0.  6. 11.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6] -> size -> 36 
adversary victory points: 1
player victory points: 4 

Reward from previous game state: 
[ -5   0   0  90   0   0  40   0   0   0   0   0   0   0 128   0] 
sum of rewards: 253 

action type: buy - action 15.0
Learning step: 0
desired expected reward: 35.13750457763672






         -------------------- Turn: 25 -------------------- 
Player: 1 
cards in hand: [ 0. 11.  8.  3.  6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.  8.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 11.  8.  3.  6.] 
cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0. 11.  1.  0.  6. 11.  6.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6] -> size -> 36 
action values: 1 
buys: 0 
player value: 0 
card supply: [22. 27. 30. 26. 30.  8.  4.  7.  6.  9.  4.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [29.  1. 15.  3.  1.] 
adversary cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29. 29. 25. 29. 29. 29. 15. 25.
 15. 29. 25.  0.  0.  0. 25. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15] -> size -> 31 
adversary victory points: 4
player victory points: 0 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 8. 3. 6.] 
cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0. 11.  1.  0.  6. 11.  6. 16.] 
cards in deck: 5 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16] -> size -> 37 
action values: 0 
buys: 0 
player value: 0 
card supply: [22. 27. 30. 26. 30.  8.  4.  6.  6.  9.  4.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [29.  1. 15.  3.  1.] 
adversary cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29. 29. 25. 29. 29. 29. 15. 25.
 15. 29. 25.  0.  0.  0. 25. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15] -> size -> 31 
adversary victory points: 4
player victory points: 0 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 8. 3. 6.] 
cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0. 11.  1.  0.  6. 11.  6. 16.] 
cards in deck: 5 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16] -> size -> 37 
action values: 0 
buys: 1 
player value: 1 
card supply: [22. 27. 30. 26. 30.  8.  4.  6.  6.  9.  4.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [29.  1. 15.  3.  1.] 
adversary cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29. 29. 25. 29. 29. 29. 15. 25.
 15. 29. 25.  0.  0.  0. 25. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15] -> size -> 31 
adversary victory points: 4
player victory points: 0 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 8. 3. 6.] 
cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0. 11.  1.  0.  6. 11.  6. 16.  0.] 
cards in deck: 5 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0] -> size -> 38 
action values: 0 
buys: 0 
player value: 1 
card supply: [21. 27. 30. 26. 30.  8.  4.  6.  6.  9.  4.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [29.  1. 15.  3.  1.] 
adversary cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29. 29. 25. 29. 29. 29. 15. 25.
 15. 29. 25.  0.  0.  0. 25. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15] -> size -> 31 
adversary victory points: 4
player victory points: 0 





Player: 0 
cards in hand: [29.  1. 15.  3.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 15.] 
expected returns: [[14.578422]
 [23.949297]
 [20.631504]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  1. 15.  3.  1.] 
cards in discard: [ 3.  3. 29. 25. 25.  3.  0. 29. 29. 15. 29. 29. 25. 29. 29. 29. 15. 25.
 15. 29. 25.  0.  0.  0. 25. 15.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15] -> size -> 31 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 27. 30. 26. 30.  8.  4.  6.  6.  9.  4.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 0.  0.  3.  0. 16.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0. 11.  1.  0.  6. 11.  6. 16.  0. 11.  0.  8.  3.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0] -> size -> 38 
adversary victory points: 0
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 120   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 115 

action type: buy - action -1
Learning step: 0
desired expected reward: 7.075366020202637



action possibilites: [-1. 15. 25.] 
expected returns: [[ 9.533992]
 [17.737871]
 [19.974028]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 15.  1. 25.] 
cards in discard: [3.] 
cards in deck: 25 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15] -> size -> 31 
action values: 1 
buys: 0 
player value: 1 
card supply: [21. 27. 30. 26. 30.  8.  4.  6.  6.  9.  4.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 0.  0.  3.  0. 16.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0. 11.  1.  0.  6. 11.  6. 16.  0. 11.  0.  8.  3.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0] -> size -> 38 
adversary victory points: 0
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 120   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 135 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 19.934036254882812



action possibilites: [-1] 
expected returns: [[5.4208755]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 15.  1.  0.  0.] 
cards in discard: [3.] 
cards in deck: 23 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15] -> size -> 31 
action values: 0 
buys: 0 
player value: 1 
card supply: [21. 27. 30. 26. 30.  8.  3.  6.  6.  9.  4.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 0.  0.  3.  0. 16.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0. 11.  1.  0.  6. 11.  6. 16.  0. 11.  0.  8.  3.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6] -> size -> 39 
adversary victory points: 0
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 120   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 155 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 19.974029541015625





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[  1.5606883]
 [ 10.287372 ]
 [-15.336075 ]
 [  4.9562263]
 [ -9.719711 ]
 [-29.199144 ]
 [  6.176673 ]
 [  9.336054 ]
 [  3.8849576]
 [ 13.821073 ]
 [  3.6491506]
 [ 11.384034 ]
 [  5.206773 ]
 [ -7.62389  ]
 [ 11.364291 ]
 [  3.1229098]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1. 15.  1.  0.  0.] 
cards in discard: [3.] 
cards in deck: 23 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15] -> size -> 31 
action values: 0 
buys: 1 
player value: 7 
card supply: [21. 27. 30. 26. 30.  8.  3.  6.  6.  9.  4.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 0.  0.  3.  0. 16.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0. 11.  1.  0.  6. 11.  6. 16.  0. 11.  0.  8.  3.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6] -> size -> 39 
adversary victory points: 0
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 120   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 155 

action type: take_action - action -1
Learning step: 0
desired expected reward: 5.420875549316406



buy possibilites: [-1] 
expected returns: [[3.9016883]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1. 15.  1.  0.  0.] 
cards in discard: [ 3. 25.] 
cards in deck: 23 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25] -> size -> 32 
action values: 0 
buys: 0 
player value: 2 
card supply: [21. 27. 30. 26. 30.  8.  3.  6.  6.  9.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 0.  0.  3.  0. 16.] 
adversary cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0. 11.  1.  0.  6. 11.  6. 16.  0. 11.  0.  8.  3.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6] -> size -> 39 
adversary victory points: 0
player victory points: 4 

Reward from previous game state: 
[ -5.    0.    0.  120.    0.    0.   40.    0.    0.    0.    0.    0.
   0.    0.   62.5   0. ] 
sum of rewards: 217.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 13.821063041687012






         -------------------- Turn: 26 -------------------- 
Player: 1 
cards in hand: [ 0.  0.  3.  0. 16.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  3.  0. 16.] 
cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0. 11.  1.  0.  6. 11.  6. 16.  0. 11.  0.  8.  3.  6.  6.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6] -> size -> 39 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 27. 30. 26. 30.  8.  3.  6.  6.  9.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [25. 15. 29. 25. 29.] 
adversary cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25] -> size -> 32 
adversary victory points: 4
player victory points: -1 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0.  3.  0. 16.] 
cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0. 11.  1.  0.  6. 11.  6. 16.  0. 11.  0.  8.  3.  6.  6.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6] -> size -> 39 
action values: 0 
buys: 1 
player value: 3 
card supply: [21. 27. 30. 26. 30.  8.  3.  6.  6.  9.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [25. 15. 29. 25. 29.] 
adversary cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25] -> size -> 32 
adversary victory points: 4
player victory points: -1 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0.  3.  0. 16.] 
cards in discard: [14. 11.  3.  3.  3.  0.  6.  0.  0.  0. 10.  0.  6.  6.  3. 16.  0. 16.
  0.  0. 11.  1.  0.  6. 11.  6. 16.  0. 11.  0.  8.  3.  6.  6.  3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3] -> size -> 40 
action values: 0 
buys: 0 
player value: 1 
card supply: [21. 27. 30. 25. 30.  8.  3.  6.  6.  9.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [25. 15. 29. 25. 29.] 
adversary cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25] -> size -> 32 
adversary victory points: 4
player victory points: 0 





Player: 0 
cards in hand: [25. 15. 29. 25. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 15. 29. 25. 29.] 
expected returns: [[69.59956 ]
 [79.80928 ]
 [77.610664]
 [81.9147  ]
 [79.80928 ]
 [81.9147  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 15. 29. 25. 29.] 
cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0.] 
cards in deck: 18 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25] -> size -> 32 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 27. 30. 25. 30.  8.  3.  6.  6.  9.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 0.  3.  0. 16.  6.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3] -> size -> 40 
adversary victory points: 0
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 120   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 115 

action type: buy - action -1
Learning step: 0
desired expected reward: 3.90168833732605



action possibilites: [-1. 25. 15. 25.] 
expected returns: [[19.705597]
 [29.126442]
 [27.104538]
 [29.126442]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 15. 25.  0.] 
cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0. 29.] 
cards in deck: 17 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25] -> size -> 32 
action values: 1 
buys: 0 
player value: 1 
card supply: [21. 27. 30. 25. 30.  8.  3.  6.  6.  9.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 0.  3.  0. 16.  6.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3] -> size -> 40 
adversary victory points: 0
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 120   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 135 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 76.62908935546875



action possibilites: [-1] 
expected returns: [[39.72886]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 25.  0. 29. 15.] 
cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0. 29.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25] -> size -> 32 
action values: 0 
buys: 0 
player value: 1 
card supply: [21. 27. 30. 25. 30.  8.  2.  6.  6.  9.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 0.  3.  0. 16.  6.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6] -> size -> 41 
adversary victory points: 0
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 120   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 155 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 29.12643814086914





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
expected returns: [[36.902653]
 [39.935375]
 [ 5.070715]
 [38.76754 ]
 [38.818813]]
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15. 25.  0. 29. 15.] 
cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0. 29.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25] -> size -> 32 
action values: 0 
buys: 1 
player value: 2 
card supply: [21. 27. 30. 25. 30.  8.  2.  6.  6.  9.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 0.  3.  0. 16.  6.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6] -> size -> 41 
adversary victory points: 0
player victory points: 4 

Reward from previous game state: 
[ -5   0   0 120   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 155 

action type: take_action - action -1
Learning step: 0
desired expected reward: 39.728858947753906



buy possibilites: [-1] 
expected returns: [[55.139454]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15. 25.  0. 29. 15.] 
cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0. 29.  3.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3] -> size -> 33 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 27. 30. 24. 30.  8.  2.  6.  6.  9.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 0.  3.  0. 16.  6.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6] -> size -> 41 
adversary victory points: 0
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 150   0   0  40   0   0   0   0   0   0   0  16   0] 
sum of rewards: 201 

action type: buy - action 3.0
Learning step: 0
desired expected reward: 39.93537902832031






         -------------------- Turn: 27 -------------------- 
Player: 1 
cards in hand: [ 0.  3.  0. 16.  6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3.  0. 16.  6.] 
cards in discard: [6.] 
cards in deck: 35 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6] -> size -> 41 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 27. 30. 24. 30.  8.  2.  6.  6.  9.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [15.  0. 29. 29. 25.] 
adversary cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0. 29.  3. 29. 25. 15. 25.  0. 29. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3] -> size -> 33 
adversary victory points: 5
player victory points: -1 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3.  0. 16.  6.] 
cards in discard: [6.] 
cards in deck: 35 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6] -> size -> 41 
action values: 0 
buys: 1 
player value: 2 
card supply: [21. 27. 30. 24. 30.  8.  2.  6.  6.  9.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [15.  0. 29. 29. 25.] 
adversary cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0. 29.  3. 29. 25. 15. 25.  0. 29. 15.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3] -> size -> 33 
adversary victory points: 5
player victory points: -1 


 --------- NOTHING HAPPENED --------- 



Player: 0 
cards in hand: [15.  0. 29. 29. 25.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 29. 29. 25.] 
expected returns: [[56.9982 ]
 [63.06415]
 [66.32569]
 [66.32569]
 [64.71339]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0. 29. 29. 25.] 
cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0. 29.  3. 29. 25. 15. 25.  0. 29. 15.] 
cards in deck: 10 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3] -> size -> 33 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 27. 30. 24. 30.  8.  2.  6.  6.  9.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 3. 16.  3.  0. 11.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6] -> size -> 41 
adversary victory points: -1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 180   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 175 

action type: buy - action -1
Learning step: 0
desired expected reward: 55.13945388793945



action possibilites: [-1. 15. 25. 25.] 
expected returns: [[ 5.1235514]
 [11.659008 ]
 [13.574586 ]
 [13.574586 ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0. 25. 25.] 
cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0. 29.  3. 29. 25. 15. 25.  0. 29. 15.
 29.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3] -> size -> 33 
action values: 1 
buys: 0 
player value: 1 
card supply: [21. 27. 30. 24. 30.  8.  2.  6.  6.  9.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 3. 16.  3.  0. 11.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6] -> size -> 41 
adversary victory points: -1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 180   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 195 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 62.34798812866211



action possibilites: [-1] 
expected returns: [[11.230332]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0. 25. 29. 25.] 
cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0. 29.  3. 29. 25. 15. 25.  0. 29. 15.
 29.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3] -> size -> 33 
action values: 0 
buys: 0 
player value: 1 
card supply: [21. 27. 30. 24. 30.  8.  1.  6.  6.  9.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 3. 16.  3.  0. 11.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6] -> size -> 42 
adversary victory points: -1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 180   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 215 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 13.574576377868652





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
expected returns: [[  7.9040403]
 [ 10.600074 ]
 [-21.09167  ]
 [  9.336555 ]
 [  9.785711 ]]
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  0. 25. 29. 25.] 
cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0. 29.  3. 29. 25. 15. 25.  0. 29. 15.
 29.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3] -> size -> 33 
action values: 0 
buys: 1 
player value: 2 
card supply: [21. 27. 30. 24. 30.  8.  1.  6.  6.  9.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 3. 16.  3.  0. 11.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6] -> size -> 42 
adversary victory points: -1
player victory points: 5 

Reward from previous game state: 
[ -5   0   0 180   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 215 

action type: take_action - action -1
Learning step: 0
desired expected reward: 11.230332374572754



buy possibilites: [-1] 
expected returns: [[-5.6499815]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  0. 25. 29. 25.] 
cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0. 29.  3. 29. 25. 15. 25.  0. 29. 15.
 29.  3.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3  3] -> size -> 34 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  9.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 3. 16.  3.  0. 11.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6] -> size -> 42 
adversary victory points: -1
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 210   0   0  40   0   0   0   0   0   0   0  16   0] 
sum of rewards: 261 

action type: buy - action 3.0
Learning step: 0
desired expected reward: 10.600052833557129






         -------------------- Turn: 28 -------------------- 
Player: 1 
cards in hand: [ 3. 16.  3.  0. 11.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16. 11.] 
Chosen action: 16 : ['Remodel' '16' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 16.  3.  0. 11.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.] 
cards in deck: 30 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16
  6  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6] -> size -> 42 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  9.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [29. 29. 29.  3.  3.] 
adversary cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0. 29.  3. 29. 25. 15. 25.  0. 29. 15.
 29.  3. 29. 25. 15.  0. 25. 29. 25.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3  3] -> size -> 34 
adversary victory points: 6
player victory points: -2 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 11.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8.] 
cards in deck: 30 
card top of deck: [] 
played cards: [16.] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [29. 29. 29.  3.  3.] 
adversary cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0. 29.  3. 29. 25. 15. 25.  0. 29. 15.
 29.  3. 29. 25. 15.  0. 25. 29. 25.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3  3] -> size -> 34 
adversary victory points: 6
player victory points: -3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0. 11.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8.] 
cards in deck: 30 
card top of deck: [] 
played cards: [16.] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
action values: 0 
buys: 1 
player value: 1 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [29. 29. 29.  3.  3.] 
adversary cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0. 29.  3. 29. 25. 15. 25.  0. 29. 15.
 29.  3. 29. 25. 15.  0. 25. 29. 25.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3  3] -> size -> 34 
adversary victory points: 6
player victory points: -3 





Player: 0 
cards in hand: [29. 29. 29.  3.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29.] 
expected returns: [[-4.4665737]
 [ 5.28069  ]
 [ 5.28069  ]
 [ 5.28069  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29. 29.  3.  3.] 
cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0. 29.  3. 29. 25. 15. 25.  0. 29. 15.
 29.  3. 29. 25. 15.  0. 25. 29. 25.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3  3] -> size -> 34 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 3.  6.  0.  0. 16.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 265 

action type: buy - action -1
Learning step: 0
desired expected reward: -5.649981498718262



action possibilites: [-1. 29. 29.] 
expected returns: [[ 3.2932394]
 [13.73669  ]
 [13.73669  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29.  3.  3.] 
cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0. 29.  3. 29. 25. 15. 25.  0. 29. 15.
 29.  3. 29. 25. 15.  0. 25. 29. 25.  3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3  3] -> size -> 34 
action values: 1 
buys: 0 
player value: 1 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 3.  6.  0.  0. 16.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 285 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 1.1041786670684814



action possibilites: [-1. 29. 15.] 
expected returns: [[ 8.192109]
 [18.683672]
 [15.021676]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  3. 15.] 
cards in discard: [ 3. 25. 29. 25.  1. 15.  1.  0.  0. 29.  3. 29. 25. 15. 25.  0. 29. 15.
 29.  3. 29. 25. 15.  0. 25. 29. 25.  3.  3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3  3] -> size -> 34 
action values: 1 
buys: 0 
player value: 2 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 3.  6.  0.  0. 16.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 305 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 9.256802558898926



action possibilites: [-1. 15. 29.] 
expected returns: [[-24.712448]
 [-17.019047]
 [-12.843997]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 29.] 
cards in discard: [3.] 
cards in deck: 28 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3  3] -> size -> 34 
action values: 1 
buys: 0 
player value: 3 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 3.  6.  0.  0. 16.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 325 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 14.208535194396973



action possibilites: [-1.] 
expected returns: [[-14.842267]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0.] 
cards in discard: [ 3. 15.] 
cards in deck: 27 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3  3] -> size -> 34 
action values: 1 
buys: 0 
player value: 4 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 3.  6.  0.  0. 16.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  80   0   0   0   0   0   0   0   0   0] 
sum of rewards: 345 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -17.926006317138672





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[-16.200748 ]
 [ -7.960907 ]
 [-13.036218 ]
 [-26.89273  ]
 [-45.513866 ]
 [-11.759611 ]
 [ -9.058207 ]
 [-14.401108 ]
 [ -5.0714273]
 [-14.197174 ]
 [ -7.035084 ]
 [-12.76129  ]
 [-25.051945 ]
 [ -7.091655 ]
 [-14.545177 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [ 3. 15.] 
cards in deck: 27 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3  3] -> size -> 34 
action values: 0 
buys: 1 
player value: 5 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  3.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 3.  6.  0.  0. 16.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  80   0   0   0   0   0   0   0   0   0] 
sum of rewards: 345 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -14.842267036437988



buy possibilites: [-1] 
expected returns: [[22.145737]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0.] 
cards in discard: [ 3. 15. 25.] 
cards in deck: 27 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3  3 25] -> size -> 35 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  2.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [ 3.  6.  0.  0. 16.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  80   0   0   0   0   0   0   0 250   0] 
sum of rewards: 595 

action type: buy - action 25.0
Learning step: 0
desired expected reward: -5.0714430809021






         -------------------- Turn: 29 -------------------- 
Player: 1 
cards in hand: [ 3.  6.  0.  0. 16.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  6.  0.  0. 16.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.] 
cards in deck: 25 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  2.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [15.  0.  3. 29.  1.] 
adversary cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3  3 25] -> size -> 35 
adversary victory points: 6
player victory points: -3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  6.  0.  0. 16.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.] 
cards in deck: 25 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
action values: 0 
buys: 1 
player value: 2 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  2.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [15.  0.  3. 29.  1.] 
adversary cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.] 
adversary owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3  3 25] -> size -> 35 
adversary victory points: 6
player victory points: -3 


 --------- NOTHING HAPPENED --------- 



Player: 0 
cards in hand: [15.  0.  3. 29.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 29.] 
expected returns: [[36.596737]
 [44.46677 ]
 [48.614323]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0.  3. 29.  1.] 
cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.] 
cards in deck: 22 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3  3 25] -> size -> 35 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  2.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [0. 0. 3. 6. 6.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 265 

action type: buy - action -1
Learning step: 0
desired expected reward: 22.145736694335938



action possibilites: [-1. 15. 15.] 
expected returns: [[45.784344]
 [53.913876]
 [53.913876]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0.  3. 15.] 
cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15
 15 25 25 25  3 25 15 25  3  3 25] -> size -> 35 
action values: 1 
buys: 0 
player value: 1 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  2.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [0. 0. 3. 6. 6.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 285 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 43.50405502319336



action possibilites: [-1] 
expected returns: [[-11.104306]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 15.] 
cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25] -> size -> 34 
action values: 0 
buys: 0 
player value: 4 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  2.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [0. 0. 3. 6. 6.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 305 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 53.91386032104492





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[-12.501037 ]
 [ -4.1154194]
 [ -9.211323 ]
 [-39.93989  ]
 [ -8.050088 ]
 [ -5.2240725]
 [-10.706735 ]
 [-10.454086 ]
 [ -9.069477 ]
 [ -3.2023149]
 [-10.951384 ]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 15.] 
cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25] -> size -> 34 
action values: 0 
buys: 1 
player value: 4 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  2.  0.  9. 10.  9. 10.  5.] 
adversary cards in hand: [0. 0. 3. 6. 6.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 305 

action type: take_action - action -1
Learning step: 0
desired expected reward: -11.1043062210083



buy possibilites: [-1] 
expected returns: [[-3.1545835]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 15.] 
cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1. 15.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15] -> size -> 35 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [0. 0. 3. 6. 6.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  40   0   0   0   0   0   0   0 128   0] 
sum of rewards: 433 

action type: buy - action 15.0
Learning step: 0
desired expected reward: -3.202317953109741






         -------------------- Turn: 30 -------------------- 
Player: 1 
cards in hand: [0. 0. 3. 6. 6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3. 6. 6.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [29. 25. 29.  1. 29.] 
adversary cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1. 15. 29. 15.  3. 15.] 
adversary owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15] -> size -> 35 
adversary victory points: 6
player victory points: -3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 6. 6.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
action values: 0 
buys: 1 
player value: 2 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [29. 25. 29.  1. 29.] 
adversary cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1. 15. 29. 15.  3. 15.] 
adversary owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15] -> size -> 35 
adversary victory points: 6
player victory points: -3 


 --------- NOTHING HAPPENED --------- 



Player: 0 
cards in hand: [29. 25. 29.  1. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 25. 29. 29.] 
expected returns: [[-1.5864811]
 [ 8.339502 ]
 [ 6.5629454]
 [ 8.339502 ]
 [ 8.339502 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 25. 29.  1. 29.] 
cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1. 15. 29. 15.  3. 15.] 
cards in deck: 16 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15] -> size -> 35 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [16.  0.  6. 14. 10.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 265 

action type: buy - action -1
Learning step: 0
desired expected reward: -3.15458345413208



action possibilites: [-1. 25. 29. 29. 25.] 
expected returns: [[15.778459]
 [23.921965]
 [25.698275]
 [25.698275]
 [23.921965]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 29. 29. 25.] 
cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1. 15. 29. 15.  3. 15.  1.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15] -> size -> 35 
action values: 1 
buys: 0 
player value: 1 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [16.  0.  6. 14. 10.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 285 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 4.090478897094727



action possibilites: [-1. 25. 29. 25.] 
expected returns: [[-2.3757946]
 [ 5.0034485]
 [ 6.6277666]
 [ 5.0034485]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 29. 25.] 
cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1. 15. 29. 15.  3. 15.  1.  3.] 
cards in deck: 14 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15] -> size -> 35 
action values: 1 
buys: 0 
player value: 2 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [16.  0.  6. 14. 10.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 305 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 21.456153869628906



action possibilites: [-1. 25.] 
expected returns: [[13.922381]
 [23.271418]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  3.] 
cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1. 15. 29. 15.  3. 15.  1.  3. 25.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15] -> size -> 35 
action values: 1 
buys: 0 
player value: 3 
card supply: [21. 27. 30. 23. 30.  8.  1.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [16.  0.  6. 14. 10.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8] -> size -> 42 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 325 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 2.7709319591522217



action possibilites: [-1] 
expected returns: [[-12.615691]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29. 25.] 
cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1. 15. 29. 15.  3. 15.  1.  3. 25.] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29. 29. 25.] 
owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15] -> size -> 35 
action values: 0 
buys: 0 
player value: 3 
card supply: [21. 27. 30. 23. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [16.  0.  6. 14. 10.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6] -> size -> 43 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  80   0   0   0   0   0   0   0   0   0] 
sum of rewards: 345 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 23.27139663696289





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3. 11.  8. 10. -1.] 
expected returns: [[-15.415886 ]
 [ -8.414549 ]
 [-12.844253 ]
 [ -9.318972 ]
 [-14.0791855]
 [-12.342222 ]
 [-13.591294 ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 29. 25.] 
cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1. 15. 29. 15.  3. 15.  1.  3. 25.] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29. 29. 25.] 
owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15] -> size -> 35 
action values: 0 
buys: 1 
player value: 3 
card supply: [21. 27. 30. 23. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [16.  0.  6. 14. 10.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6] -> size -> 43 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  80   0   0   0   0   0   0   0   0   0] 
sum of rewards: 345 

action type: take_action - action -1
Learning step: 0
desired expected reward: -12.615691184997559



buy possibilites: [-1] 
expected returns: [[-9.0269575]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 29. 25.] 
cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1. 15. 29. 15.  3. 15.  1.  3. 25.  1.] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29. 29. 25.] 
owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15  1] -> size -> 36 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 23. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [16.  0.  6. 14. 10.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6] -> size -> 43 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  80   0   0   0   0 -10   0   0  54   0] 
sum of rewards: 389 

action type: buy - action 1.0
Learning step: 0
desired expected reward: -8.414568901062012






         -------------------- Turn: 31 -------------------- 
Player: 1 
cards in hand: [16.  0.  6. 14. 10.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16. 14. 10.] 
Chosen action: 14 : ['Militia' '14' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [16.  0.  6. 14. 10.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6] -> size -> 43 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 23. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [ 0.  3. 25. 15.  3.] 
adversary cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1. 15. 29. 15.  3. 15.  1.  3. 25.  1.
 29. 29. 29. 25.  3. 29. 25.] 
adversary owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15  1] -> size -> 36 
adversary victory points: 6
player victory points: -4 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [16.  0.  6. 10.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.] 
cards in deck: 15 
card top of deck: [] 
played cards: [14.] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6] -> size -> 43 
action values: 0 
buys: 0 
player value: 2 
card supply: [21. 26. 30. 23. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [ 3. 25.  3.] 
adversary cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1. 15. 29. 15.  3. 15.  1.  3. 25.  1.
 29. 29. 29. 25.  3. 29. 25.  0. 15.] 
adversary owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15  1] -> size -> 36 
adversary victory points: 6
player victory points: -4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3. 11.  8. 10. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [16.  0.  6. 10.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.] 
cards in deck: 15 
card top of deck: [] 
played cards: [14.] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6] -> size -> 43 
action values: 0 
buys: 1 
player value: 3 
card supply: [21. 26. 30. 23. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [ 3. 25.  3.] 
adversary cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1. 15. 29. 15.  3. 15.  1.  3. 25.  1.
 29. 29. 29. 25.  3. 29. 25.  0. 15.] 
adversary owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15  1] -> size -> 36 
adversary victory points: 6
player victory points: -4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [16.  0.  6. 10.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3.] 
cards in deck: 15 
card top of deck: [] 
played cards: [14.] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3] -> size -> 44 
action values: 0 
buys: 0 
player value: 1 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [ 3. 25.  3.] 
adversary cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1. 15. 29. 15.  3. 15.  1.  3. 25.  1.
 29. 29. 29. 25.  3. 29. 25.  0. 15.] 
adversary owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15  1] -> size -> 36 
adversary victory points: 6
player victory points: -3 





Player: 0 
cards in hand: [ 3. 25.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25.] 
expected returns: [[-1.9335364]
 [ 5.969758 ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 25.  3.] 
cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1. 15. 29. 15.  3. 15.  1.  3. 25.  1.
 29. 29. 29. 25.  3. 29. 25.  0. 15.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15  1] -> size -> 36 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [11.  3.  8.  1.  6.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3] -> size -> 44 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 265 

action type: discard_down_to_3_cards - action 5
Learning step: 0
desired expected reward: -53.7219123840332



action possibilites: [-1] 
expected returns: [[-11.971671]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  3. 25. 25.] 
cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1. 15. 29. 15.  3. 15.  1.  3. 25.  1.
 29. 29. 29. 25.  3. 29. 25.  0. 15.] 
cards in deck: 4 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15  1] -> size -> 36 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [11.  3.  8.  1.  6.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3] -> size -> 44 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 285 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 5.969753265380859





----------- BUY PHASE ----------- 
buy possibilites: [ 0. -1.] 
expected returns: [[-13.860911]
 [-12.190314]]
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  3. 25. 25.] 
cards in discard: [ 3. 15. 25. 29. 29. 29. 29.  0.  1. 15. 29. 15.  3. 15.  1.  3. 25.  1.
 29. 29. 29. 25.  3. 29. 25.  0. 15.] 
cards in deck: 4 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15  1] -> size -> 36 
action values: 0 
buys: 1 
player value: 0 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [11.  3.  8.  1.  6.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3] -> size -> 44 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 285 

action type: take_action - action -1
Learning step: 0
desired expected reward: -11.971671104431152






         -------------------- Turn: 32 -------------------- 
Player: 1 
cards in hand: [11.  3.  8.  1.  6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.  8.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [11.  3.  8.  1.  6.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10.] 
cards in deck: 10 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3] -> size -> 44 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  9. 10.  4.] 
adversary cards in hand: [15. 15. 25. 29.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15  1] -> size -> 36 
adversary victory points: 6
player victory points: -3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 8. 1. 6.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10.] 
cards in deck: 10 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10] -> size -> 45 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  8. 10.  4.] 
adversary cards in hand: [15. 15. 25. 29.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15  1] -> size -> 36 
adversary victory points: 6
player victory points: -3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  8. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 8. 1. 6.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10.] 
cards in deck: 10 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10] -> size -> 45 
action values: 0 
buys: 1 
player value: 2 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  8. 10.  4.] 
adversary cards in hand: [15. 15. 25. 29.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15  1] -> size -> 36 
adversary victory points: 6
player victory points: -3 





Player: 0 
cards in hand: [15. 15. 25. 29.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 15. 25. 29.] 
expected returns: [[-6.3724794]
 [ 1.1003006]
 [ 1.1003006]
 [ 3.218272 ]
 [ 5.2844286]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 15. 25. 29.  0.] 
cards in discard: [] 
cards in deck: 31 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15  1] -> size -> 36 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  8. 10.  4.] 
adversary cards in hand: [ 0.  0.  6. 11.  3.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10] -> size -> 45 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 265 

action type: buy - action -1.0
Learning step: 0
desired expected reward: -12.1903076171875



action possibilites: [-1. 15.] 
expected returns: [[0.04231882]
 [7.114353  ]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0.  3.] 
cards in discard: [15. 25.] 
cards in deck: 30 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15
 25 25 25  3 25 15 25  3  3 25 15  1] -> size -> 36 
action values: 1 
buys: 0 
player value: 1 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  8. 10.  4.] 
adversary cards in hand: [ 0.  0.  6. 11.  3.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10] -> size -> 45 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 285 

action type: discard_n_cards - action 5
Learning step: 0
desired expected reward: 2.245857000350952



action possibilites: [-1] 
expected returns: [[-23.94656]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3.] 
cards in discard: [15. 25.] 
cards in deck: 30 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1] -> size -> 35 
action values: 0 
buys: 0 
player value: 4 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  8. 10.  4.] 
adversary cards in hand: [ 0.  0.  6. 11.  3.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10] -> size -> 45 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 305 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 7.114352226257324





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[-25.684084]
 [-17.240082]
 [-22.44686 ]
 [-21.14958 ]
 [-18.339775]
 [-23.686914]
 [-23.654985]
 [-22.139269]
 [-16.331537]
 [-24.037458]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3.] 
cards in discard: [15. 25.] 
cards in deck: 30 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1] -> size -> 35 
action values: 0 
buys: 1 
player value: 4 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  8. 10.  4.] 
adversary cards in hand: [ 0.  0.  6. 11.  3.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10] -> size -> 45 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 305 

action type: take_action - action -1
Learning step: 0
desired expected reward: -23.94655990600586



buy possibilites: [-1] 
expected returns: [[-11.73451]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3.] 
cards in discard: [15. 25. 15.] 
cards in deck: 30 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15] -> size -> 36 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  8. 10.  3.] 
adversary cards in hand: [ 0.  0.  6. 11.  3.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10] -> size -> 45 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  40   0   0   0   0 -10   0   0 128   0] 
sum of rewards: 423 

action type: buy - action 15.0
Learning step: 0
desired expected reward: -16.331525802612305






         -------------------- Turn: 33 -------------------- 
Player: 1 
cards in hand: [ 0.  0.  6. 11.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  6. 11.  3.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10] -> size -> 45 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  8.  2.  0.  9. 10.  8. 10.  3.] 
adversary cards in hand: [29. 15. 25.  1. 29.] 
adversary cards in discard: [15. 25. 15. 29. 15.  3.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15] -> size -> 36 
adversary victory points: 6
player victory points: -3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 6. 3.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.  8.] 
cards in deck: 5 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8] -> size -> 46 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  7.  2.  0.  9. 10.  8. 10.  3.] 
adversary cards in hand: [29. 15. 25.  1. 29.] 
adversary cards in discard: [15. 25. 15. 29. 15.  3.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15] -> size -> 36 
adversary victory points: 6
player victory points: -3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  8. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 6. 3.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.  8.] 
cards in deck: 5 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8] -> size -> 46 
action values: 0 
buys: 1 
player value: 2 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  7.  2.  0.  9. 10.  8. 10.  3.] 
adversary cards in hand: [29. 15. 25.  1. 29.] 
adversary cards in discard: [15. 25. 15. 29. 15.  3.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15] -> size -> 36 
adversary victory points: 6
player victory points: -3 





Player: 0 
cards in hand: [29. 15. 25.  1. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 15. 25. 29.] 
expected returns: [[ 8.537851]
 [20.305632]
 [16.206728]
 [18.300268]
 [20.305632]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 15. 25.  1. 29.] 
cards in discard: [15. 25. 15. 29. 15.  3.] 
cards in deck: 25 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15] -> size -> 36 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  7.  2.  0.  9. 10.  8. 10.  3.] 
adversary cards in hand: [ 0.  0.  0. 11.  0.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.  8.
 11.  0.  0.  6.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8] -> size -> 46 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 265 

action type: buy - action -1
Learning step: 0
desired expected reward: -11.73451042175293



action possibilites: [-1. 25. 29.] 
expected returns: [[ 3.1717775]
 [12.49709  ]
 [14.406926 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  1. 29.] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.] 
cards in deck: 24 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15] -> size -> 36 
action values: 1 
buys: 0 
player value: 1 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  7.  2.  0.  9. 10.  8. 10.  3.] 
adversary cards in hand: [ 0.  0.  0. 11.  0.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.  8.
 11.  0.  0.  6.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8] -> size -> 46 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 285 

action type: discard_n_cards - action 5
Learning step: 0
desired expected reward: 17.28043556213379



action possibilites: [-1. 25.] 
expected returns: [[36.725323]
 [46.10377 ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.] 
cards in deck: 23 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15] -> size -> 36 
action values: 1 
buys: 0 
player value: 2 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  7.  2.  0.  9. 10.  8. 10.  3.] 
adversary cards in hand: [ 0.  0.  0. 11.  0.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.  8.
 11.  0.  0.  6.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8] -> size -> 46 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 305 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 9.620365142822266



action possibilites: [-1] 
expected returns: [[19.722069]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  3.] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15] -> size -> 36 
action values: 0 
buys: 0 
player value: 2 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  7.  2.  0.  9. 10.  8. 10.  3.] 
adversary cards in hand: [ 0.  0.  0. 11.  0.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.  8.
 11.  0.  0.  6.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8] -> size -> 46 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 325 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 46.1037483215332





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  8. -1.] 
expected returns: [[17.710499]
 [20.690086]
 [19.658337]
 [19.450874]]
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [29.  3.] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15] -> size -> 36 
action values: 0 
buys: 1 
player value: 2 
card supply: [21. 26. 30. 22. 30.  8.  0.  6.  6.  7.  2.  0.  9. 10.  8. 10.  3.] 
adversary cards in hand: [ 0.  0.  0. 11.  0.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.  8.
 11.  0.  0.  6.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8] -> size -> 46 
adversary victory points: -3
player victory points: 6 

Reward from previous game state: 
[ -5   0   0 270   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 325 

action type: take_action - action -1
Learning step: 0
desired expected reward: 19.722068786621094



buy possibilites: [-1] 
expected returns: [[-3.9372354]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [29.  3.] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3] -> size -> 37 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 21. 30.  8.  0.  6.  6.  7.  2.  0.  9. 10.  8. 10.  3.] 
adversary cards in hand: [ 0.  0.  0. 11.  0.] 
adversary cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.  8.
 11.  0.  0.  6.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8] -> size -> 46 
adversary victory points: -3
player victory points: 7 

Reward from previous game state: 
[ -5   0   0 300   0   0  60   0   0   0   0 -20   0   0  16   0] 
sum of rewards: 351 

action type: buy - action 3.0
Learning step: 0
desired expected reward: 20.690086364746094






         -------------------- Turn: 34 -------------------- 
Player: 1 
cards in hand: [ 0.  0.  0. 11.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  0. 11.  0.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.  8.
 11.  0.  0.  6.  3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8] -> size -> 46 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 21. 30.  8.  0.  6.  6.  7.  2.  0.  9. 10.  8. 10.  3.] 
adversary cards in hand: [25.  3. 25. 25. 15.] 
adversary cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3] -> size -> 37 
adversary victory points: 7
player victory points: -3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 0.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.  8.
 11.  0.  0.  6.  3. 16.] 
cards in deck: 0 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16] -> size -> 47 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 21. 30.  8.  0.  5.  6.  7.  2.  0.  9. 10.  8. 10.  3.] 
adversary cards in hand: [25.  3. 25. 25. 15.] 
adversary cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3] -> size -> 37 
adversary victory points: 7
player victory points: -3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: 14.0 : ['Militia' '14' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.  8.
 11.  0.  0.  6.  3. 16.] 
cards in deck: 0 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16] -> size -> 47 
action values: 0 
buys: 1 
player value: 4 
card supply: [21. 26. 30. 21. 30.  8.  0.  5.  6.  7.  2.  0.  9. 10.  8. 10.  3.] 
adversary cards in hand: [25.  3. 25. 25. 15.] 
adversary cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3] -> size -> 37 
adversary victory points: 7
player victory points: -3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0.] 
cards in discard: [ 6.  0.  3.  0. 16.  6.  6.  8. 16.  3.  0. 11.  3.  6.  0.  0. 16.  0.
  0.  3.  6.  6.  6.  3. 14. 16.  0.  6. 10. 10. 11.  3.  8.  1.  6.  8.
 11.  0.  0.  6.  3. 16. 14.] 
cards in deck: 0 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14] -> size -> 48 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 21. 30.  8.  0.  5.  6.  7.  2.  0.  8. 10.  8. 10.  3.] 
adversary cards in hand: [25.  3. 25. 25. 15.] 
adversary cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3] -> size -> 37 
adversary victory points: 7
player victory points: -3 





Player: 0 
cards in hand: [25.  3. 25. 25. 15.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 25. 25. 15.] 
expected returns: [[11.135701]
 [20.229795]
 [20.229795]
 [20.229795]
 [18.335236]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  3. 25. 25. 15.] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3.] 
cards in deck: 16 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3] -> size -> 37 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 21. 30.  8.  0.  5.  6.  7.  2.  0.  8. 10.  8. 10.  3.] 
adversary cards in hand: [ 3.  3. 16.  6. 14.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14] -> size -> 48 
adversary victory points: -3
player victory points: 7 

Reward from previous game state: 
[ -5   0   0 300   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 295 

action type: buy - action -1
Learning step: 0
desired expected reward: -3.9372353553771973



action possibilites: [-1] 
expected returns: [[31.776573]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 25. 25. 15. 29.  0.] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3.] 
cards in deck: 14 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3] -> size -> 37 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 21. 30.  8.  0.  5.  6.  7.  2.  0.  8. 10.  8. 10.  3.] 
adversary cards in hand: [ 3.  3. 16.  6. 14.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14] -> size -> 48 
adversary victory points: -3
player victory points: 7 

Reward from previous game state: 
[ -5   0   0 300   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 315 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 20.229778289794922





----------- BUY PHASE ----------- 
buy possibilites: [ 0. -1.] 
expected returns: [[29.316257]
 [31.118435]]
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 25. 25. 15. 29.  0.] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3.] 
cards in deck: 14 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3] -> size -> 37 
action values: 0 
buys: 1 
player value: 1 
card supply: [21. 26. 30. 21. 30.  8.  0.  5.  6.  7.  2.  0.  8. 10.  8. 10.  3.] 
adversary cards in hand: [ 3.  3. 16.  6. 14.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14] -> size -> 48 
adversary victory points: -3
player victory points: 7 

Reward from previous game state: 
[ -5   0   0 300   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 315 

action type: take_action - action -1
Learning step: 0
desired expected reward: 31.776573181152344






         -------------------- Turn: 35 -------------------- 
Player: 1 
cards in hand: [ 3.  3. 16.  6. 14.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16. 14.] 
Chosen action: 16 : ['Remodel' '16' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  3. 16.  6. 14.] 
cards in discard: [] 
cards in deck: 43 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6
  0 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14] -> size -> 48 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 21. 30.  8.  0.  5.  6.  7.  2.  0.  8. 10.  8. 10.  3.] 
adversary cards in hand: [15.  3.  0. 29. 29.] 
adversary cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3. 25.  3.
 25. 25. 15. 29.  0.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3] -> size -> 37 
adversary victory points: 7
player victory points: -3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  6. 14.] 
cards in discard: [16.] 
cards in deck: 43 
card top of deck: [] 
played cards: [16.] 
owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16] -> size -> 48 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 21. 30.  8.  0.  4.  6.  7.  2.  0.  8. 10.  8. 10.  3.] 
adversary cards in hand: [15.  3.  0. 29. 29.] 
adversary cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3. 25.  3.
 25. 25. 15. 29.  0.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3] -> size -> 37 
adversary victory points: 7
player victory points: -4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  6. 14.] 
cards in discard: [16.] 
cards in deck: 43 
card top of deck: [] 
played cards: [16.] 
owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16] -> size -> 48 
action values: 0 
buys: 1 
player value: 0 
card supply: [21. 26. 30. 21. 30.  8.  0.  4.  6.  7.  2.  0.  8. 10.  8. 10.  3.] 
adversary cards in hand: [15.  3.  0. 29. 29.] 
adversary cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3. 25.  3.
 25. 25. 15. 29.  0.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3] -> size -> 37 
adversary victory points: 7
player victory points: -4 





Player: 0 
cards in hand: [15.  3.  0. 29. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 29. 29.] 
expected returns: [[-10.821007 ]
 [ -4.5917635]
 [ -1.1198514]
 [ -1.1198514]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  3.  0. 29. 29.] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3. 25.  3.
 25. 25. 15. 29.  0.] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3] -> size -> 37 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 21. 30.  8.  0.  4.  6.  7.  2.  0.  8. 10.  8. 10.  3.] 
adversary cards in hand: [3. 0. 1. 0. 0.] 
adversary cards in discard: [16. 16.  3.  6. 14.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16] -> size -> 48 
adversary victory points: -4
player victory points: 7 

Reward from previous game state: 
[ -5   0   0 330   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 325 

action type: buy - action -1.0
Learning step: 0
desired expected reward: 31.118427276611328



action possibilites: [-1. 15. 29.] 
expected returns: [[-9.680313  ]
 [-3.451691  ]
 [ 0.01909947]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 29.  1.] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3. 25.  3.
 25. 25. 15. 29.  0.  3.  0.] 
cards in deck: 8 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3] -> size -> 37 
action values: 1 
buys: 0 
player value: 1 
card supply: [21. 26. 30. 21. 30.  8.  0.  4.  6.  7.  2.  0.  8. 10.  8. 10.  3.] 
adversary cards in hand: [3. 0. 1. 0. 0.] 
adversary cards in discard: [16. 16.  3.  6. 14.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16] -> size -> 48 
adversary victory points: -4
player victory points: 7 

Reward from previous game state: 
[ -5   0   0 330   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 345 

action type: discard_n_cards - action 5
Learning step: 0
desired expected reward: -3.516950845718384



action possibilites: [-1. 15.] 
expected returns: [[-5.8210945 ]
 [ 0.41584134]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3. 25.  3.
 25. 25. 15. 29.  0.  3.  0.  1. 25.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3] -> size -> 37 
action values: 1 
buys: 0 
player value: 2 
card supply: [21. 26. 30. 21. 30.  8.  0.  4.  6.  7.  2.  0.  8. 10.  8. 10.  3.] 
adversary cards in hand: [3. 0. 1. 0. 0.] 
adversary cards in discard: [16. 16.  3.  6. 14.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16] -> size -> 48 
adversary victory points: -4
player victory points: 7 

Reward from previous game state: 
[ -5   0   0 330   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 365 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -4.151042938232422



action possibilites: [-1] 
expected returns: [[2.0459592]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3. 25.  3.
 25. 25. 15. 29.  0.  3.  0.  1. 25.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3] -> size -> 37 
action values: 0 
buys: 0 
player value: 2 
card supply: [21. 26. 30. 21. 30.  8.  0.  4.  6.  7.  2.  0.  8. 10.  8. 10.  3.] 
adversary cards in hand: [3. 0. 1. 0. 0.] 
adversary cards in discard: [16. 16.  3.  6. 14.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16] -> size -> 48 
adversary victory points: -4
player victory points: 7 

Reward from previous game state: 
[ -5   0   0 330   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 385 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 0.4158492088317871





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  8. -1.] 
expected returns: [[0.38834548]
 [3.124139  ]
 [1.4938352 ]
 [2.0236933 ]]
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3. 25.  3.
 25. 25. 15. 29.  0.  3.  0.  1. 25.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3] -> size -> 37 
action values: 0 
buys: 1 
player value: 2 
card supply: [21. 26. 30. 21. 30.  8.  0.  4.  6.  7.  2.  0.  8. 10.  8. 10.  3.] 
adversary cards in hand: [3. 0. 1. 0. 0.] 
adversary cards in discard: [16. 16.  3.  6. 14.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16] -> size -> 48 
adversary victory points: -4
player victory points: 7 

Reward from previous game state: 
[ -5   0   0 330   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 385 

action type: take_action - action -1
Learning step: 0
desired expected reward: 2.045959234237671



buy possibilites: [-1] 
expected returns: [[-2.2217097]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3. 25.  3.
 25. 25. 15. 29.  0.  3.  0.  1. 25.  3.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3] -> size -> 38 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 20. 30.  8.  0.  4.  6.  7.  2.  0.  8. 10.  8. 10.  3.] 
adversary cards in hand: [3. 0. 1. 0. 0.] 
adversary cards in discard: [16. 16.  3.  6. 14.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16] -> size -> 48 
adversary victory points: -4
player victory points: 8 

Reward from previous game state: 
[ -5   0   0 360   0   0  60   0   0   0   0 -30   0   0  16   0] 
sum of rewards: 401 

action type: buy - action 3.0
Learning step: 0
desired expected reward: 3.124133825302124






         -------------------- Turn: 36 -------------------- 
Player: 1 
cards in hand: [3. 0. 1. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 1. 0. 0.] 
cards in discard: [16. 16.  3.  6. 14.] 
cards in deck: 38 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16] -> size -> 48 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 20. 30.  8.  0.  4.  6.  7.  2.  0.  8. 10.  8. 10.  3.] 
adversary cards in hand: [15.  1. 25. 29. 29.] 
adversary cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3. 25.  3.
 25. 25. 15. 29.  0.  3.  0.  1. 25.  3. 29. 29. 15.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3] -> size -> 38 
adversary victory points: 8
player victory points: -4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
Chosen buy action: 23.0 : ['Market' '23' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 1. 0. 0.] 
cards in discard: [16. 16.  3.  6. 14.] 
cards in deck: 38 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16] -> size -> 48 
action values: 0 
buys: 1 
player value: 5 
card supply: [21. 26. 30. 20. 30.  8.  0.  4.  6.  7.  2.  0.  8. 10.  8. 10.  3.] 
adversary cards in hand: [15.  1. 25. 29. 29.] 
adversary cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3. 25.  3.
 25. 25. 15. 29.  0.  3.  0.  1. 25.  3. 29. 29. 15.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3] -> size -> 38 
adversary victory points: 8
player victory points: -4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 1. 0. 0.] 
cards in discard: [16. 16.  3.  6. 14. 23.] 
cards in deck: 38 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23] -> size -> 49 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 20. 30.  8.  0.  4.  6.  7.  2.  0.  8.  9.  8. 10.  3.] 
adversary cards in hand: [15.  1. 25. 29. 29.] 
adversary cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3. 25.  3.
 25. 25. 15. 29.  0.  3.  0.  1. 25.  3. 29. 29. 15.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3] -> size -> 38 
adversary victory points: 8
player victory points: -4 





Player: 0 
cards in hand: [15.  1. 25. 29. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 25. 29. 29.] 
expected returns: [[-5.5201325]
 [ 1.4973209]
 [ 3.4095786]
 [ 5.308524 ]
 [ 5.308524 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  1. 25. 29. 29.] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3. 25.  3.
 25. 25. 15. 29.  0.  3.  0.  1. 25.  3. 29. 29. 15.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3] -> size -> 38 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 20. 30.  8.  0.  4.  6.  7.  2.  0.  8.  9.  8. 10.  3.] 
adversary cards in hand: [ 3. 11.  0. 11.  8.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23] -> size -> 49 
adversary victory points: -4
player victory points: 8 

Reward from previous game state: 
[ -5   0   0 360   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 355 

action type: buy - action -1
Learning step: 0
desired expected reward: -2.221709728240967



action possibilites: [-1. 29. 25.] 
expected returns: [[-15.166266 ]
 [ -4.5927644]
 [ -6.5507927]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 29. 25.] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3. 25.  3.
 25. 25. 15. 29.  0.  3.  0.  1. 25.  3. 29. 29. 15. 15. 25.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3] -> size -> 38 
action values: 1 
buys: 0 
player value: 1 
card supply: [21. 26. 30. 20. 30.  8.  0.  4.  6.  7.  2.  0.  8.  9.  8. 10.  3.] 
adversary cards in hand: [ 3. 11.  0. 11.  8.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23] -> size -> 49 
adversary victory points: -4
player victory points: 8 

Reward from previous game state: 
[ -5   0   0 360   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 375 

action type: discard_n_cards - action 5
Learning step: 0
desired expected reward: 2.5566065311431885



action possibilites: [-1.] 
expected returns: [[-7.2958727]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1.] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3. 25.  3.
 25. 25. 15. 29.  0.  3.  0.  1. 25.  3. 29. 29. 15. 15. 25. 25. 29.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3] -> size -> 38 
action values: 1 
buys: 0 
player value: 2 
card supply: [21. 26. 30. 20. 30.  8.  0.  4.  6.  7.  2.  0.  8.  9.  8. 10.  3.] 
adversary cards in hand: [ 3. 11.  0. 11.  8.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23] -> size -> 49 
adversary victory points: -4
player victory points: 8 

Reward from previous game state: 
[ -5   0   0 360   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 395 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -9.383584022521973





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[-8.823564  ]
 [-1.2837193 ]
 [-5.842845  ]
 [-4.7270083 ]
 [-2.2201443 ]
 [-7.3677645 ]
 [-6.925423  ]
 [-5.6922817 ]
 [-0.47063875]
 [-7.295867  ]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1.] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3. 25.  3.
 25. 25. 15. 29.  0.  3.  0.  1. 25.  3. 29. 29. 15. 15. 25. 25. 29.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3] -> size -> 38 
action values: 0 
buys: 1 
player value: 4 
card supply: [21. 26. 30. 20. 30.  8.  0.  4.  6.  7.  2.  0.  8.  9.  8. 10.  3.] 
adversary cards in hand: [ 3. 11.  0. 11.  8.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23] -> size -> 49 
adversary victory points: -4
player victory points: 8 

Reward from previous game state: 
[ -5   0   0 360   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 395 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -7.295872688293457



buy possibilites: [-1] 
expected returns: [[-11.1338005]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1.] 
cards in discard: [15. 25. 15. 29. 15.  3. 15.  3.  1.  3.  3. 29. 29. 25. 29.  3. 25.  3.
 25. 25. 15. 29.  0.  3.  0.  1. 25.  3. 29. 29. 15. 15. 25. 25. 29. 15.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15] -> size -> 39 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 20. 30.  8.  0.  4.  6.  7.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [ 3. 11.  0. 11.  8.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23] -> size -> 49 
adversary victory points: -4
player victory points: 8 

Reward from previous game state: 
[ -5   0   0 360   0   0  40   0   0   0   0 -40   0   0 128   0] 
sum of rewards: 483 

action type: buy - action 15.0
Learning step: 0
desired expected reward: -0.4706454277038574






         -------------------- Turn: 37 -------------------- 
Player: 1 
cards in hand: [ 3. 11.  0. 11.  8.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11. 11.  8.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 11.  0. 11.  8.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.] 
cards in deck: 33 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23] -> size -> 49 
action values: 1 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 20. 30.  8.  0.  4.  6.  7.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [15. 29. 15. 15. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15] -> size -> 39 
adversary victory points: 8
player victory points: -4 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 11.  8.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.] 
cards in deck: 33 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8] -> size -> 50 
action values: 0 
buys: 0 
player value: 0 
card supply: [21. 26. 30. 20. 30.  8.  0.  4.  6.  6.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [15. 29. 15. 15. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15] -> size -> 39 
adversary victory points: 8
player victory points: -4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0. 11.  8.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.] 
cards in deck: 33 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8] -> size -> 50 
action values: 0 
buys: 1 
player value: 1 
card supply: [21. 26. 30. 20. 30.  8.  0.  4.  6.  6.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [15. 29. 15. 15. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15] -> size -> 39 
adversary victory points: 8
player victory points: -4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0. 11.  8.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0.] 
cards in deck: 33 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0] -> size -> 51 
action values: 0 
buys: 0 
player value: 1 
card supply: [20. 26. 30. 20. 30.  8.  0.  4.  6.  6.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [15. 29. 15. 15. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15] -> size -> 39 
adversary victory points: 8
player victory points: -4 





Player: 0 
cards in hand: [15. 29. 15. 15. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 29. 15. 15. 29.] 
expected returns: [[-15.749354 ]
 [ -9.001025 ]
 [ -5.3396773]
 [ -9.001025 ]
 [ -9.001025 ]
 [ -5.3396773]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 29. 15. 15. 29.] 
cards in discard: [] 
cards in deck: 34 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15] -> size -> 39 
action values: 1 
buys: 0 
player value: 0 
card supply: [20. 26. 30. 20. 30.  8.  0.  4.  6.  6.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [ 8. 10.  6.  3. 11.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0] -> size -> 51 
adversary victory points: -4
player victory points: 8 

Reward from previous game state: 
[ -5   0   0 360   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 355 

action type: buy - action -1
Learning step: 0
desired expected reward: -11.133800506591797



action possibilites: [-1. 15. 15. 15.] 
expected returns: [[-0.92372775]
 [ 5.9616947 ]
 [ 5.9616947 ]
 [ 5.9616947 ]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 15. 15.] 
cards in discard: [29. 29.] 
cards in deck: 33 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15] -> size -> 39 
action values: 1 
buys: 0 
player value: 1 
card supply: [20. 26. 30. 20. 30.  8.  0.  4.  6.  6.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [ 8. 10.  6.  3. 11.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0] -> size -> 51 
adversary victory points: -4
player victory points: 8 

Reward from previous game state: 
[ -5   0   0 360   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 375 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -9.78450870513916



action possibilites: [-1] 
expected returns: [[-7.6060762]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 15.] 
cards in discard: [29. 29.] 
cards in deck: 33 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15] -> size -> 39 
action values: 0 
buys: 0 
player value: 1 
card supply: [20. 26. 30. 20. 30.  8.  0.  4.  6.  6.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [ 8. 10.  6.  3. 11.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0] -> size -> 51 
adversary victory points: -4
player victory points: 8 

Reward from previous game state: 
[ -5   0   0 360   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 395 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 5.961699485778809





----------- BUY PHASE ----------- 
buy possibilites: [ 0. -1.] 
expected returns: [[-9.250295 ]
 [-7.5522604]]
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15. 15.] 
cards in discard: [29. 29.] 
cards in deck: 33 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15] -> size -> 39 
action values: 0 
buys: 1 
player value: 1 
card supply: [20. 26. 30. 20. 30.  8.  0.  4.  6.  6.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [ 8. 10.  6.  3. 11.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0] -> size -> 51 
adversary victory points: -4
player victory points: 8 

Reward from previous game state: 
[ -5   0   0 360   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 395 

action type: take_action - action -1
Learning step: 0
desired expected reward: -7.606076240539551






         -------------------- Turn: 38 -------------------- 
Player: 1 
cards in hand: [ 8. 10.  6.  3. 11.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8. 10. 11.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 8. 10.  6.  3. 11.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.] 
cards in deck: 28 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0] -> size -> 51 
action values: 1 
buys: 0 
player value: 0 
card supply: [20. 26. 30. 20. 30.  8.  0.  4.  6.  6.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [29. 15. 25.  3. 25.] 
adversary cards in discard: [29. 29. 29. 15. 15. 15.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15] -> size -> 39 
adversary victory points: 8
player victory points: -4 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 8. 10.  6.  3.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.] 
cards in deck: 28 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0  1] -> size -> 52 
action values: 0 
buys: 0 
player value: 0 
card supply: [20. 25. 30. 20. 30.  8.  0.  4.  6.  6.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [29. 15. 25.  3. 25.] 
adversary cards in discard: [29. 29. 29. 15. 15. 15.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15] -> size -> 39 
adversary victory points: 8
player victory points: -4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 8. 10.  6.  3.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.] 
cards in deck: 28 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0  1] -> size -> 52 
action values: 0 
buys: 1 
player value: 0 
card supply: [20. 25. 30. 20. 30.  8.  0.  4.  6.  6.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [29. 15. 25.  3. 25.] 
adversary cards in discard: [29. 29. 29. 15. 15. 15.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15] -> size -> 39 
adversary victory points: 8
player victory points: -4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 8. 10.  6.  3.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0.] 
cards in deck: 28 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0  1  0] -> size -> 53 
action values: 0 
buys: 0 
player value: 0 
card supply: [19. 25. 30. 20. 30.  8.  0.  4.  6.  6.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [29. 15. 25.  3. 25.] 
adversary cards in discard: [29. 29. 29. 15. 15. 15.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15] -> size -> 39 
adversary victory points: 8
player victory points: -4 





Player: 0 
cards in hand: [29. 15. 25.  3. 25.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 15. 25. 25.] 
expected returns: [[ 5.2020693]
 [16.201443 ]
 [12.338476 ]
 [14.273643 ]
 [14.273643 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 15. 25.  3. 25.] 
cards in discard: [29. 29. 29. 15. 15. 15.] 
cards in deck: 28 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15] -> size -> 39 
action values: 1 
buys: 0 
player value: 0 
card supply: [19. 25. 30. 20. 30.  8.  0.  4.  6.  6.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [ 6.  0.  0. 11. 14.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0  1  0] -> size -> 53 
adversary victory points: -4
player victory points: 8 

Reward from previous game state: 
[ -5   0   0 360   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 355 

action type: buy - action -1.0
Learning step: 0
desired expected reward: -7.552255630493164



action possibilites: [-1. 15. 25.] 
expected returns: [[31.762424]
 [39.06979 ]
 [41.06812 ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 25.  0.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.] 
cards in deck: 27 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15] -> size -> 39 
action values: 1 
buys: 0 
player value: 1 
card supply: [19. 25. 30. 20. 30.  8.  0.  4.  6.  6.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [ 6.  0.  0. 11. 14.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0  1  0] -> size -> 53 
adversary victory points: -4
player victory points: 8 

Reward from previous game state: 
[ -5   0   0 360   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 375 

action type: discard_n_cards - action 5
Learning step: 0
desired expected reward: 13.428351402282715



action possibilites: [-1] 
expected returns: [[2.7558968]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0. 15. 15.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.] 
cards in deck: 25 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15] -> size -> 39 
action values: 0 
buys: 0 
player value: 1 
card supply: [19. 25. 30. 20. 30.  8.  0.  4.  6.  6.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [ 6.  0.  0. 11. 14.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0  1  0] -> size -> 53 
adversary victory points: -4
player victory points: 8 

Reward from previous game state: 
[ -5   0   0 360   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 395 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 41.068111419677734





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  8. -1.] 
expected returns: [[0.72586465]
 [3.7900674 ]
 [2.5091631 ]
 [2.3166673 ]]
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  0. 15. 15.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.] 
cards in deck: 25 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15] -> size -> 39 
action values: 0 
buys: 1 
player value: 2 
card supply: [19. 25. 30. 20. 30.  8.  0.  4.  6.  6.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [ 6.  0.  0. 11. 14.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0  1  0] -> size -> 53 
adversary victory points: -4
player victory points: 8 

Reward from previous game state: 
[ -5   0   0 360   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 395 

action type: take_action - action -1
Learning step: 0
desired expected reward: 2.755896806716919



buy possibilites: [-1] 
expected returns: [[-16.516827]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  0. 15. 15.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3.] 
cards in deck: 25 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3] -> size -> 40 
action values: 0 
buys: 0 
player value: 0 
card supply: [19. 25. 30. 19. 30.  8.  0.  4.  6.  6.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [ 6.  0.  0. 11. 14.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0  1  0] -> size -> 53 
adversary victory points: -4
player victory points: 9 

Reward from previous game state: 
[ -5   0   0 390   0   0  40   0   0   0   0 -50   0   0  16   0] 
sum of rewards: 391 

action type: buy - action 3.0
Learning step: 0
desired expected reward: 3.7900660037994385






         -------------------- Turn: 39 -------------------- 
Player: 1 
cards in hand: [ 6.  0.  0. 11. 14.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11. 14.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 6.  0.  0. 11. 14.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3.] 
cards in deck: 23 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0  1  0] -> size -> 53 
action values: 1 
buys: 0 
player value: 0 
card supply: [19. 25. 30. 19. 30.  8.  0.  4.  6.  6.  2.  0.  8.  9.  8. 10.  2.] 
adversary cards in hand: [ 3. 29. 25. 25.  3.] 
adversary cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3] -> size -> 40 
adversary victory points: 9
player victory points: -4 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 6.  0.  0. 14.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14.] 
cards in deck: 23 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0  1  0 14] -> size -> 54 
action values: 0 
buys: 0 
player value: 0 
card supply: [19. 25. 30. 19. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  2.] 
adversary cards in hand: [ 3. 29. 25. 25.  3.] 
adversary cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3] -> size -> 40 
adversary victory points: 9
player victory points: -4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  8. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 6.  0.  0. 14.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14.] 
cards in deck: 23 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0  1  0 14] -> size -> 54 
action values: 0 
buys: 1 
player value: 2 
card supply: [19. 25. 30. 19. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  2.] 
adversary cards in hand: [ 3. 29. 25. 25.  3.] 
adversary cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3] -> size -> 40 
adversary victory points: 9
player victory points: -4 





Player: 0 
cards in hand: [ 3. 29. 25. 25.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 25. 25.] 
expected returns: [[35.603214]
 [46.108986]
 [44.358543]
 [44.358543]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29. 25. 25.  3.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15.] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3] -> size -> 40 
action values: 1 
buys: 0 
player value: 0 
card supply: [19. 25. 30. 19. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  2.] 
adversary cards in hand: [ 6.  3.  6. 16.  3.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0  1  0 14] -> size -> 54 
adversary victory points: -4
player victory points: 9 

Reward from previous game state: 
[ -5   0   0 390   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 385 

action type: buy - action -1
Learning step: 0
desired expected reward: -16.516826629638672



action possibilites: [-1. 25.] 
expected returns: [[25.990356]
 [34.85023 ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  3. 25.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3] -> size -> 40 
action values: 1 
buys: 0 
player value: 1 
card supply: [19. 25. 30. 19. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  2.] 
adversary cards in hand: [ 6.  3.  6. 16.  3.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0  1  0 14] -> size -> 54 
adversary victory points: -4
player victory points: 9 

Reward from previous game state: 
[ -5   0   0 390   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 405 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 41.63938522338867



action possibilites: [-1] 
expected returns: [[-16.832472]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 3. 1. 3.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.] 
cards in deck: 17 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3] -> size -> 40 
action values: 0 
buys: 0 
player value: 1 
card supply: [19. 25. 30. 19. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  2.] 
adversary cards in hand: [ 6.  3.  6. 16.  3.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0  1  0 14] -> size -> 54 
adversary victory points: -4
player victory points: 9 

Reward from previous game state: 
[ -5   0   0 390   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 425 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 34.85023498535156





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3. 11.  8. 10. -1.] 
expected returns: [[-18.360868 ]
 [-10.544469 ]
 [-15.294404 ]
 [-11.590754 ]
 [-16.96236  ]
 [-15.1638365]
 [-16.77121  ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 1. 3.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.] 
cards in deck: 17 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3] -> size -> 40 
action values: 0 
buys: 1 
player value: 3 
card supply: [19. 25. 30. 19. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  2.] 
adversary cards in hand: [ 6.  3.  6. 16.  3.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0  1  0 14] -> size -> 54 
adversary victory points: -4
player victory points: 9 

Reward from previous game state: 
[ -5   0   0 390   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 425 

action type: take_action - action -1
Learning step: 0
desired expected reward: -16.83247184753418



buy possibilites: [-1] 
expected returns: [[-22.616705]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 1. 3.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.] 
cards in deck: 17 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1] -> size -> 41 
action values: 0 
buys: 0 
player value: 0 
card supply: [19. 24. 30. 19. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  2.] 
adversary cards in hand: [ 6.  3.  6. 16.  3.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0  1  0 14] -> size -> 54 
adversary victory points: -4
player victory points: 9 

Reward from previous game state: 
[ -5   0   0 390   0   0  40   0   0   0   0 -60   0   0  54   0] 
sum of rewards: 419 

action type: buy - action 1.0
Learning step: 0
desired expected reward: -10.544458389282227






         -------------------- Turn: 40 -------------------- 
Player: 1 
cards in hand: [ 6.  3.  6. 16.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16.] 
Chosen action: 16 : ['Remodel' '16' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 6.  3.  6. 16.  3.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.] 
cards in deck: 18 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  0 16 11  6  3 11  0  1  6  0 11 16  0  3 11 16  6  0
 10  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16
 23  8  0  1  0 14] -> size -> 54 
action values: 1 
buys: 0 
player value: 0 
card supply: [19. 24. 30. 19. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  2.] 
adversary cards in hand: [ 3. 29. 15. 25. 29.] 
adversary cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.
 29. 25.  3.  3.  1.  3.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1] -> size -> 41 
adversary victory points: 9
player victory points: -4 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 6. 3.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.] 
cards in deck: 18 
card top of deck: [] 
played cards: [16.] 
owned cards: [ 0  0  0  0  0  3  0 16 11  3 11  0  1  6  0 11 16  0  3 11 16  6  0 10
  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23
  8  0  1  0 14  3] -> size -> 54 
action values: 0 
buys: 0 
player value: 0 
card supply: [19. 24. 30. 18. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  2.] 
adversary cards in hand: [ 3. 29. 15. 25. 29.] 
adversary cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.
 29. 25.  3.  3.  1.  3.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1] -> size -> 41 
adversary victory points: 9
player victory points: -2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 6. 3.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.] 
cards in deck: 18 
card top of deck: [] 
played cards: [16.] 
owned cards: [ 0  0  0  0  0  3  0 16 11  3 11  0  1  6  0 11 16  0  3 11 16  6  0 10
  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23
  8  0  1  0 14  3] -> size -> 54 
action values: 0 
buys: 1 
player value: 0 
card supply: [19. 24. 30. 18. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  2.] 
adversary cards in hand: [ 3. 29. 15. 25. 29.] 
adversary cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.
 29. 25.  3.  3.  1.  3.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1] -> size -> 41 
adversary victory points: 9
player victory points: -2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 6. 3.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0.] 
cards in deck: 18 
card top of deck: [] 
played cards: [16.] 
owned cards: [ 0  0  0  0  0  3  0 16 11  3 11  0  1  6  0 11 16  0  3 11 16  6  0 10
  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23
  8  0  1  0 14  3  0] -> size -> 55 
action values: 0 
buys: 0 
player value: 0 
card supply: [18. 24. 30. 18. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  2.] 
adversary cards in hand: [ 3. 29. 15. 25. 29.] 
adversary cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.
 29. 25.  3.  3.  1.  3.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1] -> size -> 41 
adversary victory points: 9
player victory points: -2 





Player: 0 
cards in hand: [ 3. 29. 15. 25. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 15. 25. 29.] 
expected returns: [[ 1.40995 ]
 [11.580045]
 [ 7.99269 ]
 [ 9.779371]
 [11.580045]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29. 15. 25. 29.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.
 29. 25.  3.  3.  1.  3.] 
cards in deck: 12 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1] -> size -> 41 
action values: 1 
buys: 0 
player value: 0 
card supply: [18. 24. 30. 18. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  2.] 
adversary cards in hand: [10.  0. 16.  6.  8.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  3 11  0  1  6  0 11 16  0  3 11 16  6  0 10
  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23
  8  0  1  0 14  3  0] -> size -> 55 
adversary victory points: -2
player victory points: 9 

Reward from previous game state: 
[ -5   0   0 330   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 325 

action type: buy - action -1
Learning step: 0
desired expected reward: -22.6167049407959



action possibilites: [-1. 15. 29.] 
expected returns: [[-0.6463821]
 [ 5.93542  ]
 [ 9.52204  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 29.  1.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.
 29. 25.  3.  3.  1.  3.  3. 25.] 
cards in deck: 11 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1] -> size -> 41 
action values: 1 
buys: 0 
player value: 1 
card supply: [18. 24. 30. 18. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  2.] 
adversary cards in hand: [10.  0. 16.  6.  8.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  3 11  0  1  6  0 11 16  0  3 11 16  6  0 10
  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23
  8  0  1  0 14  3  0] -> size -> 55 
adversary victory points: -2
player victory points: 9 

Reward from previous game state: 
[ -5   0   0 330   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 345 

action type: discard_n_cards - action 5
Learning step: 0
desired expected reward: 9.022010803222656



action possibilites: [-1.] 
expected returns: [[-17.457949]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.
 29. 25.  3.  3.  1.  3.  3. 25. 15.  1.] 
cards in deck: 10 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1] -> size -> 41 
action values: 1 
buys: 0 
player value: 2 
card supply: [18. 24. 30. 18. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  2.] 
adversary cards in hand: [10.  0. 16.  6.  8.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  3 11  0  1  6  0 11 16  0  3 11 16  6  0 10
  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23
  8  0  1  0 14  3  0] -> size -> 55 
adversary victory points: -2
player victory points: 9 

Reward from previous game state: 
[ -5   0   0 330   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 365 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 5.171731948852539





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[-19.121784]
 [-11.968086]
 [-16.38457 ]
 [-15.200604]
 [-13.001084]
 [-18.014318]
 [-17.315092]
 [-16.178972]
 [-11.268934]
 [-17.49536 ]]
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.
 29. 25.  3.  3.  1.  3.  3. 25. 15.  1.] 
cards in deck: 10 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1] -> size -> 41 
action values: 0 
buys: 1 
player value: 4 
card supply: [18. 24. 30. 18. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  2.] 
adversary cards in hand: [10.  0. 16.  6.  8.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  3 11  0  1  6  0 11 16  0  3 11 16  6  0 10
  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23
  8  0  1  0 14  3  0] -> size -> 55 
adversary victory points: -2
player victory points: 9 

Reward from previous game state: 
[ -5   0   0 330   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 365 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -17.457948684692383



buy possibilites: [-1] 
expected returns: [[-16.043104]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.
 29. 25.  3.  3.  1.  3.  3. 25. 15.  1. 15.] 
cards in deck: 10 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15] -> size -> 42 
action values: 0 
buys: 0 
player value: 0 
card supply: [18. 24. 30. 18. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [10.  0. 16.  6.  8.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.] 
adversary owned cards: [ 0  0  0  0  0  3  0 16 11  3 11  0  1  6  0 11 16  0  3 11 16  6  0 10
  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23
  8  0  1  0 14  3  0] -> size -> 55 
adversary victory points: -2
player victory points: 9 

Reward from previous game state: 
[ -5   0   0 330   0   0  40   0   0   0   0 -70   0   0 128   0] 
sum of rewards: 423 

action type: buy - action 15.0
Learning step: 0
desired expected reward: -11.268927574157715






         -------------------- Turn: 41 -------------------- 
Player: 1 
cards in hand: [10.  0. 16.  6.  8.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 10. 16.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [10.  0. 16.  6.  8.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  3  0 16 11  3 11  0  1  6  0 11 16  0  3 11 16  6  0 10
  0  3  8 14  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23
  8  0  1  0 14  3  0] -> size -> 55 
action values: 1 
buys: 0 
player value: 0 
card supply: [18. 24. 30. 18. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3.  3.  0. 15. 29.] 
adversary cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.
 29. 25.  3.  3.  1.  3.  3. 25. 15.  1. 15. 29. 29.  1.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15] -> size -> 42 
adversary victory points: 9
player victory points: -2 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.] 
cards in deck: 13 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  6  0  0  3  8 14
  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0
 14  3  0] -> size -> 51 
action values: 0 
buys: 0 
player value: 0 
card supply: [18. 24. 30. 18. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3.  3.  0. 15. 29.] 
adversary cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.
 29. 25.  3.  3.  1.  3.  3. 25. 15.  1. 15. 29. 29.  1.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15] -> size -> 42 
adversary victory points: 9
player victory points: -1 




----------- BUY PHASE ----------- 
buy possibilites: [ 0. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.] 
cards in deck: 13 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  6  0  0  3  8 14
  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0
 14  3  0] -> size -> 51 
action values: 0 
buys: 1 
player value: 0 
card supply: [18. 24. 30. 18. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3.  3.  0. 15. 29.] 
adversary cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.
 29. 25.  3.  3.  1.  3.  3. 25. 15.  1. 15. 29. 29.  1.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15] -> size -> 42 
adversary victory points: 9
player victory points: -1 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.  0.] 
cards in deck: 13 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  6  0  0  3  8 14
  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0
 14  3  0  0] -> size -> 52 
action values: 0 
buys: 0 
player value: 0 
card supply: [17. 24. 30. 18. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3.  3.  0. 15. 29.] 
adversary cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.
 29. 25.  3.  3.  1.  3.  3. 25. 15.  1. 15. 29. 29.  1.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15] -> size -> 42 
adversary victory points: 9
player victory points: -1 





Player: 0 
cards in hand: [ 3.  3.  0. 15. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 29.] 
expected returns: [[-10.751991 ]
 [ -3.3212752]
 [  0.9634478]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  3.  0. 15. 29.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.
 29. 25.  3.  3.  1.  3.  3. 25. 15.  1. 15. 29. 29.  1.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15] -> size -> 42 
action values: 1 
buys: 0 
player value: 0 
card supply: [17. 24. 30. 18. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [6. 0. 6. 0. 0.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.  0.  8.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  6  0  0  3  8 14
  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0
 14  3  0  0] -> size -> 52 
adversary victory points: -1
player victory points: 9 

Reward from previous game state: 
[ -5   0   0 300   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 295 

action type: buy - action -1
Learning step: 0
desired expected reward: -16.04310417175293



action possibilites: [-1.] 
expected returns: [[-12.491096]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 3. 0.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.
 29. 25.  3.  3.  1.  3.  3. 25. 15.  1. 15. 29. 29.  1. 15. 29.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15] -> size -> 42 
action values: 1 
buys: 0 
player value: 1 
card supply: [17. 24. 30. 18. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [6. 0. 6. 0. 0.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.  0.  8.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  6  0  0  3  8 14
  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0
 14  3  0  0] -> size -> 52 
adversary victory points: -1
player victory points: 9 

Reward from previous game state: 
[ -5   0   0 300   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 315 

action type: discard_n_cards - action 5
Learning step: 0
desired expected reward: -2.1528639793395996





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  8. -1.] 
expected returns: [[-13.936222 ]
 [-10.9234295]
 [-12.579992 ]
 [-12.491096 ]]
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 0.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.
 29. 25.  3.  3.  1.  3.  3. 25. 15.  1. 15. 29. 29.  1. 15. 29.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15] -> size -> 42 
action values: 0 
buys: 1 
player value: 2 
card supply: [17. 24. 30. 18. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [6. 0. 6. 0. 0.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.  0.  8.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  6  0  0  3  8 14
  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0
 14  3  0  0] -> size -> 52 
adversary victory points: -1
player victory points: 9 

Reward from previous game state: 
[ -5   0   0 300   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 315 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -12.491095542907715



buy possibilites: [-1] 
expected returns: [[-3.5035844]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 0.] 
cards in discard: [29. 29. 29. 15. 15. 15.  3. 25.  3. 29. 25. 15.  0. 15. 15. 25. 25.  1.
 29. 25.  3.  3.  1.  3.  3. 25. 15.  1. 15. 29. 29.  1. 15. 29.  3.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3] -> size -> 43 
action values: 0 
buys: 0 
player value: 0 
card supply: [17. 24. 30. 17. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [6. 0. 6. 0. 0.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.  0.  8.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  6  0  0  3  8 14
  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0
 14  3  0  0] -> size -> 52 
adversary victory points: -1
player victory points: 10 

Reward from previous game state: 
[ -5   0   0 330   0   0  20   0   0   0   0 -80   0   0  16   0] 
sum of rewards: 281 

action type: buy - action 3.0
Learning step: 0
desired expected reward: -10.923428535461426






         -------------------- Turn: 42 -------------------- 
Player: 1 
cards in hand: [6. 0. 6. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [6. 0. 6. 0. 0.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.  0.  8.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  6  0  0  3  8 14
  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0
 14  3  0  0] -> size -> 52 
action values: 1 
buys: 0 
player value: 0 
card supply: [17. 24. 30. 17. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [29.  3. 25. 25. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3] -> size -> 43 
adversary victory points: 10
player victory points: -1 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3. 11.  8. 10. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [6. 0. 6. 0. 0.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.  0.  8.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  6  0  0  3  8 14
  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0
 14  3  0  0] -> size -> 52 
action values: 0 
buys: 1 
player value: 3 
card supply: [17. 24. 30. 17. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [29.  3. 25. 25. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3] -> size -> 43 
adversary victory points: 10
player victory points: -1 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [6. 0. 6. 0. 0.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.  0.  8.  3.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  6  0  0  3  8 14
  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0
 14  3  0  0  3] -> size -> 53 
action values: 0 
buys: 0 
player value: 1 
card supply: [17. 24. 30. 16. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [29.  3. 25. 25. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3] -> size -> 43 
adversary victory points: 10
player victory points: 0 





Player: 0 
cards in hand: [29.  3. 25. 25. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 25. 25. 29.] 
expected returns: [[-9.0270605]
 [ 1.1686499]
 [-0.5540254]
 [-0.5540254]
 [ 1.1686499]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  3. 25. 25. 29.] 
cards in discard: [] 
cards in deck: 38 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3] -> size -> 43 
action values: 1 
buys: 0 
player value: 0 
card supply: [17. 24. 30. 16. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 0. 16.  0.  0.  6.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.  0.  8.  3.  6.  0.  6.  0.  0.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  6  0  0  3  8 14
  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0
 14  3  0  0  3] -> size -> 53 
adversary victory points: 0
player victory points: 10 

Reward from previous game state: 
[ -5   0   0 300   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 295 

action type: buy - action -1
Learning step: 0
desired expected reward: -3.503584384918213



action possibilites: [-1. 25. 29.] 
expected returns: [[-4.974582 ]
 [ 3.3661735]
 [ 5.1587887]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 29.  1.] 
cards in discard: [ 3. 25.] 
cards in deck: 37 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3] -> size -> 43 
action values: 1 
buys: 0 
player value: 1 
card supply: [17. 24. 30. 16. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 0. 16.  0.  0.  6.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.  0.  8.  3.  6.  0.  6.  0.  0.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  6  0  0  3  8 14
  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0
 14  3  0  0  3] -> size -> 53 
adversary victory points: 0
player victory points: 10 

Reward from previous game state: 
[ -5   0   0 300   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 315 

action type: discard_n_cards - action 5
Learning step: 0
desired expected reward: -1.4502063989639282



action possibilites: [-1. 25.] 
expected returns: [[-13.646735]
 [ -5.351774]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.] 
cards in discard: [ 3. 25.  1.  3.] 
cards in deck: 36 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3] -> size -> 43 
action values: 1 
buys: 0 
player value: 2 
card supply: [17. 24. 30. 16. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 0. 16.  0.  0.  6.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.  0.  8.  3.  6.  0.  6.  0.  0.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  6  0  0  3  8 14
  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0
 14  3  0  0  3] -> size -> 53 
adversary victory points: 0
player victory points: 10 

Reward from previous game state: 
[ -5   0   0 300   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 335 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 0.8291380405426025



action possibilites: [-1] 
expected returns: [[-2.751824]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 25.] 
cards in discard: [ 3. 25.  1.  3.] 
cards in deck: 34 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3] -> size -> 43 
action values: 0 
buys: 0 
player value: 2 
card supply: [17. 24. 30. 16. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 0. 16.  0.  0.  6.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.  0.  8.  3.  6.  0.  6.  0.  0.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  6  0  0  3  8 14
  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0
 14  3  0  0  3] -> size -> 53 
adversary victory points: 0
player victory points: 10 

Reward from previous game state: 
[ -5   0   0 300   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 355 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: -5.351771354675293





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  8. -1.] 
expected returns: [[-4.969429 ]
 [-2.2646823]
 [-3.45975  ]
 [-3.1984773]]
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [29. 25.] 
cards in discard: [ 3. 25.  1.  3.] 
cards in deck: 34 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3] -> size -> 43 
action values: 0 
buys: 1 
player value: 2 
card supply: [17. 24. 30. 16. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 0. 16.  0.  0.  6.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.  0.  8.  3.  6.  0.  6.  0.  0.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  6  0  0  3  8 14
  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0
 14  3  0  0  3] -> size -> 53 
adversary victory points: 0
player victory points: 10 

Reward from previous game state: 
[ -5   0   0 300   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 355 

action type: take_action - action -1
Learning step: 0
desired expected reward: -2.751823902130127



buy possibilites: [-1] 
expected returns: [[-12.606477]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [29. 25.] 
cards in discard: [ 3. 25.  1.  3.  3.] 
cards in deck: 34 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3] -> size -> 44 
action values: 0 
buys: 0 
player value: 0 
card supply: [17. 24. 30. 15. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 0. 16.  0.  0.  6.] 
adversary cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.  0.  8.  3.  6.  0.  6.  0.  0.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  6  0  0  3  8 14
  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0
 14  3  0  0  3] -> size -> 53 
adversary victory points: 0
player victory points: 11 

Reward from previous game state: 
[ -5   0   0 330   0   0  60   0   0   0   0 -90   0   0  16   0] 
sum of rewards: 311 

action type: buy - action 3.0
Learning step: 0
desired expected reward: -2.2646799087524414






         -------------------- Turn: 43 -------------------- 
Player: 1 
cards in hand: [ 0. 16.  0.  0.  6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16.] 
Chosen action: 16 : ['Remodel' '16' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 16.  0.  0.  6.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.  0.  8.  3.  6.  0.  6.  0.  0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  6  0  0  3  8 14
  6  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0
 14  3  0  0  3] -> size -> 53 
action values: 1 
buys: 0 
player value: 0 
card supply: [17. 24. 30. 15. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3. 15.  0.  1. 29.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3] -> size -> 44 
adversary victory points: 11
player victory points: 0 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.  0.  8.  3.  6.  0.  6.  0.  0.  3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [16.] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3] -> size -> 53 
action values: 0 
buys: 0 
player value: 0 
card supply: [17. 24. 30. 14. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3. 15.  0.  1. 29.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3] -> size -> 44 
adversary victory points: 11
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3. 11.  8. 10. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0.] 
cards in discard: [16. 16.  3.  6. 14. 23.  3.  0.  1.  0.  0.  8.  0. 11.  3.  0. 11.  8.
  1.  0. 11.  8. 10.  6.  3. 14. 11.  6.  0.  0. 14.  3.  0. 16.  3.  6.
  3.  0.  8.  3.  6.  0.  6.  0.  0.  3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [16.] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3] -> size -> 53 
action values: 0 
buys: 1 
player value: 3 
card supply: [17. 24. 30. 14. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3. 15.  0.  1. 29.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.] 
adversary owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3] -> size -> 44 
adversary victory points: 11
player victory points: 2 





Player: 0 
cards in hand: [ 3. 15.  0.  1. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 29.] 
expected returns: [[ 2.4195487]
 [ 9.203334 ]
 [12.847882 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 15.  0.  1. 29.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.] 
cards in deck: 29 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3] -> size -> 44 
action values: 1 
buys: 0 
player value: 0 
card supply: [17. 24. 30. 14. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 0.  6.  0. 16.  6.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3] -> size -> 53 
adversary victory points: 2
player victory points: 11 

Reward from previous game state: 
[ -5   0   0 270   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 265 

action type: buy - action -1
Learning step: 0
desired expected reward: -12.606476783752441



action possibilites: [-1. 15.] 
expected returns: [[30.596577]
 [38.101307]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0.  1.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29.] 
cards in deck: 28 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25
 25 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3] -> size -> 44 
action values: 1 
buys: 0 
player value: 1 
card supply: [17. 24. 30. 14. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 0.  6.  0. 16.  6.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3] -> size -> 53 
adversary victory points: 2
player victory points: 11 

Reward from previous game state: 
[ -5   0   0 270   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 285 

action type: discard_n_cards - action 5
Learning step: 0
desired expected reward: 10.219273567199707



action possibilites: [-1] 
expected returns: [[-0.40044856]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29.] 
cards in deck: 28 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3] -> size -> 43 
action values: 0 
buys: 0 
player value: 4 
card supply: [17. 24. 30. 14. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 0.  6.  0. 16.  6.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3] -> size -> 53 
adversary victory points: 2
player victory points: 11 

Reward from previous game state: 
[ -5   0   0 270   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 305 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 38.101314544677734





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ -3.6095564 ]
 [  4.0064764 ]
 [-19.724653  ]
 [ -0.79334664]
 [-14.480006  ]
 [  0.6256654 ]
 [  2.999121  ]
 [ -1.9314959 ]
 [  6.5538874 ]
 [ -1.7567357 ]
 [  4.823386  ]
 [ -0.31142735]
 [-12.554544  ]
 [  4.7682743 ]
 [ -1.8271841 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29.] 
cards in deck: 28 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3] -> size -> 43 
action values: 0 
buys: 1 
player value: 6 
card supply: [17. 24. 30. 14. 30.  8.  0.  4.  6.  6.  2.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 0.  6.  0. 16.  6.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3] -> size -> 53 
adversary victory points: 2
player victory points: 11 

Reward from previous game state: 
[ -5   0   0 270   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 305 

action type: take_action - action -1
Learning step: 0
desired expected reward: -0.4004485607147217



buy possibilites: [-1] 
expected returns: [[-17.935848]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25.] 
cards in deck: 28 
card top of deck: [] 
played cards: [29. 15.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25] -> size -> 44 
action values: 0 
buys: 0 
player value: 1 
card supply: [17. 24. 30. 14. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 0.  6.  0. 16.  6.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3] -> size -> 53 
adversary victory points: 2
player victory points: 11 

Reward from previous game state: 
[ -5.    0.    0.  270.    0.    0.   40.    0.    0.    0.    0.  -90.
   0.    0.   62.5   0. ] 
sum of rewards: 277.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 6.553889274597168






         -------------------- Turn: 44 -------------------- 
Player: 1 
cards in hand: [ 0.  6.  0. 16.  6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  6.  0. 16.  6.] 
cards in discard: [] 
cards in deck: 48 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3] -> size -> 53 
action values: 1 
buys: 0 
player value: 0 
card supply: [17. 24. 30. 14. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [15. 15. 15.  3.  3.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1.] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25] -> size -> 44 
adversary victory points: 11
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  8. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  6.  0. 16.  6.] 
cards in discard: [] 
cards in deck: 48 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3] -> size -> 53 
action values: 0 
buys: 1 
player value: 2 
card supply: [17. 24. 30. 14. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [15. 15. 15.  3.  3.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1.] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25] -> size -> 44 
adversary victory points: 11
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  6.  0. 16.  6.] 
cards in discard: [3.] 
cards in deck: 48 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3] -> size -> 54 
action values: 0 
buys: 0 
player value: 0 
card supply: [17. 24. 30. 13. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [15. 15. 15.  3.  3.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1.] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25] -> size -> 44 
adversary victory points: 11
player victory points: 3 





Player: 0 
cards in hand: [15. 15. 15.  3.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 15. 15.] 
expected returns: [[-5.7034874 ]
 [ 0.83212113]
 [ 0.83212113]
 [ 0.83212113]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 15. 15.  3.  3.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1.] 
cards in deck: 23 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25] -> size -> 44 
action values: 1 
buys: 0 
player value: 0 
card supply: [17. 24. 30. 13. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [3. 0. 6. 0. 3.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3] -> size -> 54 
adversary victory points: 3
player victory points: 11 

Reward from previous game state: 
[ -5   0   0 240   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 235 

action type: buy - action -1
Learning step: 0
desired expected reward: -17.935848236083984



action possibilites: [-1] 
expected returns: [[-21.815681]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 15.  3.  3.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1.] 
cards in deck: 23 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25] -> size -> 44 
action values: 0 
buys: 0 
player value: 0 
card supply: [17. 24. 30. 13. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [3. 0. 6. 0. 3.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3] -> size -> 54 
adversary victory points: 3
player victory points: 11 

Reward from previous game state: 
[ -5   0   0 240   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 255 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: 0.8321278095245361





----------- BUY PHASE ----------- 
buy possibilites: [ 0. -1.] 
expected returns: [[-23.345098]
 [-21.699738]]
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15. 15.  3.  3.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1.] 
cards in deck: 23 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25] -> size -> 44 
action values: 0 
buys: 1 
player value: 0 
card supply: [17. 24. 30. 13. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [3. 0. 6. 0. 3.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3] -> size -> 54 
adversary victory points: 3
player victory points: 11 

Reward from previous game state: 
[ -5   0   0 240   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 255 

action type: take_action - action -1
Learning step: 0
desired expected reward: -21.81568145751953






         -------------------- Turn: 45 -------------------- 
Player: 1 
cards in hand: [3. 0. 6. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 6. 0. 3.] 
cards in discard: [ 3.  0.  6.  0. 16.  6.] 
cards in deck: 43 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3] -> size -> 54 
action values: 1 
buys: 0 
player value: 0 
card supply: [17. 24. 30. 13. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3. 25.  0. 25.  1.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25] -> size -> 44 
adversary victory points: 11
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  8. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 6. 0. 3.] 
cards in discard: [ 3.  0.  6.  0. 16.  6.] 
cards in deck: 43 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3] -> size -> 54 
action values: 0 
buys: 1 
player value: 2 
card supply: [17. 24. 30. 13. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3. 25.  0. 25.  1.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25] -> size -> 44 
adversary victory points: 11
player victory points: 3 


 --------- NOTHING HAPPENED --------- 



Player: 0 
cards in hand: [ 3. 25.  0. 25.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 25.] 
expected returns: [[-9.366826 ]
 [-0.9641018]
 [-0.9641018]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 25.  0. 25.  1.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.] 
cards in deck: 18 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25] -> size -> 44 
action values: 1 
buys: 0 
player value: 0 
card supply: [17. 24. 30. 13. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 6. 16.  3.  8. 14.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3] -> size -> 54 
adversary victory points: 3
player victory points: 11 

Reward from previous game state: 
[ -5   0   0 240   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 235 

action type: buy - action -1.0
Learning step: 0
desired expected reward: -21.699743270874023



action possibilites: [-1] 
expected returns: [[10.21875]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 25.  1.  3. 15.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.] 
cards in deck: 16 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25] -> size -> 44 
action values: 0 
buys: 0 
player value: 0 
card supply: [17. 24. 30. 13. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 6. 16.  3.  8. 14.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3] -> size -> 54 
adversary victory points: 3
player victory points: 11 

Reward from previous game state: 
[ -5   0   0 240   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 255 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: -0.9641010761260986





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3. 11.  8. 10. -1.] 
expected returns: [[ 8.566705]
 [16.715794]
 [11.795291]
 [15.725961]
 [10.162969]
 [11.963708]
 [10.218749]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0. 25.  1.  3. 15.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.] 
cards in deck: 16 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25] -> size -> 44 
action values: 0 
buys: 1 
player value: 3 
card supply: [17. 24. 30. 13. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 6. 16.  3.  8. 14.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3] -> size -> 54 
adversary victory points: 3
player victory points: 11 

Reward from previous game state: 
[ -5   0   0 240   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 255 

action type: take_action - action -1
Learning step: 0
desired expected reward: 10.21875



buy possibilites: [-1] 
expected returns: [[6.385106]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0. 25.  1.  3. 15.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1.] 
cards in deck: 16 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1] -> size -> 45 
action values: 0 
buys: 0 
player value: 0 
card supply: [17. 23. 30. 13. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 6. 16.  3.  8. 14.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3] -> size -> 54 
adversary victory points: 3
player victory points: 11 

Reward from previous game state: 
[  -5    0    0  240    0    0   20    0    0    0    0 -100    0    0
   54    0] 
sum of rewards: 209 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 16.715797424316406






         -------------------- Turn: 46 -------------------- 
Player: 1 
cards in hand: [ 6. 16.  3.  8. 14.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 16.  8. 14.] 
Chosen action: 14 : ['Militia' '14' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 6. 16.  3.  8. 14.] 
cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.] 
cards in deck: 38 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3] -> size -> 54 
action values: 1 
buys: 0 
player value: 0 
card supply: [17. 23. 30. 13. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [15. 29.  1. 25. 15.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15.] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1] -> size -> 45 
adversary victory points: 11
player victory points: 3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 6. 16.  3.  8.] 
cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.] 
cards in deck: 38 
card top of deck: [] 
played cards: [14.] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3] -> size -> 54 
action values: 0 
buys: 0 
player value: 2 
card supply: [17. 23. 30. 13. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 1. 25. 15.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1] -> size -> 45 
adversary victory points: 11
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  8. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 6. 16.  3.  8.] 
cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.] 
cards in deck: 38 
card top of deck: [] 
played cards: [14.] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3] -> size -> 54 
action values: 0 
buys: 1 
player value: 2 
card supply: [17. 23. 30. 13. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 1. 25. 15.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1] -> size -> 45 
adversary victory points: 11
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 6. 16.  3.  8.] 
cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3.] 
cards in deck: 38 
card top of deck: [] 
played cards: [14.] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3  3] -> size -> 55 
action values: 0 
buys: 0 
player value: 0 
card supply: [17. 23. 30. 12. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 1. 25. 15.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1] -> size -> 45 
adversary victory points: 11
player victory points: 4 





Player: 0 
cards in hand: [ 1. 25. 15.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 15.] 
expected returns: [[-24.618965]
 [-16.42685 ]
 [-18.176497]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 25. 15.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1] -> size -> 45 
action values: 1 
buys: 0 
player value: 0 
card supply: [17. 23. 30. 12. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 0.  8.  6.  0. 23.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3  3] -> size -> 55 
adversary victory points: 4
player victory points: 11 

Reward from previous game state: 
[ -5   0   0 210   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 205 

action type: discard_down_to_3_cards - action 5
Learning step: 0
desired expected reward: -38.782135009765625



action possibilites: [-1] 
expected returns: [[-13.441501]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 15.  3. 29.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.] 
cards in deck: 9 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1] -> size -> 45 
action values: 0 
buys: 0 
player value: 0 
card supply: [17. 23. 30. 12. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 0.  8.  6.  0. 23.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3  3] -> size -> 55 
adversary victory points: 4
player victory points: 11 

Reward from previous game state: 
[ -5   0   0 210   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 225 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: -16.42684555053711





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  8. -1.] 
expected returns: [[-15.082061 ]
 [-12.148721 ]
 [-13.8029585]
 [-13.441501 ]]
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1. 15.  3. 29.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.] 
cards in deck: 9 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1] -> size -> 45 
action values: 0 
buys: 1 
player value: 2 
card supply: [17. 23. 30. 12. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 0.  8.  6.  0. 23.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3  3] -> size -> 55 
adversary victory points: 4
player victory points: 11 

Reward from previous game state: 
[ -5   0   0 210   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 225 

action type: take_action - action -1
Learning step: 0
desired expected reward: -13.441500663757324



buy possibilites: [-1] 
expected returns: [[-6.122857]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1. 15.  3. 29.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.  3.] 
cards in deck: 9 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3] -> size -> 46 
action values: 0 
buys: 0 
player value: 0 
card supply: [17. 23. 30. 11. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 0.  8.  6.  0. 23.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.] 
adversary owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3  3] -> size -> 55 
adversary victory points: 4
player victory points: 12 

Reward from previous game state: 
[  -5    0    0  240    0    0   20    0    0    0    0 -110    0    0
   16    0] 
sum of rewards: 161 

action type: buy - action 3.0
Learning step: 0
desired expected reward: -12.148716926574707






         -------------------- Turn: 47 -------------------- 
Player: 1 
cards in hand: [ 0.  8.  6.  0. 23.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8. 23.] 
Chosen action: 23 : ['Market' '23' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  8.  6.  0. 23.] 
cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.] 
cards in deck: 33 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3  3] -> size -> 55 
action values: 1 
buys: 0 
player value: 0 
card supply: [17. 23. 30. 11. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3. 29. 15. 29.  3.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.  3. 25.  1. 15.  3.
 29.] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3] -> size -> 46 
adversary victory points: 12
player victory points: 4 


action possibilites: [-1.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 8. 6. 0. 0.] 
cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.] 
cards in deck: 32 
card top of deck: [] 
played cards: [23.] 
owned cards: [ 0  0  0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6
  0  6  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14
  3  0  0  3  3  3  3] -> size -> 55 
action values: 1 
buys: 1 
player value: 1 
card supply: [17. 23. 30. 11. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3. 29. 15. 29.  3.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.  3. 25.  1. 15.  3.
 29.] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3] -> size -> 46 
adversary victory points: 12
player victory points: 4 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [6. 0.] 
cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.] 
cards in deck: 32 
card top of deck: [] 
played cards: [23.  8.] 
owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3] -> size -> 53 
action values: 0 
buys: 1 
player value: 1 
card supply: [17. 23. 30. 11. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3. 29. 15. 29.  3.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.  3. 25.  1. 15.  3.
 29.] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3] -> size -> 46 
adversary victory points: 12
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  8. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [6. 0.] 
cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.] 
cards in deck: 32 
card top of deck: [] 
played cards: [23.  8.] 
owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3] -> size -> 53 
action values: 0 
buys: 2 
player value: 2 
card supply: [17. 23. 30. 11. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3. 29. 15. 29.  3.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.  3. 25.  1. 15.  3.
 29.] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3] -> size -> 46 
adversary victory points: 12
player victory points: 4 


buy possibilites: [ 0. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [6. 0.] 
cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.] 
cards in deck: 32 
card top of deck: [] 
played cards: [23.  8.] 
owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3] -> size -> 54 
action values: 0 
buys: 1 
player value: 0 
card supply: [17. 23. 30. 10. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3. 29. 15. 29.  3.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.  3. 25.  1. 15.  3.
 29.] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3] -> size -> 46 
adversary victory points: 12
player victory points: 5 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [6. 0.] 
cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.
  0.] 
cards in deck: 32 
card top of deck: [] 
played cards: [23.  8.] 
owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3  0] -> size -> 55 
action values: 0 
buys: 0 
player value: 0 
card supply: [16. 23. 30. 10. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3. 29. 15. 29.  3.] 
adversary cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.  3. 25.  1. 15.  3.
 29.] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3] -> size -> 46 
adversary victory points: 12
player victory points: 5 





Player: 0 
cards in hand: [ 3. 29. 15. 29.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 15. 29.] 
expected returns: [[-21.6544  ]
 [-11.5928  ]
 [-15.205168]
 [-11.5928  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29. 15. 29.  3.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.  3. 25.  1. 15.  3.
 29.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3] -> size -> 46 
action values: 1 
buys: 0 
player value: 0 
card supply: [16. 23. 30. 10. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [14.  0.  0.  0.  0.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.
  0. 23.  8.  6.  0.] 
adversary owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3  0] -> size -> 55 
adversary victory points: 5
player victory points: 12 

Reward from previous game state: 
[ -5   0   0 210   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 205 

action type: buy - action -1
Learning step: 0
desired expected reward: -6.122857093811035



action possibilites: [-1. 15. 29.] 
expected returns: [[-32.14157 ]
 [-25.687584]
 [-22.645092]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15. 29.  3.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.  3. 25.  1. 15.  3.
 29.  3. 25.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3] -> size -> 46 
action values: 1 
buys: 0 
player value: 1 
card supply: [16. 23. 30. 10. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [14.  0.  0.  0.  0.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.
  0. 23.  8.  6.  0.] 
adversary owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3  0] -> size -> 55 
adversary victory points: 5
player victory points: 12 

Reward from previous game state: 
[ -5   0   0 210   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 225 

action type: discard_n_cards - action 5
Learning step: 0
desired expected reward: -14.106484413146973



action possibilites: [-1. 15.] 
expected returns: [[-24.690117]
 [-18.053669]]
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.  3. 25.  1. 15.  3.
 29.  3. 25. 15.  3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3] -> size -> 46 
action values: 1 
buys: 0 
player value: 2 
card supply: [16. 23. 30. 10. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [14.  0.  0.  0.  0.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.
  0. 23.  8.  6.  0.] 
adversary owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3  0] -> size -> 55 
adversary victory points: 5
player victory points: 12 

Reward from previous game state: 
[ -5   0   0 210   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 245 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -26.422855377197266



action possibilites: [-1] 
expected returns: [[-20.254208]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.  3. 25.  1. 15.  3.
 29.  3. 25. 15.  3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3] -> size -> 46 
action values: 0 
buys: 0 
player value: 2 
card supply: [16. 23. 30. 10. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [14.  0.  0.  0.  0.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.
  0. 23.  8.  6.  0.] 
adversary owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3  0] -> size -> 55 
adversary victory points: 5
player victory points: 12 

Reward from previous game state: 
[ -5   0   0 210   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 265 

action type: take_action - action 15.0
Learning step: 0
desired expected reward: -18.053653717041016





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  8. -1.] 
expected returns: [[-21.699621]
 [-18.68703 ]
 [-20.3435  ]
 [-20.254211]]
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.  3. 25.  1. 15.  3.
 29.  3. 25. 15.  3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3] -> size -> 46 
action values: 0 
buys: 1 
player value: 2 
card supply: [16. 23. 30. 10. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [14.  0.  0.  0.  0.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.
  0. 23.  8.  6.  0.] 
adversary owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3  0] -> size -> 55 
adversary victory points: 5
player victory points: 12 

Reward from previous game state: 
[ -5   0   0 210   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 265 

action type: take_action - action -1
Learning step: 0
desired expected reward: -20.254207611083984



buy possibilites: [-1] 
expected returns: [[-25.269953]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [ 3. 25.  1.  3.  3. 29. 29. 25. 29. 25.  3. 29. 25. 29. 15.  1. 15. 15.
 15.  3.  3.  1. 25.  3.  0. 25.  1.  3. 15. 15. 29.  3. 25.  1. 15.  3.
 29.  3. 25. 15.  3.  3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29. 29. 15.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3  3] -> size -> 47 
action values: 0 
buys: 0 
player value: 0 
card supply: [16. 23. 30.  9. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [14.  0.  0.  0.  0.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.
  0. 23.  8.  6.  0.] 
adversary owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3  0] -> size -> 55 
adversary victory points: 5
player victory points: 13 

Reward from previous game state: 
[  -5    0    0  240    0    0   60    0    0    0    0 -120    0    0
   16    0] 
sum of rewards: 191 

action type: buy - action 3.0
Learning step: 0
desired expected reward: -18.687026977539062






         -------------------- Turn: 48 -------------------- 
Player: 1 
cards in hand: [14.  0.  0.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 14.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [14.  0.  0.  0.  0.] 
cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.
  0. 23.  8.  6.  0.] 
cards in deck: 27 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3  0] -> size -> 55 
action values: 1 
buys: 0 
player value: 0 
card supply: [16. 23. 30.  9. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3. 15.  3. 25. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3  3] -> size -> 47 
adversary victory points: 13
player victory points: 5 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [14.  0.  0.  0.  0.] 
cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.
  0. 23.  8.  6.  0.] 
cards in deck: 27 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3  0] -> size -> 55 
action values: 0 
buys: 1 
player value: 4 
card supply: [16. 23. 30.  9. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3. 15.  3. 25. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3  3] -> size -> 47 
adversary victory points: 13
player victory points: 5 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [14.  0.  0.  0.  0.] 
cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.
  0. 23.  8.  6.  0.  0.] 
cards in deck: 27 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3  0  0] -> size -> 56 
action values: 0 
buys: 0 
player value: 4 
card supply: [15. 23. 30.  9. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3. 15.  3. 25. 29.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3  3] -> size -> 47 
adversary victory points: 13
player victory points: 5 





Player: 0 
cards in hand: [ 3. 15.  3. 25. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 25. 29.] 
expected returns: [[-13.475097 ]
 [ -7.1985807]
 [ -5.522702 ]
 [ -3.7584805]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 15.  3. 25. 29.] 
cards in discard: [] 
cards in deck: 42 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3  3] -> size -> 47 
action values: 1 
buys: 0 
player value: 0 
card supply: [15. 23. 30.  9. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [10.  0.  6.  8. 11.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.
  0. 23.  8.  6.  0.  0. 14.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3  0  0] -> size -> 56 
adversary victory points: 5
player victory points: 13 

Reward from previous game state: 
[ -5   0   0 240   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: 235 

action type: buy - action -1
Learning step: 0
desired expected reward: -25.26995277404785



action possibilites: [-1. 25. 15.] 
expected returns: [[-19.21542 ]
 [-11.071247]
 [-12.824962]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 25. 15.] 
cards in discard: [ 3. 15.] 
cards in deck: 41 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3  3] -> size -> 47 
action values: 1 
buys: 0 
player value: 1 
card supply: [15. 23. 30.  9. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [10.  0.  6.  8. 11.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.
  0. 23.  8.  6.  0.  0. 14.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3  0  0] -> size -> 56 
adversary victory points: 5
player victory points: 13 

Reward from previous game state: 
[ -5   0   0 240   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: 255 

action type: discard_n_cards - action 5
Learning step: 0
desired expected reward: -6.1520233154296875



action possibilites: [-1] 
expected returns: [[-17.820005]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 15. 15.  0.] 
cards in discard: [ 3. 15.] 
cards in deck: 39 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3  3] -> size -> 47 
action values: 0 
buys: 0 
player value: 1 
card supply: [15. 23. 30.  9. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [10.  0.  6.  8. 11.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.
  0. 23.  8.  6.  0.  0. 14.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3  0  0] -> size -> 56 
adversary victory points: 5
player victory points: 13 

Reward from previous game state: 
[ -5   0   0 240   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 275 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: -11.071236610412598





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  8. -1.] 
expected returns: [[-20.210054]
 [-17.145727]
 [-18.630611]
 [-18.566097]]
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 15. 15.  0.] 
cards in discard: [ 3. 15.] 
cards in deck: 39 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3  3] -> size -> 47 
action values: 0 
buys: 1 
player value: 2 
card supply: [15. 23. 30.  9. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [10.  0.  6.  8. 11.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.
  0. 23.  8.  6.  0.  0. 14.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3  0  0] -> size -> 56 
adversary victory points: 5
player victory points: 13 

Reward from previous game state: 
[ -5   0   0 240   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 275 

action type: take_action - action -1
Learning step: 0
desired expected reward: -17.820005416870117



buy possibilites: [-1] 
expected returns: [[-0.8370893]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 15. 15.  0.] 
cards in discard: [ 3. 15.  3.] 
cards in deck: 39 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3  3  3] -> size -> 48 
action values: 0 
buys: 0 
player value: 0 
card supply: [15. 23. 30.  8. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [10.  0.  6.  8. 11.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.
  0. 23.  8.  6.  0.  0. 14.  0.  0.  0.  0.] 
adversary owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3  0  0] -> size -> 56 
adversary victory points: 5
player victory points: 14 

Reward from previous game state: 
[  -5    0    0  270    0    0   40    0    0    0    0 -130    0    0
   16    0] 
sum of rewards: 191 

action type: buy - action 3.0
Learning step: 0
desired expected reward: -17.145719528198242






         -------------------- Turn: 49 -------------------- 
Player: 1 
cards in hand: [10.  0.  6.  8. 11.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 10.  8. 11.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [10.  0.  6.  8. 11.] 
cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.
  0. 23.  8.  6.  0.  0. 14.  0.  0.  0.  0.] 
cards in deck: 22 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3  0  0] -> size -> 56 
action values: 1 
buys: 0 
player value: 0 
card supply: [15. 23. 30.  8. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  1.] 
adversary cards in hand: [ 3.  3. 25. 25. 15.] 
adversary cards in discard: [ 3. 15.  3. 29. 25.  3. 15. 15.  0.] 
adversary owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3  3  3] -> size -> 48 
adversary victory points: 14
player victory points: 5 


Player 0 won the game! 



Player 0 bought cards:
Copper: 0 
Silver: 5 
Gold: 0 
Estate: 11 
Duchy: 0 
Province: 0 
Curse: 0 

Remodel: 0 
Workshop: 0 
Chapel: 0 
Witch: 9 
Poacher: 10 
Militia: 0 
Market: 0 
Village: 0 
Library: 0 
Moneylender: 9 


 --- GAME STATE ---
Action that is taken: None 
cards in hand: [ 3.  3. 25. 25. 15.] 
cards in discard: [ 3. 15.  3. 29. 25.  3. 15. 15.  0.] 
cards in deck: 34 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  3  3  3 29  1  1 29 29 29 29 29 29 29 29 29 25 15 15 25 15 15 25 25
 25  3 25 15 25  3  3 25 15  1 15  3  3 15  3  1 15  3  3 25  1  3  3  3] -> size -> 48 
action values: 0 
buys: 0 
player value: 0 
card supply: [15. 23. 30.  8. 30.  8.  0.  4.  6.  6.  1.  0.  7.  9.  8. 10.  0.] 
adversary cards in hand: [10.  0.  6.  8.] 
adversary cards in discard: [ 3.  0.  6.  0. 16.  6.  3.  0.  6.  0.  3.  3. 14.  6. 16.  3.  8.  3.
  0. 23.  8.  6.  0.  0. 14.  0.  0.  0.  0. 15.] 
adversary owned cards: [ 0  0  3  0 11  3 11  0  1  0 11 16  0  3 11 16  0  0  3  8 14  6  0  6
  0  6 16  0  6  3  6  6  8  6  3 10  8 16 14 16 23  8  0  1  0 14  3  0
  0  3  3  3  3  3  0  0 15] -> size -> 57 
adversary victory points: 5
player victory points: 14 

Reward from previous game state: 
[     -5 3000000       0     270       0       0       0       0       0
       0       0       0       0       0       0       0] 
sum of rewards: 3000265 

action type: buy - action -1
Learning step: 120010.625
desired expected reward: 120009.7890625



