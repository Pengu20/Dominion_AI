 --------- CARDS IN THIS GAME --------- 
Game cards: [['Copper' '0' '0']
 ['Silver' '1' '3']
 ['Gold' '2' '6']
 ['Estate' '3' '2']
 ['Duchy' '4' '5']
 ['Province' '5' '8']
 ['Curse' '6' '0']
 ['Remodel' '16' '4']
 ['Workshop' '11' '3']
 ['Chapel' '8' '2']
 ['Witch' '25' '5']
 ['Poacher' '29' '4']
 ['Militia' '14' '4']
 ['Market' '23' '5']
 ['Village' '10' '3']
 ['Library' '22' '5']
 ['Moneylender' '15' '4']]






         -------------------- Turn: 1 -------------------- 
Player: 1 
cards in hand: [3. 0. 0. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 10.0 : ['Village' '10' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 0 
buys: 1 
player value: 3 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.] 
adversary cards in hand: [0. 0. 3. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 3. 0.] 
cards in discard: [10.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10] -> size -> 11 
action values: 0 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 3. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
adversary victory points: 3
player victory points: 3 





Player: 0 
cards in hand: [0. 0. 3. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[5.181755]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3. 3. 0.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [10.  3.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[      -5 -3000000        0     -210        0        0       20        0
        0        0        0        0        0        0        0        0] 
sum of rewards: -3000195 

action type: gain_card_n - action 0
Learning step: -119991.4140625
desired expected reward: -120401.03125





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[  6.439213]
 [ 21.226866]
 [ 11.014725]
 [-95.680756]
 [ 17.298788]
 [  6.451829]
 [ 14.351206]
 [  3.863454]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 3. 0.] 
cards in discard: [] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3] -> size -> 10 
action values: 0 
buys: 1 
player value: 3 
card supply: [30. 30. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [10.  3.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 4.553812026977539



buy possibilites: [-1] 
expected returns: [[8.915365]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 3. 0.] 
cards in discard: [1.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3 1] -> size -> 11 
action values: 0 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 3.] 
adversary cards in discard: [10.  3.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0 54  0] 
sum of rewards: 49 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 21.226865768432617






         -------------------- Turn: 2 -------------------- 
Player: 1 
cards in hand: [0. 0. 0. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [10.  3.  0.  0.  3.  0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10] -> size -> 11 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 0. 3. 0.] 
adversary cards in discard: [1. 0. 0. 3. 3. 0.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 1] -> size -> 11 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 3.] 
cards in discard: [10.  3.  0.  0.  3.  0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10] -> size -> 11 
action values: 0 
buys: 1 
player value: 4 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 0. 3. 0.] 
adversary cards in discard: [1. 0. 0. 3. 3. 0.] 
adversary owned cards: [0 0 0 0 0 0 0 3 3 3 1] -> size -> 11 
adversary victory points: 3
player victory points: 3 


 --------- NOTHING HAPPENED --------- 



Player: 0 
cards in hand: [0. 0. 0. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[9.510159]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 3. 0.] 
cards in discard: [1. 0. 0. 3. 3. 0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3 1] -> size -> 11 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10.  9. 10. 10.] 
adversary cards in hand: [3. 0. 3. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 8.915365219116211





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
expected returns: [[ 11.176928]
 [ 25.866169]
 [ 15.75243 ]
 [-81.49556 ]
 [ 20.768429]
 [ 22.0365  ]
 [ 11.189539]
 [ 33.57452 ]
 [ 21.3115  ]
 [ 19.088911]
 [ 25.49326 ]
 [  8.366287]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3. 0.] 
cards in discard: [1. 0. 0. 3. 3. 0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [0 0 0 0 0 0 0 3 3 3 1] -> size -> 11 
action values: 0 
buys: 1 
player value: 4 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10. 10. 10. 10. 10. 10.  9. 10. 10.] 
adversary cards in hand: [3. 0. 3. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 7.430837631225586



buy possibilites: [-1] 
expected returns: [[-6.222788]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3. 0.] 
cards in discard: [ 1.  0.  0.  3.  3.  0. 29.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29] -> size -> 12 
action values: 0 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10.  9. 10. 10.] 
adversary cards in hand: [3. 0. 3. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0   0   0   0   0   0   0   0   0   0   0   0 128   0] 
sum of rewards: 123 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 33.57450866699219






         -------------------- Turn: 3 -------------------- 
Player: 1 
cards in hand: [3. 0. 3. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 3. 0. 0.] 
cards in discard: [] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10] -> size -> 11 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 3. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29] -> size -> 12 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 3. 0. 0.] 
cards in discard: [] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10] -> size -> 11 
action values: 0 
buys: 1 
player value: 3 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 3. 0. 0. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29] -> size -> 12 
adversary victory points: 3
player victory points: 3 


 --------- NOTHING HAPPENED --------- 



Player: 0 
cards in hand: [0. 3. 0. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[2.1266344]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29] -> size -> 12 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 0. 3. 0.] 
adversary cards in discard: [3. 0. 3. 0. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: -6.222787857055664





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
expected returns: [[  4.126754 ]
 [ 19.202625 ]
 [  8.98182  ]
 [-92.16601  ]
 [ 14.236353 ]
 [ 15.385799 ]
 [  3.9719093]
 [ 26.69743  ]
 [ 14.686287 ]
 [ 12.372    ]
 [ 18.765543 ]
 [  2.4229233]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29] -> size -> 12 
action values: 0 
buys: 1 
player value: 4 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10. 10. 10.  9. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 0. 3. 0.] 
adversary cards in discard: [3. 0. 3. 0. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 0.9604618549346924



buy possibilites: [-1] 
expected returns: [[16.854744]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [29.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29] -> size -> 13 
action values: 0 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10. 10. 10.  8. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 0. 3. 0.] 
adversary cards in discard: [3. 0. 3. 0. 0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10] -> size -> 11 
adversary victory points: 3
player victory points: 3 

Reward from previous game state: 
[ -5   0   0   0   0   0   0   0   0   0   0   0   0   0 128   0] 
sum of rewards: 123 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 26.697418212890625






         -------------------- Turn: 4 -------------------- 
Player: 1 
cards in hand: [0. 0. 0. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 3. 0.] 
cards in discard: [3. 0. 3. 0. 0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10] -> size -> 11 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10. 10. 10.  8. 10. 10.  9. 10. 10.] 
adversary cards in hand: [29.  1.  3.  3.  0.] 
adversary cards in discard: [29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29] -> size -> 13 
adversary victory points: 3
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3. 0.] 
cards in discard: [3. 0. 3. 0. 0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10] -> size -> 11 
action values: 0 
buys: 1 
player value: 4 
card supply: [30. 29. 30. 30. 30.  8. 10. 10. 10. 10. 10.  8. 10. 10.  9. 10. 10.] 
adversary cards in hand: [29.  1.  3.  3.  0.] 
adversary cards in discard: [29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29] -> size -> 13 
adversary victory points: 3
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3. 0.] 
cards in discard: [3. 0. 3. 0. 0. 3.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3] -> size -> 12 
action values: 0 
buys: 0 
player value: 2 
card supply: [30. 29. 30. 29. 30.  8. 10. 10. 10. 10. 10.  8. 10. 10.  9. 10. 10.] 
adversary cards in hand: [29.  1.  3.  3.  0.] 
adversary cards in discard: [29.  0.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29] -> size -> 13 
adversary victory points: 3
player victory points: 4 





Player: 0 
cards in hand: [29.  1.  3.  3.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[-9.562357]
 [14.698776]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  1.  3.  3.  0.] 
cards in discard: [29.  0.  3.  0.  0.  0.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29] -> size -> 13 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 29. 30.  8. 10. 10. 10. 10. 10.  8. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  3.  3.  3. 10.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3] -> size -> 12 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -35 

action type: buy - action -1
Learning step: 0
desired expected reward: 16.85474395751953



action possibilites: [-1.] 
expected returns: [[-0.02749419]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 3. 3. 0. 0.] 
cards in discard: [29.  0.  3.  0.  0.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29] -> size -> 13 
action values: 1 
buys: 0 
player value: 1 
card supply: [30. 29. 30. 29. 30.  8. 10. 10. 10. 10. 10.  8. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  3.  3.  3. 10.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3] -> size -> 12 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 13.492977142333984





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[  7.0692673]
 [ 21.58572  ]
 [ 11.364782 ]
 [-30.070225 ]
 [-99.516045 ]
 [ 16.547487 ]
 [ 17.824186 ]
 [  7.366495 ]
 [ 23.022823 ]
 [ 29.263851 ]
 [ 17.096066 ]
 [ 22.517202 ]
 [ 14.86849  ]
 [ 16.131342 ]
 [ 21.259068 ]
 [  4.337556 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 3. 3. 0. 0.] 
cards in discard: [29.  0.  3.  0.  0.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29] -> size -> 13 
action values: 0 
buys: 1 
player value: 5 
card supply: [30. 29. 30. 29. 30.  8. 10. 10. 10. 10. 10.  8. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  3.  3.  3. 10.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3] -> size -> 12 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -0.027494192123413086



buy possibilites: [-1] 
expected returns: [[-2.8988366]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 3. 3. 0. 0.] 
cards in discard: [29.  0.  3.  0.  0.  0. 29.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29] -> size -> 14 
action values: 0 
buys: 0 
player value: 1 
card supply: [30. 29. 30. 29. 30.  8. 10. 10. 10. 10. 10.  7. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  3.  3.  3. 10.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3] -> size -> 12 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5.   0.   0. -30.   0.   0.  20.   0.   0.   0.   0.   0.   0.   0.
  32.   0.] 
sum of rewards: 17.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 29.263843536376953






         -------------------- Turn: 5 -------------------- 
Player: 1 
cards in hand: [ 0.  3.  3.  3. 10.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 10.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3.  3.  3. 10.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3] -> size -> 12 
action values: 1 
buys: 0 
player value: 0 
card supply: [30. 29. 30. 29. 30.  8. 10. 10. 10. 10. 10.  7. 10. 10.  9. 10. 10.] 
adversary cards in hand: [1. 0. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29] -> size -> 14 
adversary victory points: 3
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3.  3.  3. 10.] 
cards in discard: [] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3] -> size -> 12 
action values: 0 
buys: 1 
player value: 1 
card supply: [30. 29. 30. 29. 30.  8. 10. 10. 10. 10. 10.  7. 10. 10.  9. 10. 10.] 
adversary cards in hand: [1. 0. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29] -> size -> 14 
adversary victory points: 3
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3.  3.  3. 10.] 
cards in discard: [0.] 
cards in deck: 7 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0] -> size -> 13 
action values: 0 
buys: 0 
player value: 1 
card supply: [29. 29. 30. 29. 30.  8. 10. 10. 10. 10. 10.  7. 10. 10.  9. 10. 10.] 
adversary cards in hand: [1. 0. 0. 3. 0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29] -> size -> 14 
adversary victory points: 3
player victory points: 4 





Player: 0 
cards in hand: [1. 0. 0. 3. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[-3.8420084]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 0. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29] -> size -> 14 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 29. 30.  8. 10. 10. 10. 10. 10.  7. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 0.] 
adversary cards in discard: [ 0.  0.  3.  3.  3. 10.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0] -> size -> 13 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -35 

action type: buy - action -1
Learning step: 0
desired expected reward: -2.898836612701416





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[  -4.9346495]
 [  11.601687 ]
 [  -0.5344641]
 [ -66.93528  ]
 [-136.09857  ]
 [   6.8297157]
 [   6.9414606]
 [  -7.746175 ]
 [  12.620144 ]
 [  18.20325  ]
 [   6.731641 ]
 [  12.203308 ]
 [   4.0198965]
 [   6.2186737]
 [  10.796451 ]
 [  -3.7570372]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 0. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29] -> size -> 14 
action values: 0 
buys: 1 
player value: 5 
card supply: [29. 29. 30. 29. 30.  8. 10. 10. 10. 10. 10.  7. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 0.] 
adversary cards in discard: [ 0.  0.  3.  3.  3. 10.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0] -> size -> 13 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -35 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -7.069000244140625



buy possibilites: [-1] 
expected returns: [[-5.218917]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 0. 0. 3. 0.] 
cards in discard: [29.] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29] -> size -> 15 
action values: 0 
buys: 0 
player value: 1 
card supply: [29. 29. 30. 29. 30.  8. 10. 10. 10. 10. 10.  6. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 0. 0. 0.] 
adversary cards in discard: [ 0.  0.  3.  3.  3. 10.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0] -> size -> 13 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5.   0.   0. -30.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
  32.   0.] 
sum of rewards: -3.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 18.203256607055664






         -------------------- Turn: 6 -------------------- 
Player: 1 
cards in hand: [0. 0. 0. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [ 0.  0.  3.  3.  3. 10.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0] -> size -> 13 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 29. 30.  8. 10. 10. 10. 10. 10.  6. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 0. 29.  3.  0. 29.] 
adversary cards in discard: [29.  1.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29] -> size -> 15 
adversary victory points: 3
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [ 0.  0.  3.  3.  3. 10.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0] -> size -> 13 
action values: 0 
buys: 1 
player value: 5 
card supply: [29. 29. 30. 29. 30.  8. 10. 10. 10. 10. 10.  6. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 0. 29.  3.  0. 29.] 
adversary cards in discard: [29.  1.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29] -> size -> 15 
adversary victory points: 3
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 0. 0.] 
cards in discard: [ 0.  0.  3.  3.  3. 10. 25.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25] -> size -> 14 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 29. 30.  8. 10. 10. 10. 10.  9.  6. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 0. 29.  3.  0. 29.] 
adversary cards in discard: [29.  1.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29] -> size -> 15 
adversary victory points: 3
player victory points: 4 





Player: 0 
cards in hand: [ 0. 29.  3.  0. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[ 8.612283]
 [33.676052]
 [33.676052]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29.  3.  0. 29.] 
cards in discard: [29.  1.  0.  0.  3.  0.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29] -> size -> 15 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 29. 30.  8. 10. 10. 10. 10.  9.  6. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25] -> size -> 14 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -35 

action type: buy - action -1
Learning step: 0
desired expected reward: -5.218916893005371



action possibilites: [-1. 29.] 
expected returns: [[ 3.9250019]
 [27.285057 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3.  0. 29.  0.] 
cards in discard: [29.  1.  0.  0.  3.  0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29] -> size -> 15 
action values: 1 
buys: 0 
player value: 1 
card supply: [29. 29. 30. 29. 30.  8. 10. 10. 10. 10.  9.  6. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25] -> size -> 14 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 30.861324310302734



action possibilites: [-1. 29.] 
expected returns: [[21.100101]
 [45.029408]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3.  0.  0. 29.] 
cards in discard: [29.  1.  0.  0.  3.  0.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29] -> size -> 15 
action values: 1 
buys: 0 
player value: 2 
card supply: [29. 29. 30. 29. 30.  8. 10. 10. 10. 10.  9.  6. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25] -> size -> 14 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 5 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 27.285062789916992



action possibilites: [-1.] 
expected returns: [[18.811697]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [29.  1.  0.  0.  3.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29] -> size -> 15 
action values: 1 
buys: 0 
player value: 3 
card supply: [29. 29. 30. 29. 30.  8. 10. 10. 10. 10.  9.  6. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25] -> size -> 14 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 25 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 45.0294075012207





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 22.768349]
 [ 37.145836]
 [-11.560332]
 [ 27.161493]
 [-14.864458]
 [-67.257   ]
 [ 32.287296]
 [ 33.438942]
 [ 22.996874]
 [ 38.47366 ]
 [ 44.496384]
 [ 32.750084]
 [ 37.996582]
 [ 30.613956]
 [ 31.849274]
 [ 36.746876]
 [ 20.673279]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [29.  1.  0.  0.  3.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29] -> size -> 15 
action values: 0 
buys: 1 
player value: 7 
card supply: [29. 29. 30. 29. 30.  8. 10. 10. 10. 10.  9.  6. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25] -> size -> 14 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: 25 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 18.811697006225586



buy possibilites: [-1] 
expected returns: [[12.782875]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 0. 0. 0.] 
cards in discard: [29.  1.  0.  0.  3.  0. 29.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29] -> size -> 16 
action values: 0 
buys: 0 
player value: 3 
card supply: [29. 29. 30. 29. 30.  8. 10. 10. 10. 10.  9.  5. 10. 10.  9. 10. 10.] 
adversary cards in hand: [0. 0. 3. 0. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25] -> size -> 14 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5.   0.   0. -30.   0.   0.  60.   0.   0.   0.   0.   0.   0.   0.
  32.   0.] 
sum of rewards: 57.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 44.496376037597656






         -------------------- Turn: 7 -------------------- 
Player: 1 
cards in hand: [0. 0. 3. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3. 0. 3.] 
cards in discard: [] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25] -> size -> 14 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 29. 30.  8. 10. 10. 10. 10.  9.  5. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 3.  0. 29.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29] -> size -> 16 
adversary victory points: 3
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 11.0 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 3.] 
cards in discard: [] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25] -> size -> 14 
action values: 0 
buys: 1 
player value: 3 
card supply: [29. 29. 30. 29. 30.  8. 10. 10. 10. 10.  9.  5. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 3.  0. 29.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29] -> size -> 16 
adversary victory points: 3
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 3.] 
cards in discard: [11.] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11] -> size -> 15 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 29. 30.  8. 10. 10.  9. 10.  9.  5. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 3.  0. 29.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29] -> size -> 16 
adversary victory points: 3
player victory points: 4 





Player: 0 
cards in hand: [ 3.  0. 29.  0.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[-20.062645]
 [  5.234041]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 29.  0.  3.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29] -> size -> 16 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 29. 30.  8. 10. 10.  9. 10.  9.  5. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  0.  3.  3. 25.] 
adversary cards in discard: [11.  0.  0.  3.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11] -> size -> 15 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -35 

action type: buy - action -1
Learning step: 0
desired expected reward: 12.782875061035156



action possibilites: [-1.] 
expected returns: [[5.8692913]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 10 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29] -> size -> 16 
action values: 1 
buys: 0 
player value: 1 
card supply: [29. 29. 30. 29. 30.  8. 10. 10.  9. 10.  9.  5. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  0.  3.  3. 25.] 
adversary cards in discard: [11.  0.  0.  3.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11] -> size -> 15 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 2.9939448833465576





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
expected returns: [[ 11.432694]
 [ 26.544813]
 [ 16.192715]
 [-80.30572 ]
 [ 21.417736]
 [ 22.598757]
 [ 11.342271]
 [ 34.318302]
 [ 21.87226 ]
 [ 19.575743]
 [ 26.101818]
 [  9.410387]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 3. 0.] 
cards in discard: [] 
cards in deck: 10 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29] -> size -> 16 
action values: 0 
buys: 1 
player value: 4 
card supply: [29. 29. 30. 29. 30.  8. 10. 10.  9. 10.  9.  5. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  0.  3.  3. 25.] 
adversary cards in discard: [11.  0.  0.  3.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11] -> size -> 15 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 5.869291305541992



buy possibilites: [-1] 
expected returns: [[15.486237]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 3. 0.] 
cards in discard: [29.] 
cards in deck: 10 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29] -> size -> 17 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 29. 30.  8. 10. 10.  9. 10.  9.  4. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 0.  0.  3.  3. 25.] 
adversary cards in discard: [11.  0.  0.  3.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11] -> size -> 15 
adversary victory points: 4
player victory points: 3 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0 128   0] 
sum of rewards: 113 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 34.31830978393555






         -------------------- Turn: 8 -------------------- 
Player: 1 
cards in hand: [ 0.  0.  3.  3. 25.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25.] 
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  3.  3. 25.] 
cards in discard: [11.  0.  0.  3.  0.  3.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11] -> size -> 15 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 29. 30.  8. 10. 10.  9. 10.  9.  4. 10. 10.  9. 10. 10.] 
adversary cards in hand: [1. 0. 0. 0. 3.] 
adversary cards in discard: [29. 29.  3.  0.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29] -> size -> 17 
adversary victory points: 3
player victory points: 4 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3. 3. 0. 0.] 
cards in discard: [11.  0.  0.  3.  0.  3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11] -> size -> 15 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 29. 30. 29. 30.  8.  9. 10.  9. 10.  9.  4. 10. 10.  9. 10. 10.] 
adversary cards in hand: [1. 0. 0. 0. 3.] 
adversary cards in discard: [29. 29.  3.  0.  0.  3.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6] -> size -> 18 
adversary victory points: 3
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 29. 14. 10. 15. -1.] 
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 3. 0. 0.] 
cards in discard: [11.  0.  0.  3.  0.  3.] 
cards in deck: 2 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11] -> size -> 15 
action values: 0 
buys: 1 
player value: 4 
card supply: [29. 29. 30. 29. 30.  8.  9. 10.  9. 10.  9.  4. 10. 10.  9. 10. 10.] 
adversary cards in hand: [1. 0. 0. 0. 3.] 
adversary cards in discard: [29. 29.  3.  0.  0.  3.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6] -> size -> 18 
adversary victory points: 3
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 3. 0. 0.] 
cards in discard: [11.  0.  0.  3.  0.  3.  1.] 
cards in deck: 2 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1] -> size -> 16 
action values: 0 
buys: 0 
player value: 1 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9. 10.  9.  4. 10. 10.  9. 10. 10.] 
adversary cards in hand: [1. 0. 0. 0. 3.] 
adversary cards in discard: [29. 29.  3.  0.  0.  3.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6] -> size -> 18 
adversary victory points: 3
player victory points: 4 





Player: 0 
cards in hand: [1. 0. 0. 0. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
expected returns: [[-0.34015822]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 0. 0. 0. 3.] 
cards in discard: [29. 29.  3.  0.  0.  3.  0.  6.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6] -> size -> 18 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9. 10.  9.  4. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 0. 25.  0.  0. 10.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1] -> size -> 16 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[  -5    0    0  -60    0    0    0    0    0    0    0    0    0 -300
    0    0] 
sum of rewards: -365 

action type: buy - action -1
Learning step: 0
desired expected reward: 15.486236572265625





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[  1.2279084 ]
 [ 15.180731  ]
 [  5.5127525 ]
 [-34.16166   ]
 [-91.212     ]
 [ 10.482723  ]
 [ 11.376899  ]
 [  1.0801194 ]
 [ 16.33929   ]
 [ 22.319183  ]
 [ 10.751901  ]
 [ 15.898611  ]
 [  8.6664505 ]
 [  9.9665575 ]
 [ 14.593229  ]
 [ -0.20516515]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 0. 0. 0. 3.] 
cards in discard: [29. 29.  3.  0.  0.  3.  0.  6.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6] -> size -> 18 
action values: 0 
buys: 1 
player value: 5 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9. 10.  9.  4. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 0. 25.  0.  0. 10.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1] -> size -> 16 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -65 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -2.0987043380737305



buy possibilites: [-1] 
expected returns: [[5.0297766]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 0. 0. 0. 3.] 
cards in discard: [29. 29.  3.  0.  0.  3.  0.  6. 29.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29] -> size -> 19 
action values: 0 
buys: 0 
player value: 1 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9. 10.  9.  3. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 0. 25.  0.  0. 10.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1] -> size -> 16 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5.   0.   0. -60.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
  32.   0.] 
sum of rewards: -33.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 22.31916046142578






         -------------------- Turn: 9 -------------------- 
Player: 1 
cards in hand: [ 0. 25.  0.  0. 10.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 10.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 25.  0.  0. 10.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1] -> size -> 16 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9. 10.  9.  3. 10. 10.  9. 10. 10.] 
adversary cards in hand: [29. 29. 29.  0. 29.] 
adversary cards in discard: [29. 29.  3.  0.  0.  3.  0.  6. 29.  1.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29] -> size -> 19 
adversary victory points: 2
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 25.  0.  0. 10.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1] -> size -> 16 
action values: 0 
buys: 1 
player value: 3 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9. 10.  9.  3. 10. 10.  9. 10. 10.] 
adversary cards in hand: [29. 29. 29.  0. 29.] 
adversary cards in discard: [29. 29.  3.  0.  0.  3.  0.  6. 29.  1.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29] -> size -> 19 
adversary victory points: 2
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 25.  0.  0. 10.] 
cards in discard: [8.] 
cards in deck: 11 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8] -> size -> 17 
action values: 0 
buys: 0 
player value: 1 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  9.  9.  3. 10. 10.  9. 10. 10.] 
adversary cards in hand: [29. 29. 29.  0. 29.] 
adversary cards in discard: [29. 29.  3.  0.  0.  3.  0.  6. 29.  1.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29] -> size -> 19 
adversary victory points: 2
player victory points: 4 





Player: 0 
cards in hand: [29. 29. 29.  0. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29. 29.] 
expected returns: [[31.059208]
 [51.060856]
 [51.060856]
 [51.060856]
 [51.060856]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29. 29.  0. 29.] 
cards in discard: [29. 29.  3.  0.  0.  3.  0.  6. 29.  1.  0.  0.  0.  3.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29] -> size -> 19 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  9.  9.  3. 10. 10.  9. 10. 10.] 
adversary cards in hand: [1. 0. 3. 0. 0.] 
adversary cards in discard: [ 8.  0. 25.  0.  0. 10.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8] -> size -> 17 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -65 

action type: buy - action -1
Learning step: 0
desired expected reward: 5.029776573181152



action possibilites: [-1. 29. 29. 29. 29.] 
expected returns: [[12.967339]
 [33.299942]
 [33.299942]
 [33.299942]
 [33.299942]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29.  0. 29. 29.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29] -> size -> 19 
action values: 1 
buys: 0 
player value: 1 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  9.  9.  3. 10. 10.  9. 10. 10.] 
adversary cards in hand: [1. 0. 3. 0. 0.] 
adversary cards in discard: [ 8.  0. 25.  0.  0. 10.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8] -> size -> 17 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -45 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 47.0848388671875



action possibilites: [-1. 29. 29. 29.] 
expected returns: [[-14.582032]
 [  5.851305]
 [  5.851305]
 [  5.851305]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0. 29. 29.  3.] 
cards in discard: [] 
cards in deck: 12 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29] -> size -> 19 
action values: 1 
buys: 0 
player value: 2 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  9.  9.  3. 10. 10.  9. 10. 10.] 
adversary cards in hand: [1. 0. 3. 0. 0.] 
adversary cards in discard: [ 8.  0. 25.  0.  0. 10.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8] -> size -> 17 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -25 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 33.29994583129883



action possibilites: [-1. 29. 29.] 
expected returns: [[-6.132326]
 [14.842987]
 [14.842987]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29. 29.  3.  6.] 
cards in discard: [] 
cards in deck: 11 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29] -> size -> 19 
action values: 1 
buys: 0 
player value: 3 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  9.  9.  3. 10. 10.  9. 10. 10.] 
adversary cards in hand: [1. 0. 3. 0. 0.] 
adversary cards in discard: [ 8.  0. 25.  0.  0. 10.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8] -> size -> 17 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: -5 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 5.8513031005859375



action possibilites: [-1. 29.] 
expected returns: [[14.324839]
 [35.5525  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29.  3.  6.  0.] 
cards in discard: [] 
cards in deck: 10 
card top of deck: [] 
played cards: [29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29] -> size -> 19 
action values: 1 
buys: 0 
player value: 4 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  9.  9.  3. 10. 10.  9. 10. 10.] 
adversary cards in hand: [1. 0. 3. 0. 0.] 
adversary cards in discard: [ 8.  0. 25.  0.  0. 10.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8] -> size -> 17 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  80   0   0   0   0   0   0   0   0   0] 
sum of rewards: 15 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 14.84298324584961



action possibilites: [-1.] 
expected returns: [[13.817106]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 3. 6. 0. 0.] 
cards in discard: [] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29] -> size -> 19 
action values: 1 
buys: 0 
player value: 5 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  9.  9.  3. 10. 10.  9. 10. 10.] 
adversary cards in hand: [1. 0. 3. 0. 0.] 
adversary cards in discard: [ 8.  0. 25.  0.  0. 10.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8] -> size -> 17 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0 100   0   0   0   0   0   0   0   0   0] 
sum of rewards: 35 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 35.552494049072266





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  5.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 16.84101 ]
 [ 30.47203 ]
 [-15.099826]
 [ 21.290754]
 [-18.49688 ]
 [ 22.242077]
 [-62.53868 ]
 [ 26.07835 ]
 [ 27.113451]
 [ 16.730413]
 [ 31.6636  ]
 [ 37.110737]
 [ 26.491207]
 [ 31.233837]
 [ 24.390686]
 [ 25.677029]
 [ 30.098999]
 [ 15.60203 ]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 6. 0. 0.] 
cards in discard: [] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29] -> size -> 19 
action values: 0 
buys: 1 
player value: 8 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  9.  9.  3. 10. 10.  9. 10. 10.] 
adversary cards in hand: [1. 0. 3. 0. 0.] 
adversary cards in discard: [ 8.  0. 25.  0.  0. 10.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8] -> size -> 17 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0 100   0   0   0   0   0   0   0   0   0] 
sum of rewards: 35 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 13.817106246948242



buy possibilites: [-1] 
expected returns: [[26.044672]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 3. 6. 0. 0.] 
cards in discard: [29.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 29. 29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29] -> size -> 20 
action values: 0 
buys: 0 
player value: 4 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  9.  9.  2. 10. 10.  9. 10. 10.] 
adversary cards in hand: [1. 0. 3. 0. 0.] 
adversary cards in discard: [ 8.  0. 25.  0.  0. 10.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8] -> size -> 17 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5.   0.   0. -60.   0.   0. 100.   0.   0.   0.   0.   0.   0.   0.
  32.   0.] 
sum of rewards: 67.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 37.110740661621094






         -------------------- Turn: 10 -------------------- 
Player: 1 
cards in hand: [1. 0. 3. 0. 0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 0. 3. 0. 0.] 
cards in discard: [ 8.  0. 25.  0.  0. 10.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8] -> size -> 17 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  9.  9.  2. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 3.  0.  0.  0. 29.] 
adversary cards in discard: [29. 29. 29. 29. 29. 29.  0.  3.  6.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29] -> size -> 20 
adversary victory points: 2
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 0. 3. 0. 0.] 
cards in discard: [ 8.  0. 25.  0.  0. 10.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8] -> size -> 17 
action values: 0 
buys: 1 
player value: 5 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  9.  9.  2. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 3.  0.  0.  0. 29.] 
adversary cards in discard: [29. 29. 29. 29. 29. 29.  0.  3.  6.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29] -> size -> 20 
adversary victory points: 2
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 0. 3. 0. 0.] 
cards in discard: [ 8.  0. 25.  0.  0. 10.  8.] 
cards in deck: 6 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8] -> size -> 18 
action values: 0 
buys: 0 
player value: 3 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  8.  9.  2. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 3.  0.  0.  0. 29.] 
adversary cards in discard: [29. 29. 29. 29. 29. 29.  0.  3.  6.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29] -> size -> 20 
adversary victory points: 2
player victory points: 4 





Player: 0 
cards in hand: [ 3.  0.  0.  0. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[59.21297 ]
 [79.700554]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0.  0.  0. 29.] 
cards in discard: [29. 29. 29. 29. 29. 29.  0.  3.  6.  0.  0.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29] -> size -> 20 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  8.  9.  2. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 3.  0. 11.  0.  3.] 
adversary cards in discard: [ 8.  0. 25.  0.  0. 10.  8.  1.  0.  3.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8] -> size -> 18 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -65 

action type: buy - action -1
Learning step: 0
desired expected reward: 26.0446720123291



action possibilites: [-1.] 
expected returns: [[52.95299]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 0. 1.] 
cards in discard: [29. 29. 29. 29. 29. 29.  0.  3.  6.  0.  0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29] -> size -> 20 
action values: 1 
buys: 0 
player value: 1 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  8.  9.  2. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 3.  0. 11.  0.  3.] 
adversary cards in discard: [ 8.  0. 25.  0.  0. 10.  8.  1.  0.  3.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8] -> size -> 18 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -45 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 74.75821685791016





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 60.94372 ]
 [ 74.469215]
 [ 27.437817]
 [ 65.359276]
 [ 22.88168 ]
 [-19.074093]
 [ 70.40402 ]
 [ 71.3915  ]
 [ 61.178684]
 [ 75.59765 ]
 [ 80.69325 ]
 [ 70.81112 ]
 [ 75.19533 ]
 [ 68.75794 ]
 [ 70.04857 ]
 [ 74.16063 ]
 [ 60.798023]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 1.] 
cards in discard: [29. 29. 29. 29. 29. 29.  0.  3.  6.  0.  0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29] -> size -> 20 
action values: 0 
buys: 1 
player value: 6 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  8.  9.  2. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 3.  0. 11.  0.  3.] 
adversary cards in discard: [ 8.  0. 25.  0.  0. 10.  8.  1.  0.  3.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8] -> size -> 18 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -45 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 52.9529914855957



buy possibilites: [-1] 
expected returns: [[59.389553]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 1.] 
cards in discard: [29. 29. 29. 29. 29. 29.  0.  3.  6.  0.  0. 29.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29] -> size -> 21 
action values: 0 
buys: 0 
player value: 2 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  8.  9.  1. 10. 10.  9. 10. 10.] 
adversary cards in hand: [ 3.  0. 11.  0.  3.] 
adversary cards in discard: [ 8.  0. 25.  0.  0. 10.  8.  1.  0.  3.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8] -> size -> 18 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5.   0.   0. -60.   0.   0.  20.   0.   0.   0.   0.   0.   0.   0.
  32.   0.] 
sum of rewards: -13.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 80.6932373046875






         -------------------- Turn: 11 -------------------- 
Player: 1 
cards in hand: [ 3.  0. 11.  0.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 11.  0.  3.] 
cards in discard: [ 8.  0. 25.  0.  0. 10.  8.  1.  0.  3.  0.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8] -> size -> 18 
action values: 1 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  8.  9.  1. 10. 10.  9. 10. 10.] 
adversary cards in hand: [29.  1. 29.  3.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29] -> size -> 21 
adversary victory points: 2
player victory points: 4 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 3.] 
cards in discard: [ 8.  0. 25.  0.  0. 10.  8.  1.  0.  3.  0.  0. 15.] 
cards in deck: 1 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15] -> size -> 19 
action values: 0 
buys: 0 
player value: 0 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  8.  9.  1. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29.  1. 29.  3.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29] -> size -> 21 
adversary victory points: 2
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 3.] 
cards in discard: [ 8.  0. 25.  0.  0. 10.  8.  1.  0.  3.  0.  0. 15.] 
cards in deck: 1 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15] -> size -> 19 
action values: 0 
buys: 1 
player value: 2 
card supply: [29. 28. 30. 29. 30.  8.  9. 10.  9.  8.  9.  1. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29.  1. 29.  3.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29] -> size -> 21 
adversary victory points: 2
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 3.] 
cards in discard: [ 8.  0. 25.  0.  0. 10.  8.  1.  0.  3.  0.  0. 15.  0.] 
cards in deck: 1 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0] -> size -> 20 
action values: 0 
buys: 0 
player value: 2 
card supply: [28. 28. 30. 29. 30.  8.  9. 10.  9.  8.  9.  1. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29.  1. 29.  3.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29] -> size -> 21 
adversary victory points: 2
player victory points: 4 





Player: 0 
cards in hand: [29.  1. 29.  3.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[-11.877153]
 [  8.210431]
 [  8.210431]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  1. 29.  3.  0.] 
cards in discard: [] 
cards in deck: 16 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29] -> size -> 21 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 28. 30. 29. 30.  8.  9. 10.  9.  8.  9.  1. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0. 25.  3.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0] -> size -> 20 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -65 

action type: buy - action -1
Learning step: 0
desired expected reward: 59.38955307006836



action possibilites: [-1. 29.] 
expected returns: [[-5.7926044]
 [13.49548  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 29.  3.  0.  0.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29] -> size -> 21 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 28. 30. 29. 30.  8.  9. 10.  9.  8.  9.  1. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0. 25.  3.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0] -> size -> 20 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -45 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 5.448572158813477



action possibilites: [-1. 29.] 
expected returns: [[11.879479]
 [31.513397]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  3.  0.  0. 29.] 
cards in discard: [] 
cards in deck: 14 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29] -> size -> 21 
action values: 1 
buys: 0 
player value: 2 
card supply: [28. 28. 30. 29. 30.  8.  9. 10.  9.  8.  9.  1. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0. 25.  3.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0] -> size -> 20 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -25 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 13.4954833984375



action possibilites: [-1.] 
expected returns: [[-0.13206291]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29] -> size -> 21 
action values: 1 
buys: 0 
player value: 3 
card supply: [28. 28. 30. 29. 30.  8.  9. 10.  9.  8.  9.  1. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0. 25.  3.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0] -> size -> 20 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: -5 

action type: take_action - action 29.0
Learning step: 0
desired expected reward: 31.51340103149414





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  5.  6. 16. 11.  8. 25. 29. 14. 23. 10. 22. 15. -1.] 
expected returns: [[  1.5903571 ]
 [ 15.153742  ]
 [-29.89754   ]
 [  6.0650177 ]
 [-33.311356  ]
 [  6.903804  ]
 [-90.66459   ]
 [ 10.826554  ]
 [ 11.849951  ]
 [  1.4292457 ]
 [ 16.334673  ]
 [ 21.698866  ]
 [ 11.236804  ]
 [ 15.910065  ]
 [  9.133728  ]
 [ 10.435367  ]
 [ 14.795975  ]
 [  0.48752093]]
Chosen buy action: 29.0 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 3. 0. 0. 0.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29] -> size -> 21 
action values: 0 
buys: 1 
player value: 8 
card supply: [28. 28. 30. 29. 30.  8.  9. 10.  9.  8.  9.  1. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0. 25.  3.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0] -> size -> 20 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -0.1320629119873047



buy possibilites: [-1] 
expected returns: [[19.1865]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 3. 0. 0. 0.] 
cards in discard: [29.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29] -> size -> 22 
action values: 0 
buys: 0 
player value: 4 
card supply: [28. 28. 30. 29. 30.  8.  9. 10.  9.  8.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0. 25.  3.  0.  3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0] -> size -> 20 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5.   0.   0. -60.   0.   0.  60.   0.   0.   0.   0.   0.   0.   0.
  32.   0.] 
sum of rewards: 27.0 

action type: buy - action 29.0
Learning step: 0
desired expected reward: 21.6988582611084






         -------------------- Turn: 12 -------------------- 
Player: 1 
cards in hand: [ 0. 25.  3.  0.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 25.  3.  0.  3.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0] -> size -> 20 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 28. 30. 29. 30.  8.  9. 10.  9.  8.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29. 29.  3.  0. 29.] 
adversary cards in discard: [29. 29. 29. 29.  1.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29] -> size -> 22 
adversary victory points: 2
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 25.  3.  0.  3.] 
cards in discard: [] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0] -> size -> 20 
action values: 0 
buys: 1 
player value: 2 
card supply: [28. 28. 30. 29. 30.  8.  9. 10.  9.  8.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29. 29.  3.  0. 29.] 
adversary cards in discard: [29. 29. 29. 29.  1.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29] -> size -> 22 
adversary victory points: 2
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 25.  3.  0.  3.] 
cards in discard: [8.] 
cards in deck: 15 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8] -> size -> 21 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 28. 30. 29. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29. 29.  3.  0. 29.] 
adversary cards in discard: [29. 29. 29. 29.  1.  3.  0.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29] -> size -> 22 
adversary victory points: 2
player victory points: 4 





Player: 0 
cards in hand: [29. 29.  3.  0. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29.] 
expected returns: [[63.95164]
 [85.42088]
 [85.42088]
 [85.42088]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29.  3.  0. 29.] 
cards in discard: [29. 29. 29. 29.  1.  3.  0.  0.  0.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 28. 30. 29. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [15.  0.  0. 11.  3.] 
adversary cards in discard: [ 8.  0. 25.  3.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8] -> size -> 21 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -65 

action type: buy - action -1
Learning step: 0
desired expected reward: 19.186500549316406



action possibilites: [-1. 29. 29.] 
expected returns: [[72.6129 ]
 [93.59418]
 [93.59418]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 29. 29.] 
cards in discard: [29. 29. 29. 29.  1.  3.  0.  0.  0. 29.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 28. 30. 29. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [15.  0.  0. 11.  3.] 
adversary cards in discard: [ 8.  0. 25.  3.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8] -> size -> 21 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 75.85050201416016



action possibilites: [-1. 29.] 
expected returns: [[64.473145]
 [85.45442 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 29.] 
cards in discard: [29. 29. 29. 29.  1.  3.  0.  0.  0. 29. 29.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 2 
card supply: [28. 28. 30. 29. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [15.  0.  0. 11.  3.] 
adversary cards in discard: [ 8.  0. 25.  3.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8] -> size -> 21 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -25 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 87.07496643066406



action possibilites: [-1.] 
expected returns: [[86.780876]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0.] 
cards in discard: [29. 29. 29. 29.  1.  3.  0.  0.  0. 29. 29.  6.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29] -> size -> 22 
action values: 1 
buys: 0 
player value: 3 
card supply: [28. 28. 30. 29. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [15.  0.  0. 11.  3.] 
adversary cards in discard: [ 8.  0. 25.  3.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8] -> size -> 21 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: -5 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 78.95091247558594





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[87.177536 ]
 [99.95921  ]
 [91.57939  ]
 [10.9116745]
 [96.29597  ]
 [97.26247  ]
 [87.267914 ]
 [96.72669  ]
 [94.72978  ]
 [99.762054 ]
 [87.33357  ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0.] 
cards in discard: [29. 29. 29. 29.  1.  3.  0.  0.  0. 29. 29.  6.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29] -> size -> 22 
action values: 0 
buys: 1 
player value: 4 
card supply: [28. 28. 30. 29. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [15.  0.  0. 11.  3.] 
adversary cards in discard: [ 8.  0. 25.  3.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8] -> size -> 21 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: -5 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 86.78087615966797



buy possibilites: [-1] 
expected returns: [[150.33586]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0.] 
cards in discard: [29. 29. 29. 29.  1.  3.  0.  0.  0. 29. 29.  6.  1.] 
cards in deck: 5 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1] -> size -> 23 
action values: 0 
buys: 0 
player value: 1 
card supply: [28. 27. 30. 29. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [15.  0.  0. 11.  3.] 
adversary cards in discard: [ 8.  0. 25.  3.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8] -> size -> 21 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5.    0.    0.  -60.    0.    0.   60.    0.    0.    0.    0.    0.
   0.    0.   13.5   0. ] 
sum of rewards: 8.5 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 99.95919799804688






         -------------------- Turn: 13 -------------------- 
Player: 1 
cards in hand: [15.  0.  0. 11.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 11.] 
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0.  0. 11.  3.] 
cards in discard: [ 8.  0. 25.  3.  0.  3.] 
cards in deck: 10 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8] -> size -> 21 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 27. 30. 29. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29.  0.  0.  0.  3.] 
adversary cards in discard: [29. 29. 29. 29.  1.  3.  0.  0.  0. 29. 29.  6.  1. 29. 29. 29.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1] -> size -> 23 
adversary victory points: 2
player victory points: 4 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 11.  3.] 
cards in discard: [ 8.  0. 25.  3.  0.  3.] 
cards in deck: 10 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8] -> size -> 20 
action values: 0 
buys: 0 
player value: 3 
card supply: [28. 27. 30. 29. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29.  0.  0.  0.  3.] 
adversary cards in discard: [29. 29. 29. 29.  1.  3.  0.  0.  0. 29. 29.  6.  1. 29. 29. 29.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1] -> size -> 23 
adversary victory points: 2
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 11.  3.] 
cards in discard: [ 8.  0. 25.  3.  0.  3.] 
cards in deck: 10 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8] -> size -> 20 
action values: 0 
buys: 1 
player value: 4 
card supply: [28. 27. 30. 29. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29.  0.  0.  0.  3.] 
adversary cards in discard: [29. 29. 29. 29.  1.  3.  0.  0.  0. 29. 29.  6.  1. 29. 29. 29.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1] -> size -> 23 
adversary victory points: 2
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0. 11.  3.] 
cards in discard: [ 8.  0. 25.  3.  0.  3.  3.] 
cards in deck: 10 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8  3] -> size -> 21 
action values: 0 
buys: 0 
player value: 2 
card supply: [28. 27. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29.  0.  0.  0.  3.] 
adversary cards in discard: [29. 29. 29. 29.  1.  3.  0.  0.  0. 29. 29.  6.  1. 29. 29. 29.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1] -> size -> 23 
adversary victory points: 2
player victory points: 5 





Player: 0 
cards in hand: [29.  0.  0.  0.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[-5.9181294]
 [ 9.893497 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0.  0.  0.  3.] 
cards in discard: [29. 29. 29. 29.  1.  3.  0.  0.  0. 29. 29.  6.  1. 29. 29. 29.  3.  0.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1] -> size -> 23 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 27. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 3.  0.  0.  1. 10.] 
adversary cards in discard: [ 8.  0. 25.  3.  0.  3.  3. 15.  0. 11.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8  3] -> size -> 21 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -95 

action type: buy - action -1
Learning step: 0
desired expected reward: 150.3358612060547



action possibilites: [-1.] 
expected returns: [[-1.3261049]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 0. 3.] 
cards in discard: [1.] 
cards in deck: 17 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1] -> size -> 23 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 27. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 3.  0.  0.  1. 10.] 
adversary cards in discard: [ 8.  0. 25.  3.  0.  3.  3. 15.  0. 11.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8  3] -> size -> 21 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -75 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 5.178647041320801





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[  -0.70969915]
 [  12.924471  ]
 [   3.9823053 ]
 [-102.46502   ]
 [   8.753472  ]
 [   9.461688  ]
 [  -1.5259559 ]
 [   8.917139  ]
 [   6.738982  ]
 [  12.285872  ]
 [  -0.6294513 ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3.] 
cards in discard: [1.] 
cards in deck: 17 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1] -> size -> 23 
action values: 0 
buys: 1 
player value: 4 
card supply: [28. 27. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 3.  0.  0.  1. 10.] 
adversary cards in discard: [ 8.  0. 25.  3.  0.  3.  3. 15.  0. 11.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8  3] -> size -> 21 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -75 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -1.3261048793792725



buy possibilites: [-1] 
expected returns: [[9.444647]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 0. 3.] 
cards in discard: [1. 1.] 
cards in deck: 17 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1] -> size -> 24 
action values: 0 
buys: 0 
player value: 1 
card supply: [28. 26. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 3.  0.  0.  1. 10.] 
adversary cards in discard: [ 8.  0. 25.  3.  0.  3.  3. 15.  0. 11.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8  3] -> size -> 21 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5.    0.    0.  -90.    0.    0.   20.    0.    0.    0.    0.    0.
   0.    0.   13.5   0. ] 
sum of rewards: -61.5 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 12.924467086791992






         -------------------- Turn: 14 -------------------- 
Player: 1 
cards in hand: [ 3.  0.  0.  1. 10.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 10.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0.  0.  1. 10.] 
cards in discard: [ 8.  0. 25.  3.  0.  3.  3. 15.  0. 11.  3.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8  3] -> size -> 21 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 26. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0. 29.  0.  6. 29.] 
adversary cards in discard: [ 1.  1. 29.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1] -> size -> 24 
adversary victory points: 2
player victory points: 5 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0.  0.  1. 10.] 
cards in discard: [ 8.  0. 25.  3.  0.  3.  3. 15.  0. 11.  3.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8  3] -> size -> 21 
action values: 0 
buys: 1 
player value: 4 
card supply: [28. 26. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0. 29.  0.  6. 29.] 
adversary cards in discard: [ 1.  1. 29.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1] -> size -> 24 
adversary victory points: 2
player victory points: 5 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0.  0.  1. 10.] 
cards in discard: [ 8.  0. 25.  3.  0.  3.  3. 15.  0. 11.  3.  1.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8  3  1] -> size -> 22 
action values: 0 
buys: 0 
player value: 1 
card supply: [28. 25. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0. 29.  0.  6. 29.] 
adversary cards in discard: [ 1.  1. 29.  0.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1] -> size -> 24 
adversary victory points: 2
player victory points: 5 





Player: 0 
cards in hand: [ 0. 29.  0.  6. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[ 6.1860943]
 [27.85134  ]
 [27.85134  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29.  0.  6. 29.] 
cards in discard: [ 1.  1. 29.  0.  0.  0.  3.] 
cards in deck: 12 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1] -> size -> 24 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 25. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [8. 0. 0. 0. 8.] 
adversary cards in discard: [ 8.  0. 25.  3.  0.  3.  3. 15.  0. 11.  3.  1.  3.  0.  0.  1. 10.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8  3  1] -> size -> 22 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -95 

action type: buy - action -1
Learning step: 0
desired expected reward: 9.444646835327148



action possibilites: [-1. 29.] 
expected returns: [[ 7.4343805]
 [29.809803 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  6. 29.] 
cards in discard: [ 1.  1. 29.  0.  0.  0.  3. 29.] 
cards in deck: 11 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1] -> size -> 24 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 25. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [8. 0. 0. 0. 8.] 
adversary cards in discard: [ 8.  0. 25.  3.  0.  3.  3. 15.  0. 11.  3.  1.  3.  0.  0.  1. 10.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8  3  1] -> size -> 22 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -75 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 18.29759407043457



action possibilites: [-1.] 
expected returns: [[-7.899724]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 6.] 
cards in discard: [ 1.  1. 29.  0.  0.  0.  3. 29. 29.] 
cards in deck: 10 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1] -> size -> 24 
action values: 1 
buys: 0 
player value: 2 
card supply: [28. 25. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [8. 0. 0. 0. 8.] 
adversary cards in discard: [ 8.  0. 25.  3.  0.  3.  3. 15.  0. 11.  3.  1.  3.  0.  0.  1. 10.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8  3  1] -> size -> 22 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -55 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 22.983043670654297





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[  -4.9012365]
 [   8.166595 ]
 [  -0.1193707]
 [-102.8206   ]
 [   4.6998787]
 [   5.709957 ]
 [  -4.9165864]
 [   5.1869783]
 [   3.0818164]
 [   8.08077  ]
 [  -3.9816453]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 6.] 
cards in discard: [ 1.  1. 29.  0.  0.  0.  3. 29. 29.] 
cards in deck: 10 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1] -> size -> 24 
action values: 0 
buys: 1 
player value: 4 
card supply: [28. 25. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [8. 0. 0. 0. 8.] 
adversary cards in discard: [ 8.  0. 25.  3.  0.  3.  3. 15.  0. 11.  3.  1.  3.  0.  0.  1. 10.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8  3  1] -> size -> 22 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -55 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -7.899724006652832



buy possibilites: [-1] 
expected returns: [[28.302975]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 6.] 
cards in discard: [ 1.  1. 29.  0.  0.  0.  3. 29. 29.  1.] 
cards in deck: 10 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1] -> size -> 25 
action values: 0 
buys: 0 
player value: 1 
card supply: [28. 24. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [8. 0. 0. 0. 8.] 
adversary cards in discard: [ 8.  0. 25.  3.  0.  3.  3. 15.  0. 11.  3.  1.  3.  0.  0.  1. 10.] 
adversary owned cards: [ 0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8  3  1] -> size -> 22 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5.    0.    0.  -90.    0.    0.   40.    0.    0.    0.    0.    0.
   0.    0.   13.5   0. ] 
sum of rewards: -41.5 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 8.166604995727539






         -------------------- Turn: 15 -------------------- 
Player: 1 
cards in hand: [8. 0. 0. 0. 8.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [8. 0. 0. 0. 8.] 
cards in discard: [ 8.  0. 25.  3.  0.  3.  3. 15.  0. 11.  3.  1.  3.  0.  0.  1. 10.] 
cards in deck: 0 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  3  3  3 10  3  0 25 11  1  8  8 15  0  8  3  1] -> size -> 22 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 24. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29.  3. 29.  1.  3.] 
adversary cards in discard: [ 1.  1. 29.  0.  0.  0.  3. 29. 29.  1. 29. 29.  0.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1] -> size -> 25 
adversary victory points: 2
player victory points: 5 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [] 
cards in discard: [ 8.  0. 25.  3.  0.  3.  3. 15.  0. 11.  3.  1.  3.  0.  0.  1. 10.] 
cards in deck: 0 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  0  3  3  3 10  3  0 25 11  1  8 15  0  8  3  1] -> size -> 18 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 24. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29.  3. 29.  1.  3.] 
adversary cards in discard: [ 1.  1. 29.  0.  0.  0.  3. 29. 29.  1. 29. 29.  0.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1] -> size -> 25 
adversary victory points: 2
player victory points: 5 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [] 
cards in discard: [ 8.  0. 25.  3.  0.  3.  3. 15.  0. 11.  3.  1.  3.  0.  0.  1. 10.] 
cards in deck: 0 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  0  3  3  3 10  3  0 25 11  1  8 15  0  8  3  1] -> size -> 18 
action values: 0 
buys: 1 
player value: 0 
card supply: [28. 24. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29.  3. 29.  1.  3.] 
adversary cards in discard: [ 1.  1. 29.  0.  0.  0.  3. 29. 29.  1. 29. 29.  0.  0.  6.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1] -> size -> 25 
adversary victory points: 2
player victory points: 5 





Player: 0 
cards in hand: [29.  3. 29.  1.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[108.38433]
 [125.04266]
 [125.04266]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  3. 29.  1.  3.] 
cards in discard: [ 1.  1. 29.  0.  0.  0.  3. 29. 29.  1. 29. 29.  0.  0.  6.] 
cards in deck: 5 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1] -> size -> 25 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 24. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [8. 0. 1. 3. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  3 10  3  0 25 11  1  8 15  0  8  3  1] -> size -> 18 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -95 

action type: buy - action -1
Learning step: 0
desired expected reward: 28.302974700927734



action possibilites: [-1. 29. 29.] 
expected returns: [[102.80862]
 [120.33321]
 [120.33321]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 29.  3. 29.] 
cards in discard: [ 1.  1. 29.  0.  0.  0.  3. 29. 29.  1. 29. 29.  0.  0.  6.  1.] 
cards in deck: 4 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1] -> size -> 25 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 24. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [8. 0. 1. 3. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  3 10  3  0 25 11  1  8 15  0  8  3  1] -> size -> 18 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -75 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 119.13414001464844



action possibilites: [-1.] 
expected returns: [[1.8242161]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 3. 0.] 
cards in discard: [ 1.  1. 29.  0.  0.  0.  3. 29. 29.  1. 29. 29.  0.  0.  6.  1. 29.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1] -> size -> 25 
action values: 1 
buys: 0 
player value: 2 
card supply: [28. 24. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [8. 0. 1. 3. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  3 10  3  0 25 11  1  8 15  0  8  3  1] -> size -> 18 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -55 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 115.04669189453125





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[  1.2362444]
 [ 14.131048 ]
 [  6.095686 ]
 [-94.789925 ]
 [ 11.787532 ]
 [  1.0934036]
 [  9.120217 ]
 [  1.7339103]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 0.] 
cards in discard: [ 1.  1. 29.  0.  0.  0.  3. 29. 29.  1. 29. 29.  0.  0.  6.  1. 29.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1] -> size -> 25 
action values: 0 
buys: 1 
player value: 3 
card supply: [28. 24. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [8. 0. 1. 3. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  3 10  3  0 25 11  1  8 15  0  8  3  1] -> size -> 18 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -55 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 1.8242161273956299



buy possibilites: [-1] 
expected returns: [[19.423592]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3. 0.] 
cards in discard: [ 1.  1. 29.  0.  0.  0.  3. 29. 29.  1. 29. 29.  0.  0.  6.  1. 29.  1.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1] -> size -> 26 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 23. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [8. 0. 1. 3. 3.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  3  3  3 10  3  0 25 11  1  8 15  0  8  3  1] -> size -> 18 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0  40   0   0   0   0   0   0   0  54   0] 
sum of rewards: -1 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 14.131038665771484






         -------------------- Turn: 16 -------------------- 
Player: 1 
cards in hand: [8. 0. 1. 3. 3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [8. 0. 1. 3. 3.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  3  3  3 10  3  0 25 11  1  8 15  0  8  3  1] -> size -> 18 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 23. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29.  0. 29. 29.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1] -> size -> 26 
adversary victory points: 2
player victory points: 5 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 3.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1] -> size -> 16 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 23. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29.  0. 29. 29.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1] -> size -> 26 
adversary victory points: 2
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 3.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1] -> size -> 16 
action values: 0 
buys: 1 
player value: 2 
card supply: [28. 23. 30. 28. 30.  8.  9. 10.  9.  7.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29.  0. 29. 29.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1] -> size -> 26 
adversary victory points: 2
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 3.] 
cards in discard: [8.] 
cards in deck: 13 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8] -> size -> 17 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 23. 30. 28. 30.  8.  9. 10.  9.  6.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29.  0. 29. 29.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1] -> size -> 26 
adversary victory points: 2
player victory points: 4 





Player: 0 
cards in hand: [29.  0. 29. 29.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29.] 
expected returns: [[-5.6404314]
 [14.370493 ]
 [14.370493 ]
 [14.370493 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29.  0. 29. 29.  0.] 
cards in discard: [] 
cards in deck: 21 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1] -> size -> 26 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 23. 30. 28. 30.  8.  9. 10.  9.  6.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [11.  0. 15. 25.  3.] 
adversary cards in discard: [8. 8. 1. 3.] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8] -> size -> 17 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -65 

action type: buy - action -1
Learning step: 0
desired expected reward: 19.42359161376953



action possibilites: [-1. 29.] 
expected returns: [[16.725357]
 [38.201588]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29.  0.  3.] 
cards in discard: [29.] 
cards in deck: 20 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1] -> size -> 26 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 23. 30. 28. 30.  8.  9. 10.  9.  6.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [11.  0. 15. 25.  3.] 
adversary cards in discard: [8. 8. 1. 3.] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8] -> size -> 17 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 3.5986907482147217



action possibilites: [-1.] 
expected returns: [[23.490211]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3.] 
cards in discard: [29.  1.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1] -> size -> 26 
action values: 1 
buys: 0 
player value: 2 
card supply: [28. 23. 30. 28. 30.  8.  9. 10.  9.  6.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [11.  0. 15. 25.  3.] 
adversary cards in discard: [8. 8. 1. 3.] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8] -> size -> 17 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -25 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 30.03421401977539





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[ 24.752317]
 [ 38.197666]
 [ 29.394505]
 [-48.121983]
 [ 33.957302]
 [ 34.870785]
 [ 24.184584]
 [ 34.302956]
 [ 32.17124 ]
 [ 37.818066]
 [ 24.476627]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3.] 
cards in discard: [29.  1.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1] -> size -> 26 
action values: 0 
buys: 1 
player value: 4 
card supply: [28. 23. 30. 28. 30.  8.  9. 10.  9.  6.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [11.  0. 15. 25.  3.] 
adversary cards in discard: [8. 8. 1. 3.] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8] -> size -> 17 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -25 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 23.490211486816406



buy possibilites: [-1] 
expected returns: [[17.521145]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3.] 
cards in discard: [29.  1.  1.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1] -> size -> 27 
action values: 0 
buys: 0 
player value: 1 
card supply: [28. 22. 30. 28. 30.  8.  9. 10.  9.  6.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [11.  0. 15. 25.  3.] 
adversary cards in discard: [8. 8. 1. 3.] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8] -> size -> 17 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5.    0.    0.  -60.    0.    0.   40.    0.    0.    0.    0.    0.
   0.    0.   13.5   0. ] 
sum of rewards: -11.5 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 38.197662353515625






         -------------------- Turn: 17 -------------------- 
Player: 1 
cards in hand: [11.  0. 15. 25.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 11. 15. 25.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [11.  0. 15. 25.  3.] 
cards in discard: [8. 8. 1. 3.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8] -> size -> 17 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 22. 30. 28. 30.  8.  9. 10.  9.  6.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29. 29.  0.  0.  1.] 
adversary cards in discard: [29.  1.  1. 29. 29.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1] -> size -> 27 
adversary victory points: 2
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [11.  0. 15. 25.  3.] 
cards in discard: [8. 8. 1. 3.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8] -> size -> 17 
action values: 0 
buys: 1 
player value: 1 
card supply: [28. 22. 30. 28. 30.  8.  9. 10.  9.  6.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [29. 29.  0.  0.  1.] 
adversary cards in discard: [29.  1.  1. 29. 29.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1] -> size -> 27 
adversary victory points: 2
player victory points: 4 


 --------- NOTHING HAPPENED --------- 



Player: 0 
cards in hand: [29. 29.  0.  0.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[ 5.267332]
 [22.944359]
 [22.944359]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29.  0.  0.  1.] 
cards in discard: [29.  1.  1. 29. 29.  0.  0.  3.] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1] -> size -> 27 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 22. 30. 28. 30.  8.  9. 10.  9.  6.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 3.  0.  0. 10.  3.] 
adversary cards in discard: [ 8.  8.  1.  3. 11.  0. 15. 25.  3.] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8] -> size -> 17 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -65 

action type: buy - action -1
Learning step: 0
desired expected reward: 17.52114486694336



action possibilites: [-1.] 
expected returns: [[52.617245]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 1. 3.] 
cards in discard: [29.  1.  1. 29. 29.  0.  0.  3. 29.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1] -> size -> 27 
action values: 1 
buys: 0 
player value: 1 
card supply: [28. 22. 30. 28. 30.  8.  9. 10.  9.  6.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 3.  0.  0. 10.  3.] 
adversary cards in discard: [ 8.  8.  1.  3. 11.  0. 15. 25.  3.] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8] -> size -> 17 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 16.18673324584961





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 53.64474 ]
 [ 67.43861 ]
 [ 58.011715]
 [ 16.463253]
 [-24.041893]
 [ 63.026943]
 [ 63.978497]
 [ 53.672543]
 [ 68.57559 ]
 [ 63.368725]
 [ 68.15483 ]
 [ 61.287495]
 [ 62.585728]
 [ 66.97692 ]
 [ 52.67843 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 1. 3.] 
cards in discard: [29.  1.  1. 29. 29.  0.  0.  3. 29.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1] -> size -> 27 
action values: 0 
buys: 1 
player value: 5 
card supply: [28. 22. 30. 28. 30.  8.  9. 10.  9.  6.  9.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 3.  0.  0. 10.  3.] 
adversary cards in discard: [ 8.  8.  1.  3. 11.  0. 15. 25.  3.] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8] -> size -> 17 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -45 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 52.617244720458984



buy possibilites: [-1] 
expected returns: [[70.31625]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 1. 3.] 
cards in discard: [29.  1.  1. 29. 29.  0.  0.  3. 29. 25.] 
cards in deck: 13 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25] -> size -> 28 
action values: 0 
buys: 0 
player value: 0 
card supply: [28. 22. 30. 28. 30.  8.  9. 10.  9.  6.  8.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 3.  0.  0. 10.  3.] 
adversary cards in discard: [ 8.  8.  1.  3. 11.  0. 15. 25.  3.] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8] -> size -> 17 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  20   0   0   0   0   0   0   0 250   0] 
sum of rewards: 205 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 68.57559967041016






         -------------------- Turn: 18 -------------------- 
Player: 1 
cards in hand: [ 3.  0.  0. 10.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 10.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0.  0. 10.  3.] 
cards in discard: [ 8.  8.  1.  3. 11.  0. 15. 25.  3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8] -> size -> 17 
action values: 1 
buys: 0 
player value: 0 
card supply: [28. 22. 30. 28. 30.  8.  9. 10.  9.  6.  8.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 1. 29.  0. 29. 29.] 
adversary cards in discard: [29.  1.  1. 29. 29.  0.  0.  3. 29. 25. 29.  0.  0.  1.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25] -> size -> 28 
adversary victory points: 2
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0.  0. 10.  3.] 
cards in discard: [ 8.  8.  1.  3. 11.  0. 15. 25.  3.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8] -> size -> 17 
action values: 0 
buys: 1 
player value: 2 
card supply: [28. 22. 30. 28. 30.  8.  9. 10.  9.  6.  8.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 1. 29.  0. 29. 29.] 
adversary cards in discard: [29.  1.  1. 29. 29.  0.  0.  3. 29. 25. 29.  0.  0.  1.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25] -> size -> 28 
adversary victory points: 2
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0.  0. 10.  3.] 
cards in discard: [ 8.  8.  1.  3. 11.  0. 15. 25.  3.  0.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8  0] -> size -> 18 
action values: 0 
buys: 0 
player value: 2 
card supply: [27. 22. 30. 28. 30.  8.  9. 10.  9.  6.  8.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 1. 29.  0. 29. 29.] 
adversary cards in discard: [29.  1.  1. 29. 29.  0.  0.  3. 29. 25. 29.  0.  0.  1.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25] -> size -> 28 
adversary victory points: 2
player victory points: 4 





Player: 0 
cards in hand: [ 1. 29.  0. 29. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29. 29.] 
expected returns: [[74.462006]
 [89.99504 ]
 [89.99504 ]
 [89.99504 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 29.  0. 29. 29.] 
cards in discard: [29.  1.  1. 29. 29.  0.  0.  3. 29. 25. 29.  0.  0.  1.  3.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25] -> size -> 28 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 22. 30. 28. 30.  8.  9. 10.  9.  6.  8.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [3. 0. 0. 8. 1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8  0] -> size -> 18 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -65 

action type: buy - action -1
Learning step: 0
desired expected reward: 70.31625366210938



action possibilites: [-1. 29. 29.] 
expected returns: [[73.315895]
 [89.59473 ]
 [89.59473 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29. 29.  0.] 
cards in discard: [29.  1.  1. 29. 29.  0.  0.  3. 29. 25. 29.  0.  0.  1.  3.  1.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25] -> size -> 28 
action values: 1 
buys: 0 
player value: 1 
card supply: [27. 22. 30. 28. 30.  8.  9. 10.  9.  6.  8.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [3. 0. 0. 8. 1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8  0] -> size -> 18 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 85.21588134765625



action possibilites: [-1.] 
expected returns: [[125.54761]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 1.] 
cards in discard: [29.  1.  1. 29. 29.  0.  0.  3. 29. 25. 29.  0.  0.  1.  3.  1. 29.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25] -> size -> 28 
action values: 1 
buys: 0 
player value: 2 
card supply: [27. 22. 30. 28. 30.  8.  9. 10.  9.  6.  8.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [3. 0. 0. 8. 1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8  0] -> size -> 18 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -25 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 84.53826904296875





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[122.02116 ]
 [135.09293 ]
 [ 88.79804 ]
 [127.41138 ]
 [ 83.11061 ]
 [ 41.701336]
 [131.98474 ]
 [133.10811 ]
 [121.53542 ]
 [136.22447 ]
 [132.60283 ]
 [135.8682  ]
 [130.33507 ]
 [131.88309 ]
 [135.24585 ]
 [123.74497 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 1.] 
cards in discard: [29.  1.  1. 29. 29.  0.  0.  3. 29. 25. 29.  0.  0.  1.  3.  1. 29.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25] -> size -> 28 
action values: 0 
buys: 1 
player value: 6 
card supply: [27. 22. 30. 28. 30.  8.  9. 10.  9.  6.  8.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [3. 0. 0. 8. 1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8  0] -> size -> 18 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -25 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 125.547607421875



buy possibilites: [-1] 
expected returns: [[79.00859]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 1.] 
cards in discard: [29.  1.  1. 29. 29.  0.  0.  3. 29. 25. 29.  0.  0.  1.  3.  1. 29. 25.] 
cards in deck: 6 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25] -> size -> 29 
action values: 0 
buys: 0 
player value: 1 
card supply: [27. 22. 30. 28. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [3. 0. 0. 8. 1.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8  0] -> size -> 18 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5.    0.    0.  -60.    0.    0.   40.    0.    0.    0.    0.    0.
   0.    0.   62.5   0. ] 
sum of rewards: 37.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 136.2244873046875






         -------------------- Turn: 19 -------------------- 
Player: 1 
cards in hand: [3. 0. 0. 8. 1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 8. 1.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8  0] -> size -> 18 
action values: 1 
buys: 0 
player value: 0 
card supply: [27. 22. 30. 28. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0.  6. 29.  3.  1.] 
adversary cards in discard: [29.  1.  1. 29. 29.  0.  0.  3. 29. 25. 29.  0.  0.  1.  3.  1. 29. 25.
 29. 29.  0.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25] -> size -> 29 
adversary victory points: 2
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 8. 1.] 
cards in discard: [] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8  0] -> size -> 18 
action values: 0 
buys: 1 
player value: 4 
card supply: [27. 22. 30. 28. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0.  6. 29.  3.  1.] 
adversary cards in discard: [29.  1.  1. 29. 29.  0.  0.  3. 29. 25. 29.  0.  0.  1.  3.  1. 29. 25.
 29. 29.  0.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25] -> size -> 29 
adversary victory points: 2
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 8. 1.] 
cards in discard: [0.] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8  0  0] -> size -> 19 
action values: 0 
buys: 0 
player value: 4 
card supply: [26. 22. 30. 28. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0.  6. 29.  3.  1.] 
adversary cards in discard: [29.  1.  1. 29. 29.  0.  0.  3. 29. 25. 29.  0.  0.  1.  3.  1. 29. 25.
 29. 29.  0.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25] -> size -> 29 
adversary victory points: 2
player victory points: 4 





Player: 0 
cards in hand: [ 0.  6. 29.  3.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[-18.661951 ]
 [ -1.2308781]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  6. 29.  3.  1.] 
cards in discard: [29.  1.  1. 29. 29.  0.  0.  3. 29. 25. 29.  0.  0.  1.  3.  1. 29. 25.
 29. 29.  0.  0.  1.] 
cards in deck: 1 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25] -> size -> 29 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 22. 30. 28. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [25.  8.  1.  0.  8.] 
adversary cards in discard: [0. 3. 0. 0. 8. 1.] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8  0  0] -> size -> 19 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -65 

action type: buy - action -1
Learning step: 0
desired expected reward: 79.00859069824219



action possibilites: [-1. 29.] 
expected returns: [[-20.009405 ]
 [ -2.4453704]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  6.  3. 29.] 
cards in discard: [29.  1.  1. 29. 29.  0.  0.  3. 29. 25. 29.  0.  0.  1.  3.  1. 29. 25.
 29. 29.  0.  0.  1.  1.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25] -> size -> 29 
action values: 1 
buys: 0 
player value: 1 
card supply: [26. 22. 30. 28. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [25.  8.  1.  0.  8.] 
adversary cards in discard: [0. 3. 0. 0. 8. 1.] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8  0  0] -> size -> 19 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -6.648284912109375



action possibilites: [-1.] 
expected returns: [[-5.9075437]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 6. 3.] 
cards in discard: [29.] 
cards in deck: 23 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25] -> size -> 29 
action values: 1 
buys: 0 
player value: 2 
card supply: [26. 22. 30. 28. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [25.  8.  1.  0.  8.] 
adversary cards in discard: [0. 3. 0. 0. 8. 1.] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8  0  0] -> size -> 19 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -25 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -7.9290666580200195





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[  -6.15285  ]
 [   7.374403 ]
 [  -1.281188 ]
 [-107.15153  ]
 [   4.0620575]
 [  -7.1474123]
 [   1.3449166]
 [  -5.232604 ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 6. 3.] 
cards in discard: [29.] 
cards in deck: 23 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25] -> size -> 29 
action values: 0 
buys: 1 
player value: 3 
card supply: [26. 22. 30. 28. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [25.  8.  1.  0.  8.] 
adversary cards in discard: [0. 3. 0. 0. 8. 1.] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8  0  0] -> size -> 19 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -25 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -5.907543659210205



buy possibilites: [-1] 
expected returns: [[4.800457]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 6. 3.] 
cards in discard: [29.  1.] 
cards in deck: 23 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1] -> size -> 30 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 21. 30. 28. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [25.  8.  1.  0.  8.] 
adversary cards in discard: [0. 3. 0. 0. 8. 1.] 
adversary owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8  0  0] -> size -> 19 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0  54   0] 
sum of rewards: 29 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 7.374415397644043






         -------------------- Turn: 20 -------------------- 
Player: 1 
cards in hand: [25.  8.  1.  0.  8.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25.  8.  8.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  8.  1.  0.  8.] 
cards in discard: [0. 3. 0. 0. 8. 1.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  3  3 10  3  0 25 11  1  8 15  0  8  3  1  8  0  0] -> size -> 19 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 21. 30. 28. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 1.  1. 29. 29.  0.] 
adversary cards in discard: [29.  1. 29. 29.  0.  6.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1] -> size -> 30 
adversary victory points: 2
player victory points: 4 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  1.] 
cards in discard: [0. 3. 0. 0. 8. 1.] 
cards in deck: 8 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0] -> size -> 17 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 21. 30. 28. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 1.  1. 29. 29.  0.] 
adversary cards in discard: [29.  1. 29. 29.  0.  6.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1] -> size -> 30 
adversary victory points: 2
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [25.  1.] 
cards in discard: [0. 3. 0. 0. 8. 1.] 
cards in deck: 8 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0] -> size -> 17 
action values: 0 
buys: 1 
player value: 2 
card supply: [26. 21. 30. 28. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 1.  1. 29. 29.  0.] 
adversary cards in discard: [29.  1. 29. 29.  0.  6.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1] -> size -> 30 
adversary victory points: 2
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [25.  1.] 
cards in discard: [0. 3. 0. 0. 8. 1. 3.] 
cards in deck: 8 
card top of deck: [] 
played cards: [8.] 
owned cards: [ 0  3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3] -> size -> 18 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 21. 30. 27. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 1.  1. 29. 29.  0.] 
adversary cards in discard: [29.  1. 29. 29.  0.  6.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1] -> size -> 30 
adversary victory points: 2
player victory points: 5 





Player: 0 
cards in hand: [ 1.  1. 29. 29.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[52.953228]
 [71.91501 ]
 [71.91501 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  1. 29. 29.  0.] 
cards in discard: [29.  1. 29. 29.  0.  6.  3.] 
cards in deck: 18 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1] -> size -> 30 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 21. 30. 27. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0. 15.  3. 10.  3.] 
adversary cards in discard: [ 0.  3.  0.  0.  8.  1.  3.  8. 25.  1.] 
adversary owned cards: [ 0  3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3] -> size -> 18 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -95 

action type: buy - action -1
Learning step: 0
desired expected reward: 4.800457000732422



action possibilites: [-1. 29. 29.] 
expected returns: [[50.580273]
 [71.04528 ]
 [71.04528 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 29.  0. 29.] 
cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.] 
cards in deck: 17 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1] -> size -> 30 
action values: 1 
buys: 0 
player value: 1 
card supply: [26. 21. 30. 27. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0. 15.  3. 10.  3.] 
adversary cards in discard: [ 0.  3.  0.  0.  8.  1.  3.  8. 25.  1.] 
adversary owned cards: [ 0  3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3] -> size -> 18 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -75 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 65.2495346069336



action possibilites: [-1. 29. 29.] 
expected returns: [[25.760317]
 [43.8606  ]
 [43.8606  ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 29. 29.] 
cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1.] 
cards in deck: 16 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1] -> size -> 30 
action values: 1 
buys: 0 
player value: 2 
card supply: [26. 21. 30. 27. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0. 15.  3. 10.  3.] 
adversary cards in discard: [ 0.  3.  0.  0.  8.  1.  3.  8. 25.  1.] 
adversary owned cards: [ 0  3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3] -> size -> 18 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -55 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 64.720703125



action possibilites: [-1.] 
expected returns: [[50.820858]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0.] 
cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1] -> size -> 30 
action values: 1 
buys: 0 
player value: 3 
card supply: [26. 21. 30. 27. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0. 15.  3. 10.  3.] 
adversary cards in discard: [ 0.  3.  0.  0.  8.  1.  3.  8. 25.  1.] 
adversary owned cards: [ 0  3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3] -> size -> 18 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: -35 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 38.31081771850586





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 51.504368]
 [ 64.9695  ]
 [ 56.332058]
 [ 14.495808]
 [-26.85609 ]
 [ 61.04504 ]
 [ 62.189007]
 [ 51.34597 ]
 [ 66.19799 ]
 [ 61.59688 ]
 [ 65.784386]
 [ 59.42627 ]
 [ 60.78992 ]
 [ 64.87288 ]
 [ 51.21609 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0.] 
cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1] -> size -> 30 
action values: 0 
buys: 1 
player value: 5 
card supply: [26. 21. 30. 27. 30.  8.  9. 10.  9.  6.  7.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0. 15.  3. 10.  3.] 
adversary cards in discard: [ 0.  3.  0.  0.  8.  1.  3.  8. 25.  1.] 
adversary owned cards: [ 0  3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3] -> size -> 18 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: -35 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: 50.820858001708984



buy possibilites: [-1] 
expected returns: [[28.09845]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0.] 
cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29. 25.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25] -> size -> 31 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 21. 30. 27. 30.  8.  9. 10.  9.  6.  6.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0. 15.  3. 10.  3.] 
adversary cards in discard: [ 0.  3.  0.  0.  8.  1.  3.  8. 25.  1.] 
adversary owned cards: [ 0  3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3] -> size -> 18 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0  60   0   0   0   0   0   0   0 250   0] 
sum of rewards: 215 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 66.197998046875






         -------------------- Turn: 21 -------------------- 
Player: 1 
cards in hand: [ 0. 15.  3. 10.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15. 10.] 
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 15.  3. 10.  3.] 
cards in discard: [ 0.  3.  0.  0.  8.  1.  3.  8. 25.  1.] 
cards in deck: 3 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3] -> size -> 18 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 21. 30. 27. 30.  8.  9. 10.  9.  6.  6.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0.  1.  0. 29.  3.] 
adversary cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29. 25. 29. 29. 29.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25] -> size -> 31 
adversary victory points: 2
player victory points: 5 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 10.  3.] 
cards in discard: [ 0.  3.  0.  0.  8.  1.  3.  8. 25.  1.] 
cards in deck: 3 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3] -> size -> 17 
action values: 0 
buys: 0 
player value: 3 
card supply: [26. 21. 30. 27. 30.  8.  9. 10.  9.  6.  6.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0.  1.  0. 29.  3.] 
adversary cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29. 25. 29. 29. 29.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25] -> size -> 31 
adversary victory points: 2
player victory points: 5 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 10.  3.] 
cards in discard: [ 0.  3.  0.  0.  8.  1.  3.  8. 25.  1.] 
cards in deck: 3 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3] -> size -> 17 
action values: 0 
buys: 1 
player value: 3 
card supply: [26. 21. 30. 27. 30.  8.  9. 10.  9.  6.  6.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0.  1.  0. 29.  3.] 
adversary cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29. 25. 29. 29. 29.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25] -> size -> 31 
adversary victory points: 2
player victory points: 5 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 10.  3.] 
cards in discard: [ 0.  3.  0.  0.  8.  1.  3.  8. 25.  1.  1.] 
cards in deck: 3 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1] -> size -> 18 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 20. 30. 27. 30.  8.  9. 10.  9.  6.  6.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 0.  1.  0. 29.  3.] 
adversary cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29. 25. 29. 29. 29.  0.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25] -> size -> 31 
adversary victory points: 2
player victory points: 5 





Player: 0 
cards in hand: [ 0.  1.  0. 29.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[102.20475 ]
 [119.415344]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  1.  0. 29.  3.] 
cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29. 25. 29. 29. 29.  0.  0.] 
cards in deck: 10 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25] -> size -> 31 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 20. 30. 27. 30.  8.  9. 10.  9.  6.  6.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 3.  8.  3.  0. 11.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1] -> size -> 18 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -95 

action type: buy - action -1
Learning step: 0
desired expected reward: 28.09844970703125



action possibilites: [-1. 25.] 
expected returns: [[126.88208]
 [138.92194]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  3. 25.] 
cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29. 25. 29. 29. 29.  0.  0.  1.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25] -> size -> 31 
action values: 1 
buys: 0 
player value: 1 
card supply: [26. 20. 30. 27. 30.  8.  9. 10.  9.  6.  6.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 3.  8.  3.  0. 11.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1] -> size -> 18 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -75 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 114.1463623046875



action possibilites: [-1] 
expected returns: [[25.639177]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3. 0. 1.] 
cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29. 25. 29. 29. 29.  0.  0.  1.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25] -> size -> 31 
action values: 0 
buys: 0 
player value: 1 
card supply: [26. 20. 30. 27. 30.  8.  8. 10.  9.  6.  6.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 3.  8.  3.  0. 11.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6] -> size -> 19 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -55 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 138.92193603515625





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 22.668556]
 [ 34.773197]
 [ -9.387283]
 [ 27.537338]
 [-16.688953]
 [-83.15529 ]
 [ 31.765533]
 [ 32.65414 ]
 [ 22.369583]
 [ 35.81594 ]
 [ 32.19279 ]
 [ 35.475193]
 [ 30.138775]
 [ 31.582218]
 [ 34.792385]
 [ 24.369425]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 1.] 
cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29. 25. 29. 29. 29.  0.  0.  1.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25] -> size -> 31 
action values: 0 
buys: 1 
player value: 6 
card supply: [26. 20. 30. 27. 30.  8.  8. 10.  9.  6.  6.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 3.  8.  3.  0. 11.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6] -> size -> 19 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -90   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -55 

action type: take_action - action -1
Learning step: 0
desired expected reward: 25.639177322387695



buy possibilites: [-1] 
expected returns: [[-24.786638]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3. 0. 1.] 
cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29. 25. 29. 29. 29.  0.  0.  1. 25.] 
cards in deck: 7 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25] -> size -> 32 
action values: 0 
buys: 0 
player value: 1 
card supply: [26. 20. 30. 27. 30.  8.  8. 10.  9.  6.  5.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 3.  8.  3.  0. 11.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6] -> size -> 19 
adversary victory points: 5
player victory points: 2 

Reward from previous game state: 
[ -5.    0.    0.  -90.    0.    0.   40.    0.    0.    0.    0.    0.
   0.    0.   62.5   0. ] 
sum of rewards: 7.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 35.81596755981445






         -------------------- Turn: 22 -------------------- 
Player: 1 
cards in hand: [ 3.  8.  3.  0. 11.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8. 11.] 
Chosen action: 11 : ['Workshop' '11' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  8.  3.  0. 11.] 
cards in discard: [6.] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6] -> size -> 19 
action values: 1 
buys: 0 
player value: 0 
card supply: [26. 20. 30. 27. 30.  8.  8. 10.  9.  6.  5.  0. 10. 10.  9. 10.  9.] 
adversary cards in hand: [ 1.  1.  0. 29. 25.] 
adversary cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29. 25. 29. 29. 29.  0.  0.  1. 25.
 29. 25.  0.  0.  3.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25] -> size -> 32 
adversary victory points: 2
player victory points: 4 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 8. 3. 0.] 
cards in discard: [ 6. 15.] 
cards in deck: 13 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15] -> size -> 20 
action values: 0 
buys: 0 
player value: 0 
card supply: [26. 20. 30. 27. 30.  8.  8. 10.  9.  6.  5.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 1.  1.  0. 29. 25.] 
adversary cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29. 25. 29. 29. 29.  0.  0.  1. 25.
 29. 25.  0.  0.  3.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25] -> size -> 32 
adversary victory points: 2
player victory points: 4 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 8. 3. 0.] 
cards in discard: [ 6. 15.] 
cards in deck: 13 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15] -> size -> 20 
action values: 0 
buys: 1 
player value: 1 
card supply: [26. 20. 30. 27. 30.  8.  8. 10.  9.  6.  5.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 1.  1.  0. 29. 25.] 
adversary cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29. 25. 29. 29. 29.  0.  0.  1. 25.
 29. 25.  0.  0.  3.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25] -> size -> 32 
adversary victory points: 2
player victory points: 4 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 8. 3. 0.] 
cards in discard: [ 6. 15.  0.] 
cards in deck: 13 
card top of deck: [] 
played cards: [11.] 
owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0] -> size -> 21 
action values: 0 
buys: 0 
player value: 1 
card supply: [25. 20. 30. 27. 30.  8.  8. 10.  9.  6.  5.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 1.  1.  0. 29. 25.] 
adversary cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29. 25. 29. 29. 29.  0.  0.  1. 25.
 29. 25.  0.  0.  3.  0.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25] -> size -> 32 
adversary victory points: 2
player victory points: 4 





Player: 0 
cards in hand: [ 1.  1.  0. 29. 25.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 25.] 
expected returns: [[16.190063]
 [33.60772 ]
 [29.085289]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  1.  0. 29. 25.] 
cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29. 25. 29. 29. 29.  0.  0.  1. 25.
 29. 25.  0.  0.  3.  0.  1.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25] -> size -> 32 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 20. 30. 27. 30.  8.  8. 10.  9.  6.  5.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 3.  0. 10.  0.  0.] 
adversary cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0] -> size -> 21 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -65 

action type: buy - action -1
Learning step: 0
desired expected reward: -24.786638259887695



action possibilites: [-1. 25. 29.] 
expected returns: [[16.224821]
 [29.969479]
 [34.82925 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  0. 25. 29.] 
cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29. 25. 29. 29. 29.  0.  0.  1. 25.
 29. 25.  0.  0.  3.  0.  1.  1.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25] -> size -> 32 
action values: 1 
buys: 0 
player value: 1 
card supply: [25. 20. 30. 27. 30.  8.  8. 10.  9.  6.  5.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 3.  0. 10.  0.  0.] 
adversary cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0] -> size -> 21 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 28.200645446777344



action possibilites: [-1. 25.] 
expected returns: [[10.895559]
 [24.640203]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 25.  3.] 
cards in discard: [29.  1. 29. 29.  0.  6.  3.  1.  1. 29. 25. 29. 29. 29.  0.  0.  1. 25.
 29. 25.  0.  0.  3.  0.  1.  1.  1.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25] -> size -> 32 
action values: 1 
buys: 0 
player value: 2 
card supply: [25. 20. 30. 27. 30.  8.  8. 10.  9.  6.  5.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 3.  0. 10.  0.  0.] 
adversary cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0] -> size -> 21 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -25 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 29.05173110961914



action possibilites: [-1] 
expected returns: [[9.276135]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  3.  0. 29.] 
cards in discard: [] 
cards in deck: 25 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25] -> size -> 32 
action values: 0 
buys: 0 
player value: 2 
card supply: [25. 20. 30. 27. 30.  8.  7. 10.  9.  6.  5.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 3.  0. 10.  0.  0.] 
adversary cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.  6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6] -> size -> 22 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: -5 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 24.64020538330078





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[  9.291731]
 [ 23.17714 ]
 [ 14.152878]
 [-63.28246 ]
 [ 19.025013]
 [ 19.788324]
 [  8.50834 ]
 [ 19.236517]
 [ 17.003193]
 [ 22.602987]
 [  9.589632]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3.  0. 29.] 
cards in discard: [] 
cards in deck: 25 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25] -> size -> 32 
action values: 0 
buys: 1 
player value: 4 
card supply: [25. 20. 30. 27. 30.  8.  7. 10.  9.  6.  5.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 3.  0. 10.  0.  0.] 
adversary cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.  6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6] -> size -> 22 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  60   0   0   0   0   0   0   0   0   0] 
sum of rewards: -5 

action type: take_action - action -1
Learning step: 0
desired expected reward: 9.276135444641113



buy possibilites: [-1] 
expected returns: [[24.734741]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  3.  0. 29.] 
cards in discard: [1.] 
cards in deck: 25 
card top of deck: [] 
played cards: [29. 29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1] -> size -> 33 
action values: 0 
buys: 0 
player value: 1 
card supply: [25. 19. 30. 27. 30.  8.  7. 10.  9.  6.  5.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 3.  0. 10.  0.  0.] 
adversary cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.  6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6] -> size -> 22 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5.    0.    0.  -60.    0.    0.   60.    0.    0.    0.    0.    0.
   0.    0.   13.5   0. ] 
sum of rewards: 8.5 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 23.177146911621094






         -------------------- Turn: 23 -------------------- 
Player: 1 
cards in hand: [ 3.  0. 10.  0.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 10.] 
Chosen action: 10 : ['Village' '10' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 10.  0.  0.] 
cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.  6.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6] -> size -> 22 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 19. 30. 27. 30.  8.  7. 10.  9.  6.  5.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 3.  0. 25.  0. 29.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1] -> size -> 33 
adversary victory points: 2
player victory points: 3 


action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 0. 0. 0. 3.] 
cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.  6.] 
cards in deck: 7 
card top of deck: [] 
played cards: [10.] 
owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6] -> size -> 22 
action values: 2 
buys: 0 
player value: 0 
card supply: [25. 19. 30. 27. 30.  8.  7. 10.  9.  6.  5.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 3.  0. 25.  0. 29.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1] -> size -> 33 
adversary victory points: 2
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 3.] 
cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.  6.] 
cards in deck: 7 
card top of deck: [] 
played cards: [10.] 
owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6] -> size -> 22 
action values: 0 
buys: 1 
player value: 3 
card supply: [25. 19. 30. 27. 30.  8.  7. 10.  9.  6.  5.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 3.  0. 25.  0. 29.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1] -> size -> 33 
adversary victory points: 2
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 0. 0. 0. 3.] 
cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.  6.  3.] 
cards in deck: 7 
card top of deck: [] 
played cards: [10.] 
owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6  3] -> size -> 23 
action values: 0 
buys: 0 
player value: 1 
card supply: [25. 19. 30. 26. 30.  8.  7. 10.  9.  6.  5.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 3.  0. 25.  0. 29.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1] -> size -> 33 
adversary victory points: 2
player victory points: 4 





Player: 0 
cards in hand: [ 3.  0. 25.  0. 29.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 29.] 
expected returns: [[32.44061 ]
 [45.908474]
 [50.36231 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 25.  0. 29.] 
cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29.] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1] -> size -> 33 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 19. 30. 26. 30.  8.  7. 10.  9.  6.  5.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 3. 25. 15.  1.  1.] 
adversary cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.  6.  3. 10.  3.  0.  0.  0.  3.] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6  3] -> size -> 23 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -65 

action type: buy - action -1
Learning step: 0
desired expected reward: 24.7347412109375



action possibilites: [-1. 25.] 
expected returns: [[21.186787]
 [34.10583 ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 25.  0.] 
cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29.] 
cards in deck: 19 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1] -> size -> 33 
action values: 1 
buys: 0 
player value: 1 
card supply: [25. 19. 30. 26. 30.  8.  7. 10.  9.  6.  5.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 3. 25. 15.  1.  1.] 
adversary cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.  6.  3. 10.  3.  0.  0.  0.  3.] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6  3] -> size -> 23 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 44.85087585449219



action possibilites: [-1] 
expected returns: [[35.24094]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0.  0. 29.  1.] 
cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29.] 
cards in deck: 17 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1] -> size -> 33 
action values: 0 
buys: 0 
player value: 1 
card supply: [25. 19. 30. 26. 30.  8.  6. 10.  9.  6.  5.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 3. 25. 15.  1.  1.] 
adversary cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.  6.  3. 10.  3.  0.  0.  0.  3.  6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6] -> size -> 24 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -25 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 34.105831146240234





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 34.612415 ]
 [ 47.582226 ]
 [ 39.09872  ]
 [ -0.0711329]
 [-39.016914 ]
 [ 43.697014 ]
 [ 44.594517 ]
 [ 34.284534 ]
 [ 48.626945 ]
 [ 44.047565 ]
 [ 48.24854  ]
 [ 41.98986  ]
 [ 43.335926 ]
 [ 47.238438 ]
 [ 34.45877  ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0.  0. 29.  1.] 
cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29.] 
cards in deck: 17 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1] -> size -> 33 
action values: 0 
buys: 1 
player value: 5 
card supply: [25. 19. 30. 26. 30.  8.  6. 10.  9.  6.  5.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 3. 25. 15.  1.  1.] 
adversary cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.  6.  3. 10.  3.  0.  0.  0.  3.  6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6] -> size -> 24 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: -25 

action type: take_action - action -1
Learning step: 0
desired expected reward: 35.24094009399414



buy possibilites: [-1] 
expected returns: [[19.158167]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  0.  0. 29.  1.] 
cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25.] 
cards in deck: 17 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25] -> size -> 34 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 19. 30. 26. 30.  8.  6. 10.  9.  6.  4.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [ 3. 25. 15.  1.  1.] 
adversary cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.  6.  3. 10.  3.  0.  0.  0.  3.  6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6] -> size -> 24 
adversary victory points: 4
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -60   0   0  40   0   0   0   0   0   0   0 250   0] 
sum of rewards: 225 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 48.62693405151367






         -------------------- Turn: 24 -------------------- 
Player: 1 
cards in hand: [ 3. 25. 15.  1.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 15.] 
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 25. 15.  1.  1.] 
cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.  6.  3. 10.  3.  0.  0.  0.  3.  6.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6] -> size -> 24 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 19. 30. 26. 30.  8.  6. 10.  9.  6.  4.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [25. 29.  1.  1.  1.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25] -> size -> 34 
adversary victory points: 2
player victory points: 3 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 25.  1.  1.] 
cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.  6.  3. 10.  3.  0.  0.  0.  3.  6.] 
cards in deck: 2 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6] -> size -> 24 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 19. 30. 26. 30.  8.  6. 10.  9.  6.  4.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [25. 29.  1.  1.  1.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25] -> size -> 34 
adversary victory points: 2
player victory points: 3 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: 15.0 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 25.  1.  1.] 
cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.  6.  3. 10.  3.  0.  0.  0.  3.  6.] 
cards in deck: 2 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6] -> size -> 24 
action values: 0 
buys: 1 
player value: 4 
card supply: [25. 19. 30. 26. 30.  8.  6. 10.  9.  6.  4.  0. 10. 10.  9. 10.  8.] 
adversary cards in hand: [25. 29.  1.  1.  1.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25] -> size -> 34 
adversary victory points: 2
player victory points: 3 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 25.  1.  1.] 
cards in discard: [ 6. 15.  0. 11.  3.  8.  3.  0.  6.  3. 10.  3.  0.  0.  0.  3.  6. 15.] 
cards in deck: 2 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6
 15] -> size -> 25 
action values: 0 
buys: 0 
player value: 0 
card supply: [25. 19. 30. 26. 30.  8.  6. 10.  9.  6.  4.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [25. 29.  1.  1.  1.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25] -> size -> 34 
adversary victory points: 2
player victory points: 3 





Player: 0 
cards in hand: [25. 29.  1.  1.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 29.] 
expected returns: [[12.647999]
 [23.305965]
 [27.339533]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 29.  1.  1.  1.] 
cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.] 
cards in deck: 12 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25] -> size -> 34 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 19. 30. 26. 30.  8.  6. 10.  9.  6.  4.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 3.  3. 10.  1.  8.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6
 15] -> size -> 25 
adversary victory points: 3
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -30   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -35 

action type: buy - action -1
Learning step: 0
desired expected reward: 19.158166885375977



action possibilites: [-1. 25.] 
expected returns: [[22.688515]
 [34.532482]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  1.  1.  0.] 
cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.  1.] 
cards in deck: 11 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25] -> size -> 34 
action values: 1 
buys: 0 
player value: 1 
card supply: [25. 19. 30. 26. 30.  8.  6. 10.  9.  6.  4.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 3.  3. 10.  1.  8.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6
 15] -> size -> 25 
adversary victory points: 3
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 22.49136734008789



action possibilites: [-1] 
expected returns: [[-3.9951284]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1. 1. 0. 3. 0.] 
cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.  1.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25] -> size -> 34 
action values: 0 
buys: 0 
player value: 1 
card supply: [25. 19. 30. 26. 30.  8.  5. 10.  9.  6.  4.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 3.  3. 10.  1.  8.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6
 15  6] -> size -> 26 
adversary victory points: 3
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -30   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 5 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 34.53249740600586





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[-7.1312380e+00]
 [ 4.6346521e+00]
 [-3.6899982e+01]
 [-2.5043402e+00]
 [-4.2120991e+01]
 [-9.3772041e+01]
 [ 1.8220594e+00]
 [ 2.4484336e+00]
 [-7.7966728e+00]
 [ 5.3756447e+00]
 [ 2.0563762e+00]
 [ 5.1035156e+00]
 [ 7.0055723e-02]
 [ 1.5498669e+00]
 [ 4.3611355e+00]
 [-4.8168259e+00]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 1. 0. 3. 0.] 
cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.  1.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25] -> size -> 34 
action values: 0 
buys: 1 
player value: 7 
card supply: [25. 19. 30. 26. 30.  8.  5. 10.  9.  6.  4.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 3.  3. 10.  1.  8.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6
 15  6] -> size -> 26 
adversary victory points: 3
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -30   0   0  40   0   0   0   0   0   0   0   0   0] 
sum of rewards: 5 

action type: take_action - action -1
Learning step: 0
desired expected reward: -3.9951283931732178



buy possibilites: [-1] 
expected returns: [[-23.37969]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1. 1. 0. 3. 0.] 
cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.  1.
 25.] 
cards in deck: 9 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25] -> size -> 35 
action values: 0 
buys: 0 
player value: 2 
card supply: [25. 19. 30. 26. 30.  8.  5. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 3.  3. 10.  1.  8.] 
adversary cards in discard: [6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6
 15  6] -> size -> 26 
adversary victory points: 3
player victory points: 2 

Reward from previous game state: 
[ -5.    0.    0.  -30.    0.    0.   40.    0.    0.    0.    0.    0.
   0.    0.   62.5   0. ] 
sum of rewards: 67.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 5.375646591186523






         -------------------- Turn: 25 -------------------- 
Player: 1 
cards in hand: [ 3.  3. 10.  1.  8.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 10.  8.] 
Chosen action: 10 : ['Village' '10' '3']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  3. 10.  1.  8.] 
cards in discard: [6.] 
cards in deck: 20 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6
 15  6] -> size -> 26 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 19. 30. 26. 30.  8.  5. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1. 29.  0.  1.  1.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.  1.
 25. 29. 25.  1.  1.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25] -> size -> 35 
adversary victory points: 2
player victory points: 2 


action possibilites: [-1.  8. 15.] 
Chosen action: 8 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  3.  1.  8. 15.] 
cards in discard: [6.] 
cards in deck: 19 
card top of deck: [] 
played cards: [10.] 
owned cards: [ 3  3 10  3  0 25 11  1 15  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6
 15  6] -> size -> 26 
action values: 2 
buys: 0 
player value: 0 
card supply: [25. 19. 30. 26. 30.  8.  5. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1. 29.  0.  1.  1.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.  1.
 25. 29. 25.  1.  1.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25] -> size -> 35 
adversary victory points: 2
player victory points: 2 


action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [3. 3.] 
cards in discard: [6.] 
cards in deck: 19 
card top of deck: [] 
played cards: [10.  8.] 
owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6] -> size -> 24 
action values: 1 
buys: 0 
player value: 0 
card supply: [25. 19. 30. 26. 30.  8.  5. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1. 29.  0.  1.  1.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.  1.
 25. 29. 25.  1.  1.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25] -> size -> 35 
adversary victory points: 2
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: 0.0 : ['Copper' '0' '0']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3.] 
cards in discard: [6.] 
cards in deck: 19 
card top of deck: [] 
played cards: [10.  8.] 
owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6] -> size -> 24 
action values: 1 
buys: 1 
player value: 0 
card supply: [25. 19. 30. 26. 30.  8.  5. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1. 29.  0.  1.  1.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.  1.
 25. 29. 25.  1.  1.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25] -> size -> 35 
adversary victory points: 2
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [3. 3.] 
cards in discard: [6. 0.] 
cards in deck: 19 
card top of deck: [] 
played cards: [10.  8.] 
owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0] -> size -> 25 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 19. 30. 26. 30.  8.  5. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1. 29.  0.  1.  1.] 
adversary cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.  1.
 25. 29. 25.  1.  1.  0.  3.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25] -> size -> 35 
adversary victory points: 2
player victory points: 2 





Player: 0 
cards in hand: [ 1. 29.  0.  1.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29.] 
expected returns: [[-15.542719 ]
 [  2.2818038]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 29.  0.  1.  1.] 
cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.  1.
 25. 29. 25.  1.  1.  0.  3.  0.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25] -> size -> 35 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 19. 30. 26. 30.  8.  5. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [15.  0.  0.  0.  6.] 
adversary cards in discard: [ 6.  0. 10.  8.  3.  3.] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0] -> size -> 25 
adversary victory points: 2
player victory points: 2 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: -23.379690170288086



action possibilites: [-1. 29.] 
expected returns: [[-1.6608791]
 [16.073387 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  1.  1. 29.] 
cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.  1.
 25. 29. 25.  1.  1.  0.  3.  0.  1.] 
cards in deck: 3 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25] -> size -> 35 
action values: 1 
buys: 0 
player value: 1 
card supply: [24. 19. 30. 26. 30.  8.  5. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [15.  0.  0.  0.  6.] 
adversary cards in discard: [ 6.  0. 10.  8.  3.  3.] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0] -> size -> 25 
adversary victory points: 2
player victory points: 2 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -3.252660036087036



action possibilites: [-1. 29.] 
expected returns: [[-18.061766 ]
 [ -0.6530919]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  1. 29.] 
cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.  1.
 25. 29. 25.  1.  1.  0.  3.  0.  1.  1.] 
cards in deck: 2 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25] -> size -> 35 
action values: 1 
buys: 0 
player value: 2 
card supply: [24. 19. 30. 26. 30.  8.  5. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [15.  0.  0.  0.  6.] 
adversary cards in discard: [ 6.  0. 10.  8.  3.  3.] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0] -> size -> 25 
adversary victory points: 2
player victory points: 2 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 10.575349807739258



action possibilites: [-1.] 
expected returns: [[-30.238535]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 6.] 
cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.  1.
 25. 29. 25.  1.  1.  0.  3.  0.  1.  1.  1.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25] -> size -> 35 
action values: 1 
buys: 0 
player value: 3 
card supply: [24. 19. 30. 26. 30.  8.  5. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [15.  0.  0.  0.  6.] 
adversary cards in discard: [ 6.  0. 10.  8.  3.  3.] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0] -> size -> 25 
adversary victory points: 2
player victory points: 2 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -6.051185607910156





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[ -31.163961]
 [ -18.196056]
 [ -26.280678]
 [-112.1307  ]
 [ -21.878613]
 [ -21.154446]
 [ -32.261482]
 [ -21.651728]
 [ -23.83304 ]
 [ -18.655472]
 [ -30.369953]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 6.] 
cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.  1.
 25. 29. 25.  1.  1.  0.  3.  0.  1.  1.  1.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25] -> size -> 35 
action values: 0 
buys: 1 
player value: 4 
card supply: [24. 19. 30. 26. 30.  8.  5. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [15.  0.  0.  0.  6.] 
adversary cards in discard: [ 6.  0. 10.  8.  3.  3.] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0] -> size -> 25 
adversary victory points: 2
player victory points: 2 

Reward from previous game state: 
[-5  0  0  0  0  0 60  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -30.238534927368164



buy possibilites: [-1] 
expected returns: [[-55.721226]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 6.] 
cards in discard: [ 1. 29. 29. 25.  0.  3.  0. 29. 29. 25. 29. 25.  3.  0.  0. 29.  1.  1.
 25. 29. 25.  1.  1.  0.  3.  0.  1.  1.  1.  1.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29. 29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1] -> size -> 36 
action values: 0 
buys: 0 
player value: 1 
card supply: [24. 18. 30. 26. 30.  8.  5. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [15.  0.  0.  0.  6.] 
adversary cards in discard: [ 6.  0. 10.  8.  3.  3.] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0] -> size -> 25 
adversary victory points: 2
player victory points: 2 

Reward from previous game state: 
[ -5.    0.    0.    0.    0.    0.   60.    0.    0.    0.    0.  -10.
   0.    0.   13.5   0. ] 
sum of rewards: 58.5 

action type: buy - action 1.0
Learning step: 0
desired expected reward: -18.196062088012695






         -------------------- Turn: 26 -------------------- 
Player: 1 
cards in hand: [15.  0.  0.  0.  6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 15.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [15.  0.  0.  0.  6.] 
cards in discard: [ 6.  0. 10.  8.  3.  3.] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0] -> size -> 25 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 18. 30. 26. 30.  8.  5. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1.  0.  0.  0. 25.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1] -> size -> 36 
adversary victory points: 2
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  0.  0.  0.  6.] 
cards in discard: [ 6.  0. 10.  8.  3.  3.] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0] -> size -> 25 
action values: 0 
buys: 1 
player value: 3 
card supply: [24. 18. 30. 26. 30.  8.  5. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1.  0.  0.  0. 25.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1] -> size -> 36 
adversary victory points: 2
player victory points: 2 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [15.  0.  0.  0.  6.] 
cards in discard: [ 6.  0. 10.  8.  3.  3.  3.] 
cards in deck: 14 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3] -> size -> 26 
action values: 0 
buys: 0 
player value: 1 
card supply: [24. 18. 30. 25. 30.  8.  5. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1.  0.  0.  0. 25.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1] -> size -> 36 
adversary victory points: 2
player victory points: 3 





Player: 0 
cards in hand: [ 1.  0.  0.  0. 25.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25.] 
expected returns: [[-12.504673 ]
 [  1.6155727]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  0.  0.  0. 25.] 
cards in discard: [] 
cards in deck: 31 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1] -> size -> 36 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 18. 30. 25. 30.  8.  5. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1.  1.  3.  6. 25.] 
adversary cards in discard: [ 6.  0. 10.  8.  3.  3.  3. 15.  0.  0.  0.  6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3] -> size -> 26 
adversary victory points: 3
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -30   0   0   0   0   0   0   0   0   0   0   0   0] 
sum of rewards: -35 

action type: buy - action -1
Learning step: 0
desired expected reward: -55.72122573852539



action possibilites: [-1] 
expected returns: [[7.605358]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  0.  0.  0.  1. 29.] 
cards in discard: [] 
cards in deck: 29 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1] -> size -> 36 
action values: 0 
buys: 0 
player value: 0 
card supply: [24. 18. 30. 25. 30.  8.  4. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1.  1.  3.  6. 25.] 
adversary cards in discard: [ 6.  0. 10.  8.  3.  3.  3. 15.  0.  0.  0.  6.  6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6] -> size -> 27 
adversary victory points: 3
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 1.615558385848999





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[ 10.810035]
 [ 23.366028]
 [-17.548496]
 [ 15.124083]
 [-20.626049]
 [-56.723316]
 [ 19.475718]
 [ 20.281784]
 [ 10.259521]
 [ 24.34455 ]
 [ 19.749592]
 [ 23.977877]
 [ 17.748842]
 [ 19.071388]
 [ 22.924028]
 [ 10.419375]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  0.  0.  0.  1. 29.] 
cards in discard: [] 
cards in deck: 29 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1] -> size -> 36 
action values: 0 
buys: 1 
player value: 7 
card supply: [24. 18. 30. 25. 30.  8.  4. 10.  9.  6.  3.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1.  1.  3.  6. 25.] 
adversary cards in discard: [ 6.  0. 10.  8.  3.  3.  3. 15.  0.  0.  0.  6.  6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6] -> size -> 27 
adversary victory points: 3
player victory points: 2 

Reward from previous game state: 
[ -5   0   0 -30   0   0  20   0   0   0   0   0   0   0   0   0] 
sum of rewards: -15 

action type: take_action - action -1
Learning step: 0
desired expected reward: 7.605358123779297



buy possibilites: [-1] 
expected returns: [[12.409918]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  0.  0.  0.  1. 29.] 
cards in discard: [25.] 
cards in deck: 29 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25] -> size -> 37 
action values: 0 
buys: 0 
player value: 2 
card supply: [24. 18. 30. 25. 30.  8.  4. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1.  1.  3.  6. 25.] 
adversary cards in discard: [ 6.  0. 10.  8.  3.  3.  3. 15.  0.  0.  0.  6.  6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6] -> size -> 27 
adversary victory points: 3
player victory points: 2 

Reward from previous game state: 
[ -5.    0.    0.  -30.    0.    0.   20.    0.    0.    0.    0.  -20.
   0.    0.   62.5   0. ] 
sum of rewards: 27.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: 24.34452247619629






         -------------------- Turn: 27 -------------------- 
Player: 1 
cards in hand: [ 1.  1.  3.  6. 25.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  1.  3.  6. 25.] 
cards in discard: [ 6.  0. 10.  8.  3.  3.  3. 15.  0.  0.  0.  6.  6.] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6] -> size -> 27 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 18. 30. 25. 30.  8.  4. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [29. 25. 29.  6. 25.] 
adversary cards in discard: [25. 25.  1.  0.  0.  0.  1. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25] -> size -> 37 
adversary victory points: 2
player victory points: 2 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  1.  3.  6. 25.] 
cards in discard: [ 6.  0. 10.  8.  3.  3.  3. 15.  0.  0.  0.  6.  6.] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6] -> size -> 27 
action values: 0 
buys: 1 
player value: 4 
card supply: [24. 18. 30. 25. 30.  8.  4. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [29. 25. 29.  6. 25.] 
adversary cards in discard: [25. 25.  1.  0.  0.  0.  1. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25] -> size -> 37 
adversary victory points: 2
player victory points: 2 


 --------- NOTHING HAPPENED --------- 



Player: 0 
cards in hand: [29. 25. 29.  6. 25.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 25. 29. 25.] 
expected returns: [[ 6.569068]
 [23.641388]
 [19.30196 ]
 [23.641388]
 [19.30196 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 25. 29.  6. 25.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29.] 
cards in deck: 24 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25] -> size -> 37 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 18. 30. 25. 30.  8.  4. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 3.  8. 11.  0.  3.] 
adversary cards in discard: [ 6.  0. 10.  8.  3.  3.  3. 15.  0.  0.  0.  6.  6.  1.  1.  3.  6. 25.] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6] -> size -> 27 
adversary victory points: 2
player victory points: 2 

Reward from previous game state: 
[-5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: -5 

action type: buy - action -1
Learning step: 0
desired expected reward: 12.409917831420898



action possibilites: [-1. 25. 25.] 
expected returns: [[-9.776596 ]
 [ 3.0724018]
 [ 3.0724018]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  6. 25.  3.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.] 
cards in deck: 23 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25] -> size -> 37 
action values: 1 
buys: 0 
player value: 1 
card supply: [24. 18. 30. 25. 30.  8.  4. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 3.  8. 11.  0.  3.] 
adversary cards in discard: [ 6.  0. 10.  8.  3.  3.  3. 15.  0.  0.  0.  6.  6.  1.  1.  3.  6. 25.] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6] -> size -> 27 
adversary victory points: 2
player victory points: 2 

Reward from previous game state: 
[-5  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 15 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 18.36444854736328



action possibilites: [-1] 
expected returns: [[-12.936855]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 6. 25.  3.  1.  0.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25] -> size -> 37 
action values: 0 
buys: 0 
player value: 1 
card supply: [24. 18. 30. 25. 30.  8.  3. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 3.  8. 11.  0.  3.] 
adversary cards in discard: [ 6.  0. 10.  8.  3.  3.  3. 15.  0.  0.  0.  6.  6.  1.  1.  3.  6. 25.
  6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6  6] -> size -> 28 
adversary victory points: 2
player victory points: 2 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 3.072402238845825





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[ -13.129734 ]
 [  -0.5255916]
 [  -8.78189  ]
 [-137.61877  ]
 [  -4.295166 ]
 [  -3.463214 ]
 [ -13.483615 ]
 [  -3.987406 ]
 [  -5.9844503]
 [  -0.9001405]
 [ -13.177044 ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 6. 25.  3.  1.  0.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25] -> size -> 37 
action values: 0 
buys: 1 
player value: 4 
card supply: [24. 18. 30. 25. 30.  8.  3. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 3.  8. 11.  0.  3.] 
adversary cards in discard: [ 6.  0. 10.  8.  3.  3.  3. 15.  0.  0.  0.  6.  6.  1.  1.  3.  6. 25.
  6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6  6] -> size -> 28 
adversary victory points: 2
player victory points: 2 

Reward from previous game state: 
[-5  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 35 

action type: take_action - action -1
Learning step: 0
desired expected reward: -12.93685531616211



buy possibilites: [-1] 
expected returns: [[-18.938898]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 6. 25.  3.  1.  0.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1.] 
cards in deck: 21 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1] -> size -> 38 
action values: 0 
buys: 0 
player value: 1 
card supply: [24. 17. 30. 25. 30.  8.  3. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 3.  8. 11.  0.  3.] 
adversary cards in discard: [ 6.  0. 10.  8.  3.  3.  3. 15.  0.  0.  0.  6.  6.  1.  1.  3.  6. 25.
  6.] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6  6] -> size -> 28 
adversary victory points: 2
player victory points: 2 

Reward from previous game state: 
[ -5.    0.    0.    0.    0.    0.   40.    0.    0.    0.    0.  -30.
   0.    0.   13.5   0. ] 
sum of rewards: 18.5 

action type: buy - action 1.0
Learning step: 0
desired expected reward: -0.5255825519561768






         -------------------- Turn: 28 -------------------- 
Player: 1 
cards in hand: [ 3.  8. 11.  0.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.  8. 11.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  8. 11.  0.  3.] 
cards in discard: [ 6.  0. 10.  8.  3.  3.  3. 15.  0.  0.  0.  6.  6.  1.  1.  3.  6. 25.
  6.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6  6] -> size -> 28 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 17. 30. 25. 30.  8.  3. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [29. 29.  0.  0.  3.] 
adversary cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1] -> size -> 38 
adversary victory points: 2
player victory points: 1 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  6. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3.  8. 11.  0.  3.] 
cards in discard: [ 6.  0. 10.  8.  3.  3.  3. 15.  0.  0.  0.  6.  6.  1.  1.  3.  6. 25.
  6.] 
cards in deck: 4 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6  6] -> size -> 28 
action values: 0 
buys: 1 
player value: 1 
card supply: [24. 17. 30. 25. 30.  8.  3. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [29. 29.  0.  0.  3.] 
adversary cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1] -> size -> 38 
adversary victory points: 2
player victory points: 1 


 --------- NOTHING HAPPENED --------- 



Player: 0 
cards in hand: [29. 29.  0.  0.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 29. 29.] 
expected returns: [[38.52776 ]
 [53.615883]
 [53.615883]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [29. 29.  0.  0.  3.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0.] 
cards in deck: 16 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1] -> size -> 38 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 17. 30. 25. 30.  8.  3. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [25.  6.  3.  0. 15.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6  6] -> size -> 28 
adversary victory points: 1
player victory points: 2 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: -18.93889808654785



action possibilites: [-1. 29.] 
expected returns: [[33.82784 ]
 [48.893856]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0.  3. 29.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.] 
cards in deck: 15 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1] -> size -> 38 
action values: 1 
buys: 0 
player value: 1 
card supply: [24. 17. 30. 25. 30.  8.  3. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [25.  6.  3.  0. 15.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6  6] -> size -> 28 
adversary victory points: 1
player victory points: 2 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 48.94073486328125



action possibilites: [-1.] 
expected returns: [[-54.87371]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 3.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.] 
cards in deck: 14 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1] -> size -> 38 
action values: 1 
buys: 0 
player value: 2 
card supply: [24. 17. 30. 25. 30.  8.  3. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [25.  6.  3.  0. 15.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6  6] -> size -> 28 
adversary victory points: 1
player victory points: 2 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: 44.27455139160156





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[ -79.25746 ]
 [ -59.362347]
 [ -74.582664]
 [-197.35323 ]
 [ -63.910877]
 [ -69.74291 ]
 [ -85.27864 ]
 [ -69.233734]
 [ -71.85768 ]
 [ -67.03128 ]
 [ -59.81423 ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.] 
cards in deck: 14 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1] -> size -> 38 
action values: 0 
buys: 1 
player value: 4 
card supply: [24. 17. 30. 25. 30.  8.  3. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [25.  6.  3.  0. 15.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6  6] -> size -> 28 
adversary victory points: 1
player victory points: 2 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -54.87371063232422



buy possibilites: [-1] 
expected returns: [[-70.106155]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 3.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.  1.] 
cards in deck: 14 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1] -> size -> 39 
action values: 0 
buys: 0 
player value: 1 
card supply: [24. 16. 30. 25. 30.  8.  3. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [25.  6.  3.  0. 15.] 
adversary cards in discard: [] 
adversary owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6  6] -> size -> 28 
adversary victory points: 1
player victory points: 2 

Reward from previous game state: 
[ -5.    0.    0.   30.    0.    0.   40.    0.    0.    0.    0.  -40.
   0.    0.   13.5   0. ] 
sum of rewards: 38.5 

action type: buy - action 1.0
Learning step: 0
desired expected reward: -59.36234664916992






         -------------------- Turn: 29 -------------------- 
Player: 1 
cards in hand: [25.  6.  3.  0. 15.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 15.] 
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  6.  3.  0. 15.] 
cards in discard: [] 
cards in deck: 23 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3  0 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6
  0  3  6  6] -> size -> 28 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 16. 30. 25. 30.  8.  3. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1.  3. 25.  1.  1.] 
adversary cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.  1. 29. 29.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1] -> size -> 39 
adversary victory points: 2
player victory points: 1 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25.  6.  3.] 
cards in discard: [] 
cards in deck: 23 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6] -> size -> 27 
action values: 0 
buys: 0 
player value: 3 
card supply: [24. 16. 30. 25. 30.  8.  3. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1.  3. 25.  1.  1.] 
adversary cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.  1. 29. 29.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1] -> size -> 39 
adversary victory points: 2
player victory points: 1 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: -1.0 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [25.  6.  3.] 
cards in discard: [] 
cards in deck: 23 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6] -> size -> 27 
action values: 0 
buys: 1 
player value: 3 
card supply: [24. 16. 30. 25. 30.  8.  3. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1.  3. 25.  1.  1.] 
adversary cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.  1. 29. 29.  0.  0.  3.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1] -> size -> 39 
adversary victory points: 2
player victory points: 1 





Player: 0 
cards in hand: [ 1.  3. 25.  1.  1.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25.] 
expected returns: [[10.355923]
 [23.223461]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  3. 25.  1.  1.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.  1. 29. 29.  0.  0.  3.] 
cards in deck: 9 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1] -> size -> 39 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 16. 30. 25. 30.  8.  3. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [0. 0. 1. 0. 6.] 
adversary cards in discard: [15. 25.  6.  3.] 
adversary owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6] -> size -> 27 
adversary victory points: 1
player victory points: 2 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: -70.10615539550781



action possibilites: [-1] 
expected returns: [[-16.52119]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  3.  1.  1.  0. 29.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.  1. 29. 29.  0.  0.  3.] 
cards in deck: 7 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1] -> size -> 39 
action values: 0 
buys: 0 
player value: 0 
card supply: [24. 16. 30. 25. 30.  8.  2. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [0. 0. 1. 0. 6.] 
adversary cards in discard: [15. 25.  6.  3.  6.] 
adversary owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6] -> size -> 28 
adversary victory points: 1
player victory points: 2 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 23.223468780517578





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  2.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
expected returns: [[-18.062605 ]
 [ -5.5668545]
 [-45.496967 ]
 [-13.209066 ]
 [-49.14946  ]
 [-81.54034  ]
 [ -8.968834 ]
 [ -8.3015375]
 [-19.247158 ]
 [ -4.7386036]
 [ -8.760651 ]
 [ -5.0542684]
 [-10.898324 ]
 [ -9.339769 ]
 [ -5.9930587]
 [-16.809607 ]]
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  3.  1.  1.  0. 29.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.  1. 29. 29.  0.  0.  3.] 
cards in deck: 7 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1] -> size -> 39 
action values: 0 
buys: 1 
player value: 7 
card supply: [24. 16. 30. 25. 30.  8.  2. 10.  9.  6.  2.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [0. 0. 1. 0. 6.] 
adversary cards in discard: [15. 25.  6.  3.  6.] 
adversary owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6] -> size -> 28 
adversary victory points: 1
player victory points: 2 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: take_action - action -1
Learning step: 0
desired expected reward: -16.521190643310547



buy possibilites: [-1] 
expected returns: [[-38.107243]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 1.  3.  1.  1.  0. 29.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.  1. 29. 29.  0.  0.  3. 25.] 
cards in deck: 7 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25] -> size -> 40 
action values: 0 
buys: 0 
player value: 2 
card supply: [24. 16. 30. 25. 30.  8.  2. 10.  9.  6.  1.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [0. 0. 1. 0. 6.] 
adversary cards in discard: [15. 25.  6.  3.  6.] 
adversary owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6] -> size -> 28 
adversary victory points: 1
player victory points: 2 

Reward from previous game state: 
[ -5.    0.    0.   30.    0.    0.   20.    0.    0.    0.    0.  -50.
   0.    0.   62.5   0. ] 
sum of rewards: 57.5 

action type: buy - action 25.0
Learning step: 0
desired expected reward: -4.738584518432617






         -------------------- Turn: 30 -------------------- 
Player: 1 
cards in hand: [0. 0. 1. 0. 6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [0. 0. 1. 0. 6.] 
cards in discard: [15. 25.  6.  3.  6.] 
cards in deck: 18 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6] -> size -> 28 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 16. 30. 25. 30.  8.  2. 10.  9.  6.  1.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1. 25.  1. 29. 25.] 
adversary cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.  1. 29. 29.  0.  0.  3. 25. 25.  1.  3.  1.  1.  0. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25] -> size -> 40 
adversary victory points: 2
player victory points: 0 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  4.  6. 16. 11.  8. 25. 14. 23. 10. 22. 15. -1.] 
Chosen buy action: 25.0 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 1. 0. 6.] 
cards in discard: [15. 25.  6.  3.  6.] 
cards in deck: 18 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6] -> size -> 28 
action values: 0 
buys: 1 
player value: 5 
card supply: [24. 16. 30. 25. 30.  8.  2. 10.  9.  6.  1.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1. 25.  1. 29. 25.] 
adversary cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.  1. 29. 29.  0.  0.  3. 25. 25.  1.  3.  1.  1.  0. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25] -> size -> 40 
adversary victory points: 2
player victory points: 0 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [0. 0. 1. 0. 6.] 
cards in discard: [15. 25.  6.  3.  6. 25.] 
cards in deck: 18 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6 25] -> size -> 29 
action values: 0 
buys: 0 
player value: 0 
card supply: [24. 16. 30. 25. 30.  8.  2. 10.  9.  6.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1. 25.  1. 29. 25.] 
adversary cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.  1. 29. 29.  0.  0.  3. 25. 25.  1.  3.  1.  1.  0. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25] -> size -> 40 
adversary victory points: 2
player victory points: 0 





Player: 0 
cards in hand: [ 1. 25.  1. 29. 25.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 29. 25.] 
expected returns: [[-41.469425]
 [-28.606668]
 [-24.034504]
 [-28.606668]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1. 25.  1. 29. 25.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.  1. 29. 29.  0.  0.  3. 25. 25.  1.  3.  1.  1.  0. 29.] 
cards in deck: 2 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25] -> size -> 40 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 16. 30. 25. 30.  8.  2. 10.  9.  6.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [6. 1. 6. 6. 6.] 
adversary cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.] 
adversary owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6 25] -> size -> 29 
adversary victory points: 0
player victory points: 2 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: -38.107242584228516



action possibilites: [-1. 25. 25. 29.] 
expected returns: [[-84.13275 ]
 [-80.08217 ]
 [-80.08217 ]
 [-73.791885]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [25. 25. 29.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.  1. 29. 29.  0.  0.  3. 25. 25.  1.  3.  1.  1.  0. 29.  1.  1.] 
cards in deck: 1 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25] -> size -> 40 
action values: 1 
buys: 0 
player value: 1 
card supply: [24. 16. 30. 25. 30.  8.  2. 10.  9.  6.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [6. 1. 6. 6. 6.] 
adversary cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.] 
adversary owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6 25] -> size -> 29 
adversary victory points: 0
player victory points: 2 

Reward from previous game state: 
[-5  0  0 60  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 75 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -29.453073501586914



action possibilites: [-1.] 
expected returns: [[-79.48718]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [1.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.  1. 29. 29.  0.  0.  3. 25. 25.  1.  3.  1.  1.  0. 29.  1.  1. 25.
 25.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25] -> size -> 40 
action values: 1 
buys: 0 
player value: 2 
card supply: [24. 16. 30. 25. 30.  8.  2. 10.  9.  6.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [6. 1. 6. 6. 6.] 
adversary cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.] 
adversary owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6 25] -> size -> 29 
adversary victory points: 0
player victory points: 2 

Reward from previous game state: 
[-5  0  0 60  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 95 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -77.02047729492188





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 16. 11.  8. 14. 10. 15. -1.] 
expected returns: [[ -90.74786 ]
 [ -71.55653 ]
 [ -85.98051 ]
 [-203.85881 ]
 [ -76.82028 ]
 [ -80.30559 ]
 [ -98.59355 ]
 [ -80.26143 ]
 [ -82.696144]
 [ -77.536545]
 [ -79.48716 ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.  1. 29. 29.  0.  0.  3. 25. 25.  1.  3.  1.  1.  0. 29.  1.  1. 25.
 25.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25] -> size -> 40 
action values: 0 
buys: 1 
player value: 4 
card supply: [24. 16. 30. 25. 30.  8.  2. 10.  9.  6.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [6. 1. 6. 6. 6.] 
adversary cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.] 
adversary owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6 25] -> size -> 29 
adversary victory points: 0
player victory points: 2 

Reward from previous game state: 
[-5  0  0 60  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 95 

action type: take_action - action -1.0
Learning step: 0
desired expected reward: -79.4871826171875



buy possibilites: [-1] 
expected returns: [[-185.54462]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [1.] 
cards in discard: [25. 25.  1.  0.  0.  0.  1. 29. 29.  1. 29. 25.  6. 25.  3.  1.  0. 29.
 29.  1. 29. 29.  0.  0.  3. 25. 25.  1.  3.  1.  1.  0. 29.  1.  1. 25.
 25.  1.] 
cards in deck: 0 
card top of deck: [] 
played cards: [29. 29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25  1] -> size -> 41 
action values: 0 
buys: 0 
player value: 1 
card supply: [24. 15. 30. 25. 30.  8.  2. 10.  9.  6.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [6. 1. 6. 6. 6.] 
adversary cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.] 
adversary owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6 25] -> size -> 29 
adversary victory points: 0
player victory points: 2 

Reward from previous game state: 
[ -5.    0.    0.   60.    0.    0.   40.    0.    0.    0.    0.  -60.
   0.    0.   13.5   0. ] 
sum of rewards: 48.5 

action type: buy - action 1.0
Learning step: 0
desired expected reward: -71.55655670166016






         -------------------- Turn: 31 -------------------- 
Player: 1 
cards in hand: [6. 1. 6. 6. 6.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1.] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [6. 1. 6. 6. 6.] 
cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6 25] -> size -> 29 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 15. 30. 25. 30.  8.  2. 10.  9.  6.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 0. 25. 29. 25.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25  1] -> size -> 41 
adversary victory points: 2
player victory points: 0 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  3.  6.  8. -1.] 
Chosen buy action: 3.0 : ['Estate' '3' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [6. 1. 6. 6. 6.] 
cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6 25] -> size -> 29 
action values: 0 
buys: 1 
player value: 2 
card supply: [24. 15. 30. 25. 30.  8.  2. 10.  9.  6.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 0. 25. 29. 25.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25  1] -> size -> 41 
adversary victory points: 2
player victory points: 0 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [6. 1. 6. 6. 6.] 
cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.  3.] 
cards in deck: 13 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6 25  3] -> size -> 30 
action values: 0 
buys: 0 
player value: 0 
card supply: [24. 15. 30. 24. 30.  8.  2. 10.  9.  6.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 0. 25. 29. 25.  0.] 
adversary cards in discard: [] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25  1] -> size -> 41 
adversary victory points: 2
player victory points: 1 





Player: 0 
cards in hand: [ 0. 25. 29. 25.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25. 29. 25.] 
expected returns: [[-22.071722 ]
 [ -9.171668 ]
 [ -4.5564666]
 [ -9.171668 ]]
Chosen action: 29 : ['Poacher' '29' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 25. 29. 25.  0.] 
cards in discard: [] 
cards in deck: 36 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25  1] -> size -> 41 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 15. 30. 24. 30.  8.  2. 10.  9.  6.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 3.  0. 10. 15.  3.] 
adversary cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.  3.  6.  1.  6.  6.  6.] 
adversary owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6 25  3] -> size -> 30 
adversary victory points: 1
player victory points: 2 

Reward from previous game state: 
[-5  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 25 

action type: buy - action -1
Learning step: 0
desired expected reward: -185.54461669921875



action possibilites: [-1. 25.] 
expected returns: [[ 2.7042596]
 [17.149141 ]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0. 25.  0.] 
cards in discard: [25.  3.] 
cards in deck: 35 
card top of deck: [] 
played cards: [29.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25  1] -> size -> 41 
action values: 1 
buys: 0 
player value: 1 
card supply: [24. 15. 30. 24. 30.  8.  2. 10.  9.  6.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 3.  0. 10. 15.  3.] 
adversary cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.  3.  6.  1.  6.  6.  6.] 
adversary owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6 25  3] -> size -> 30 
adversary victory points: 1
player victory points: 2 

Reward from previous game state: 
[-5  0  0 30  0  0 20  0  0  0  0  0  0  0  0  0] 
sum of rewards: 45 

action type: discard_n_cards - action 1
Learning step: 0
desired expected reward: -10.005236625671387



action possibilites: [-1] 
expected returns: [[0.08610702]]
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 0.  0. 25. 29.] 
cards in discard: [25.  3.] 
cards in deck: 33 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25  1] -> size -> 41 
action values: 0 
buys: 0 
player value: 1 
card supply: [24. 15. 30. 24. 30.  8.  1. 10.  9.  6.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 3.  0. 10. 15.  3.] 
adversary cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.  3.  6.  1.  6.  6.  6.  6.] 
adversary owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6 25  3  6] -> size -> 31 
adversary victory points: 1
player victory points: 2 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action 25.0
Learning step: 0
desired expected reward: 17.14913558959961





----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
expected returns: [[  0.18336678]
 [ 12.523512  ]
 [  4.507613  ]
 [-69.50492   ]
 [  9.764251  ]
 [ -0.11803651]
 [  7.287368  ]
 [  0.3126366 ]]
Chosen buy action: 1.0 : ['Silver' '1' '3']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0. 25. 29.] 
cards in discard: [25.  3.] 
cards in deck: 33 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25  1] -> size -> 41 
action values: 0 
buys: 1 
player value: 3 
card supply: [24. 15. 30. 24. 30.  8.  1. 10.  9.  6.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 3.  0. 10. 15.  3.] 
adversary cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.  3.  6.  1.  6.  6.  6.  6.] 
adversary owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6 25  3  6] -> size -> 31 
adversary victory points: 1
player victory points: 2 

Reward from previous game state: 
[-5  0  0 30  0  0 40  0  0  0  0  0  0  0  0  0] 
sum of rewards: 65 

action type: take_action - action -1
Learning step: 0
desired expected reward: 0.08610701560974121



buy possibilites: [-1] 
expected returns: [[4.526477]]
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 0.  0. 25. 29.] 
cards in discard: [25.  3.  1.] 
cards in deck: 33 
card top of deck: [] 
played cards: [29. 25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25  1  1] -> size -> 42 
action values: 0 
buys: 0 
player value: 0 
card supply: [24. 14. 30. 24. 30.  8.  1. 10.  9.  6.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 3.  0. 10. 15.  3.] 
adversary cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.  3.  6.  1.  6.  6.  6.  6.] 
adversary owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6 25  3  6] -> size -> 31 
adversary victory points: 1
player victory points: 2 

Reward from previous game state: 
[ -5   0   0  30   0   0  40   0   0   0   0 -70   0   0  54   0] 
sum of rewards: 49 

action type: buy - action 1.0
Learning step: 0
desired expected reward: 12.523494720458984






         -------------------- Turn: 32 -------------------- 
Player: 1 
cards in hand: [ 3.  0. 10. 15.  3.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 10. 15.] 
Chosen action: 15 : ['Moneylender' '15' '4']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3.  0. 10. 15.  3.] 
cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.  3.  6.  1.  6.  6.  6.  6.] 
cards in deck: 8 
card top of deck: [] 
played cards: [] 
owned cards: [ 3  3 10  3 25 11  0  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0
  3  6  6  6 25  3  6] -> size -> 31 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 14. 30. 24. 30.  8.  1. 10.  9.  6.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1.  1.  1. 25.  0.] 
adversary cards in discard: [25.  3.  1. 29. 25.  0.  0. 25. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25  1  1] -> size -> 42 
adversary victory points: 2
player victory points: 0 


action possibilites: [-1] 
Chosen action: -1 : None


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 3. 10.  3.] 
cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.  3.  6.  1.  6.  6.  6.  6.] 
cards in deck: 8 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 3  3 10  3 25 11  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0  3
  6  6  6 25  3  6] -> size -> 30 
action values: 0 
buys: 0 
player value: 3 
card supply: [24. 14. 30. 24. 30.  8.  1. 10.  9.  6.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1.  1.  1. 25.  0.] 
adversary cards in discard: [25.  3.  1. 29. 25.  0.  0. 25. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25  1  1] -> size -> 42 
adversary victory points: 2
player victory points: 0 




----------- BUY PHASE ----------- 
buy possibilites: [ 0.  1.  3.  6. 11.  8. 10. -1.] 
Chosen buy action: 8.0 : ['Chapel' '8' '2']


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 10.  3.] 
cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.  3.  6.  1.  6.  6.  6.  6.] 
cards in deck: 8 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 3  3 10  3 25 11  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0  3
  6  6  6 25  3  6] -> size -> 30 
action values: 0 
buys: 1 
player value: 3 
card supply: [24. 14. 30. 24. 30.  8.  1. 10.  9.  6.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1.  1.  1. 25.  0.] 
adversary cards in discard: [25.  3.  1. 29. 25.  0.  0. 25. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25  1  1] -> size -> 42 
adversary victory points: 2
player victory points: 0 


buy possibilites: [-1] 
Chosen buy action: -1 : None


 --- GAME STATE ---
Action that is taken: buy 
cards in hand: [ 3. 10.  3.] 
cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.  3.  6.  1.  6.  6.  6.  6.
  8.] 
cards in deck: 8 
card top of deck: [] 
played cards: [15.] 
owned cards: [ 3  3 10  3 25 11  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0  3
  6  6  6 25  3  6  8] -> size -> 31 
action values: 0 
buys: 0 
player value: 1 
card supply: [24. 14. 30. 24. 30.  8.  1. 10.  9.  5.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [ 1.  1.  1. 25.  0.] 
adversary cards in discard: [25.  3.  1. 29. 25.  0.  0. 25. 29.] 
adversary owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25  1  1] -> size -> 42 
adversary victory points: 2
player victory points: 0 





Player: 0 
cards in hand: [ 1.  1.  1. 25.  0.] 
----------- ACTION PHASE ----------- 
action possibilites: [-1. 25.] 
expected returns: [[-33.338276]
 [-20.388746]]
Chosen action: 25 : ['Witch' '25' '5']


 --- GAME STATE ---
Action that is taken: take_action 
cards in hand: [ 1.  1.  1. 25.  0.] 
cards in discard: [25.  3.  1. 29. 25.  0.  0. 25. 29.] 
cards in deck: 28 
card top of deck: [] 
played cards: [] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25  1  1] -> size -> 42 
action values: 1 
buys: 0 
player value: 0 
card supply: [24. 14. 30. 24. 30.  8.  1. 10.  9.  5.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [3. 3. 3. 8. 0.] 
adversary cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.  3.  6.  1.  6.  6.  6.  6.
  8. 15.  3. 10.  3.] 
adversary owned cards: [ 3  3 10  3 25 11  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0  3
  6  6  6 25  3  6  8] -> size -> 31 
adversary victory points: 0
player victory points: 2 

Reward from previous game state: 
[-5  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0] 
sum of rewards: 55 

action type: buy - action -1
Learning step: 0
desired expected reward: 4.526476860046387



Player 0 won the game! 



Player 0 bought cards:
Copper: 0 
Silver: 13 
Gold: 0 
Estate: 0 
Duchy: 0 
Province: 0 
Curse: 0 

Remodel: 0 
Workshop: 0 
Chapel: 0 
Witch: 8 
Poacher: 10 
Militia: 0 
Market: 0 
Village: 0 
Library: 0 
Moneylender: 0 


 --- GAME STATE ---
Action that is taken: None 
cards in hand: [1. 1. 1. 0. 0. 0.] 
cards in discard: [25.  3.  1. 29. 25.  0.  0. 25. 29.] 
cards in deck: 26 
card top of deck: [] 
played cards: [25.] 
owned cards: [ 0  0  0  0  0  0  0  3  3  3  1 29 29 29 29 29 29  6 29 29 29 29  1  1
  1  1  1 25 25  1 25 25  1 25 25  1 25  1  1 25  1  1] -> size -> 42 
action values: 0 
buys: 0 
player value: 0 
card supply: [24. 14. 30. 24. 30.  8.  0. 10.  9.  5.  0.  0. 10. 10.  9. 10.  7.] 
adversary cards in hand: [3. 3. 3. 8. 0.] 
adversary cards in discard: [15. 25.  6.  3.  6. 25.  0.  0.  1.  0.  6.  3.  6.  1.  6.  6.  6.  6.
  8. 15.  3. 10.  3.  6.] 
adversary owned cards: [ 3  3 10  3 25 11  8  3  1  8  0  0  3  1  6 15  0  6  3  6 15  6  0  3
  6  6  6 25  3  6  8  6] -> size -> 32 
adversary victory points: 0
player victory points: 2 

Reward from previous game state: 
[     -5 3000000       0      60       0       0      20       0       0
       0       0       0       0       0       0       0] 
sum of rewards: 3000075 

action type: take_action - action 25.0
Learning step: 120003.8203125
desired expected reward: 119983.4296875



