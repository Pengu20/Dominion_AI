% @Inbook{handbook-of-robotics,
@book{handbook-of-robotics,
    author="Siciliano, Bruno
    and Khatib, Oussama",
    editor="Siciliano, Bruno
    and Khatib, Oussama",
    title="Robotics and the Handbook",
    bookTitle="Springer Handbook of Robotics",
    year="2016",
    publisher="Springer International Publishing",
    address="Cham",
    pages="955-988",
    abstract="Robots! Robots on Mars and in oceans, in hospitals and homes, in factories and schools; robots fighting fires, making goods and products, saving time and lives. Robots today are making a considerable impact on many aspects of modern life, from industrial manufacturing to healthcare, transportation, and exploration of the deep space and sea. Tomorrow, robots will be as pervasive and personal as today's personal computers. This chapter retraces the evolution of this fascinating field from the ancient to the modern times through a number of milestones: from the first automated mechanical artifact (1400 BC) through the establishment of the robot concept in the 1920s, the realization of the first industrial robots in the 1960s, the definition of robotics science and the birth of an active research community in the 1980s, and the expansion towards the challenges of the human world of the twenty-first century. Robotics in its long journey has inspired this handbook which is organized in three layers: the foundations of robotics science; the consolidated methodologies and technologies of robot design, sensing and perception, manipulation and interfaces, mobile and distributed robotics; the advanced applications of field and service robotics, as well as of human-centered and life-like robotics.",
    isbn="978-3-319-32552-1",
    doi="10.1007/978-3-319-32552-1_1",
    url="https://doi.org/10.1007/978-3-319-32552-1_1"
}

@ARTICLE{ graduated-non-convexity-for-robust-spatial-perception,
  author={Yang, Heng and Antonante, Pasquale and Tzoumas, Vasileios and Carlone, Luca},
  journal={IEEE Robotics and Automation Letters}, 
  title={Graduated Non-Convexity for Robust Spatial Perception: From Non-Minimal Solvers to Global Outlier Rejection}, 
  year={2020},
  volume={5},
  number={2},
  pages={1127-1134},
  doi={10.1109/LRA.2020.2965893}
}

@article{In-Hand-Object-Pose-Estimation-Using-Covariance-Based-Tactile-To-Geometry-Matching,
    author = {Bimbo, Joao and Luo, Shan and Althoefer, Kaspar and Liu, Hongbin},
    year = {2016},
    month = {01},
    pages = {1-1},
    title = {In-Hand Object Pose Estimation Using Covariance-Based Tactile To Geometry Matching},
    volume = {1},
    journal = {IEEE Robotics and Automation Letters},
    doi = {10.1109/LRA.2016.2517244}
}

@misc{cloudcompare,
  title = {CloudCompare},
  howpublished = {\url{http://cloudcompare.org/}},
  note = {Accessed: July 29, 2022}
}

@misc{pc-classification,
  doi = {10.48550/ARXIV.1908.04616},
  url = {https://arxiv.org/abs/1908.04616},
  author = {Uy, Mikaela Angelina and Pham, Quang-Hieu and Hua, Binh-Son and Nguyen, Duc Thanh and Yeung, Sai-Kit},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data}
}

@InProceedings{point-pose-net,
    author = {Chen, Wei and Duan, Jinming and Basevi, Hector and Chang, Hyung Jin and Leonardis, Ales},
    title = {PointPoseNet: Point Pose Network for Robust 6D Object Pose Estimation},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month = {March},
    year = {2020}
} 

@misc{shadow-dex-hand,
  title = {Shadow Robot},
  howpublished = {\url{https://www.shadowrobot.com/dexterous-hand-series/}},
  note = {Accessed: July 29, 2022}
}

@article{learning-dexterous-in-hand-manipulation,
author = {OpenAI: Marcin Andrychowicz and Bowen Baker and Maciek Chociej and Rafal Józefowicz and Bob McGrew and Jakub Pachocki and Arthur Petron and Matthias Plappert and Glenn Powell and Alex Ray and Jonas Schneider and Szymon Sidor and Josh Tobin and Peter Welinder and Lilian Weng and Wojciech Zaremba},
title ={Learning dexterous in-hand manipulation},
journal = {The International Journal of Robotics Research},
volume = {39},
number = {1},
pages = {3-20},
year = {2020},
doi = {10.1177/0278364919887447},
url = {https://doi.org/10.1177/0278364919887447},
eprint = {  https://doi.org/10.1177/0278364919887447},
    abstract = { We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies that can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system such as friction coefficients and an object’s appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM. }
}

@INPROCEEDINGS{extrinsic-dexterous-hand,
        author={Dafle, Nikhil Chavan and Rodriguez, Alberto and Paolini, Robert and Tang, Bowei and Srinivasa, Siddhartha S. and Erdmann, Michael and Mason, Matthew T. and Lundberg, Ivan and Staab, Harald and Fuhlbrigge, Thomas},
        booktitle={2014 IEEE International Conference on Robotics and Automation (ICRA)}, 
        title={Extrinsic dexterity: In-hand manipulation with external forces}, 
        year={2014},
        volume={},
        number={},
        pages={1578-1585},
        doi={10.1109/ICRA.2014.6907062}
}


@inproceedings{surprisingly-robust-in-hand-manipulation-an-empirical-study,
        doi = {10.15607/rss.2021.xvii.089},
        url = {https://doi.org/10.15607\%2Frss.2021.xvii.089},
        year = {2021},
        month = {July},
        publisher = {Robotics: Science and Systems Foundation},
        author = {Aditya Bhatt and Adrian Sieler and Steffen Puhlmann and Oliver Brock},
        title = {Surprisingly Robust In-Hand Manipulation: An Empirical Study},
        booktitle = {Robotics: Science and Systems {XVII}}
}

@inproceedings{learning-hierarchical-control-for-robust-in-hand-manipulation,
        author={Li, Tingguang and Srinivasan, Krishnan and Meng, Max Qing-Hu and Yuan, Wenzhen and Bohg, Jeannette},
        booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)}, 
        title={Learning Hierarchical Control for Robust In-Hand Manipulation},
        year={2020},
        volume={},
        number={},
        pages={8855-8862},
        doi={10.1109/ICRA40945.2020.9197343}
}


@INPROCEEDINGS{a-triangle-histogram-for-object-classification-by-tactile-sensing,  
author={Zhang, Mabel M. and Kennedy, Monroe D. and Hsieh, M. Ani and Daniilidis, Kostas},  booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},   title={A triangle histogram for object classification by tactile sensing},   year={2016},  volume={},  number={},  pages={4931-4938},  doi={10.1109/IROS.2016.7759724}
}

@ARTICLE{a-probabilistic-approach-to-tactile-shape-reconstruction,  author={Meier, Martin and Schopfer, Matthias and Haschke, Robert and Ritter, Helge},  journal={IEEE Transactions on Robotics},   title={A Probabilistic Approach to Tactile Shape Reconstruction},   year={2011},  volume={27},  number={3},  pages={630-635},  doi={10.1109/TRO.2011.2120830}}

@INPROCEEDINGS{patchGraph-in-hand-tactile-tracking-with-learned-surface-normals,
  author={Sodhi, Paloma and Kaess, Michael and Mukadanr, Mustafa and Anderson, Stuart},
  booktitle={2022 International Conference on Robotics and Automation (ICRA)}, 
  title={PatchGraph: In-hand tactile tracking with learned surface normals}, 
  year={2022},
  volume={},
  number={},
  pages={2164-2170},
  doi={10.1109/ICRA46639.2022.9811953}
}

@ARTICLE{visuotactile-6d-pose-estimation-of-an-in-hand-object-using-vision-and-tactile-sensor-data,

  author={Dikhale, Snehal and Patel, Karankumar and Dhingra, Daksh and Naramura, Itoshi and Hayashi, Akinobu and Iba, Soshi and Jamali, Nawid},

  journal={IEEE Robotics and Automation Letters}, 

  title={VisuoTactile 6D Pose Estimation of an In-Hand Object Using Vision and Tactile Sensor Data}, 

  year={2022},

  volume={7},

  number={2},

  pages={2148-2155},

  doi={10.1109/LRA.2022.3143289}}

@INPROCEEDINGS{contact-based-in-hand-pose-estimation-using-bayesian-state-estimation-and-particle-filtering,

  author={von Drigalski, Felix and Taniguchi, Shohei and Lee, Robert and Matsubara, Takamitsu and Hamaya, Masashi and Tanaka, Kazutoshi and Ijiri, Yoshihisa},

  booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)}, 

  title={Contact-based in-hand pose estimation using Bayesian state estimation and particle filtering}, 

  year={2020},

  volume={},

  number={},

  pages={7294-7299},

  doi={10.1109/ICRA40945.2020.9196640}}

@ARTICLE{bayesian-exploration-for-intelligent-identification-of-textures,
  
AUTHOR={Fishel, Jeremy and Loeb, Gerald},   
	 
TITLE={Bayesian Exploration for Intelligent Identification of Textures},      
	
JOURNAL={Frontiers in Neurorobotics},      
	
VOLUME={6},           
	
YEAR={2012},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fnbot.2012.00004},       
	
DOI={10.3389/fnbot.2012.00004},      
	
ISSN={1662-5218},   
   
ABSTRACT={In order to endow robots with human-like abilities to characterize and identify objects, they must be provided with tactile sensors and intelligent algorithms to select, control, and interpret data from useful exploratory movements. Humans make informed decisions on the sequence of exploratory movements that would yield the most information for the task, depending on what the object may be and prior knowledge of what to expect from possible exploratory movements. This study is focused on texture discrimination, a subset of a much larger group of exploratory movements and percepts that humans use to discriminate, characterize, and identify objects. Using a testbed equipped with a biologically inspired tactile sensor (the BioTac), we produced sliding movements similar to those that humans make when exploring textures. Measurement of tactile vibrations and reaction forces when exploring textures were used to extract measures of textural properties inspired from psychophysical literature (traction, roughness, and fineness). Different combinations of normal force and velocity were identified to be useful for each of these three properties. A total of 117 textures were explored with these three movements to create a database of prior experience to use for identifying these same textures in future encounters. When exploring a texture, the discrimination algorithm adaptively selects the optimal movement to make and property to measure based on previous experience to differentiate the texture from a set of plausible candidates, a process we call Bayesian exploration. Performance of 99.6\% in correctly discriminating pairs of similar textures was found to exceed human capabilities. Absolute classification from the entire set of 117 textures generally required a small number of well-chosen exploratory movements (median=5) and yielded a 95.4\% success rate. The method of Bayesian exploration developed and tested in this paper may generalize well to other cognitive problems.}
}

% Contact models

% contact image based
@inproceedings{tactile-mapping-and-localization-from-high-resolution-tactile-imprints,
author = {Bauza, Maria and Canal, Oleguer and Rodriguez, Alberto},
year = {2019},
month = {05},
pages = {3811-3817},
title = {Tactile Mapping and Localization from High-Resolution Tactile Imprints},
doi = {10.1109/ICRA.2019.8794298}
}

% point cloud based 
@INPROCEEDINGS{tracking-objects-with-point-clouds-from-vision-and-touch,  author={Izatt, Gregory and Mirano, Geronimo and Adelson, Edward and Tedrake, Russ},  booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)},   title={Tracking objects with point clouds from vision and touch},   year={2017},  volume={},  number={},  pages={4000-4007},  doi={10.1109/ICRA.2017.7989460}}

@INPROCEEDINGS{planning-in-hand-object-manipulation-with-multiﬁngered-hands-considering-task-constraints,

  author={Hertkorn, Katharina and Roa, Maximo A. and Borst, Christoph},

  booktitle={2013 IEEE International Conference on Robotics and Automation}, 

  title={Planning in-hand object manipulation with multifingered hands considering task constraints}, 

  year={2013},

  volume={},

  number={},

  pages={617-624},

  doi={10.1109/ICRA.2013.6630637}}

